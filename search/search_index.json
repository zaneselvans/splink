{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#fast-accurate-and-scalable-probabilistic-data-linkage","title":"Fast, accurate and scalable probabilistic data linkage","text":"<p>Splink is a Python package for probabilistic record linkage (entity resolution) that allows you to deduplicate and link records from datasets without unique identifiers.</p> <p>Get Started with Splink</p>"},{"location":"index.html#key-features","title":"Key Features","text":"<p>\u26a1 Speed: Capable of linking a million records on a laptop in approximately one minute. \ud83c\udfaf Accuracy: Full support for term frequency adjustments and user-defined fuzzy matching logic. \ud83c\udf10 Scalability: Execute linkage jobs in Python (using DuckDB) or big-data backends like AWS Athena or Spark for 100+ million records. \ud83c\udf93 Unsupervised Learning: No training data is required, as models can be trained using an unsupervised approach. \ud83d\udcca Interactive Outputs: Provides a wide range of interactive outputs to help users understand their model and diagnose linkage problems.  </p> <p>Splink's core linkage algorithm is based on Fellegi-Sunter's model of record linkage, with various customizations to improve accuracy.</p>"},{"location":"index.html#what-does-splink-do","title":"What does Splink do?","text":"<p>Consider the following records that lack a unique person identifier:</p> <p></p> <p>Splink predicts which rows link together:</p> <p></p> <p>and clusters these links to produce an estimated person ID:</p> <p></p>"},{"location":"index.html#what-data-does-splink-work-best-with","title":"What data does Splink work best with?","text":"<p>Before using Splink, input data should be standardized, with consistent column names and formatting (e.g., lowercased, punctuation cleaned up, etc.).</p> <p>Splink performs best with input data containing multiple columns that are not highly correlated. For instance, if the entity type is persons, you may have columns for full name, date of birth, and city. If the entity type is companies, you could have columns for name, turnover, sector, and telephone number.</p> <p>High correlation occurs when the value of a column is highly constrained (predictable) from the value of another column. For example, a 'city' field is almost perfectly correlated with 'postcode'. Gender is highly correlated with 'first name'. Correlation is particularly problematic if all of your input columns are highly correlated.</p> <p>Splink is not designed for linking a single column containing a 'bag of words'. For example, a table with a single 'company name' column, and no other details.</p>"},{"location":"index.html#support","title":"Support","text":"<p>To find the best place to ask a question, report a bug or get general advice, please refer to our Contributing Guide.</p>"},{"location":"index.html#use-cases","title":"Use Cases","text":"<p>Here is a list of some of our known users and their use cases:</p> Public Sector (UK)Public Sector (International)Academia <ul> <li>Ministry of Justice created linked datasets (combining courts, prisons and probation data) for use by researchers as part of the Data First programme</li> <li>Office for National Statistics's Business Index (formerly the Inter Departmental Business Register), Demographic Index and the 2021 Census</li> <li>Lewisham Council (London) identified and auto-enrolled over 500 additional eligible families to receive Free School Meals</li> <li>London Office of Technology and Innovation created a dashboard to help better measure and reduce rough sleeping across London</li> <li>Competition and Markets Authority identified 'Persons with Significant Control' and estimated ownership groups across companies</li> <li>Office for Health Improvement and Disparities linked Health and Justice data to assess the pathways between probation and specialist alcohol and drug treatment services as part of the Better Outcomes through Linked Data programme</li> <li>Ministry of Defence recently launched their Veteran's Card system which uses Splink to verify applicants against historic records. This project was shortlisted for the Civil Service Awards</li> <li>Gateshead Council, in partnership with the National Innovation Centre for Data are creating a single view of debt</li> </ul> <ul> <li>Chilean Ministry of Health and University College London have assessed the access to immunisation programs among the migrant population</li> <li>Florida Cancer Registry, published a feasibility study which showed Splink was faster and more accurate than alternatives</li> </ul> <ul> <li>Stanford University investigated the impact of receiving government assistance has on political attitudes</li> <li>Bern University researched how Active Learning can be applied to Biomedical Record Linkage</li> </ul> <p>Sadly, we don't hear about the majority of our users or what they are working on. If you have a use case and it is not shown here please add it to the list!</p>"},{"location":"index.html#awards","title":"Awards","text":"<p>\u2753 Future of Government Awards 2023: Open Source Creation - Shortlisted, result to be announced shortly</p> <p>\ud83e\udd48 Civil Service Awards 2023: Best Use of Data, Science, and Technology - Runner up</p> <p>\ud83e\udd47 Analysis in Government Awards 2022: People's Choice Award - Winner</p> <p>\ud83e\udd48 Analysis in Government Awards 2022: Innovative Methods - Runner up</p> <p>\ud83e\udd47 Analysis in Government Awards 2020: Innovative Methods - Winner</p> <p>\ud83e\udd47 MoJ Data and Analytical Services Directorate (DASD) Awards 2020: Innovation and Impact - Winner</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use Splink in your research, we'd be grateful for a citation as follows:</p> <pre><code>@article{Linacre_Lindsay_Manassis_Slade_Hepworth_2022,\n    title        = {Splink: Free software for probabilistic record linkage at scale.},\n    author       = {Linacre, Robin and Lindsay, Sam and Manassis, Theodore and Slade, Zoe and Hepworth, Tom and Kennedy, Ross and Bond, Andrew},\n    year         = 2022,\n    month        = {Aug.},\n    journal      = {International Journal of Population Data Science},\n    volume       = 7,\n    number       = 3,\n    doi          = {10.23889/ijpds.v7i3.1794},\n    url          = {https://ijpds.org/article/view/1794},\n}\n</code></pre>"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>We are very grateful to ADR UK (Administrative Data Research UK) for providing the initial funding for this work as part of the Data First project.</p> <p>We are extremely grateful to professors Katie Harron, James Doidge and Peter Christen for their expert advice and guidance in the development of Splink. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work. Any errors remain our own.</p>"},{"location":"CONTRIBUTING.html","title":"Contributor Guide","text":""},{"location":"CONTRIBUTING.html#contributing-to-splink","title":"Contributing to Splink","text":"<p>Contributing to an open source project takes many forms. Below are some of the ways you can contribute to Splink!</p>"},{"location":"CONTRIBUTING.html#asking-questions","title":"Asking questions","text":"<p>If you have a question about Splink, we recommended asking on our GitHub discussion board. This means that other users can benefit from the answers too! On that note, it is always worth checking if a similar question has been asked (and answered) before.</p>"},{"location":"CONTRIBUTING.html#reporting-issues","title":"Reporting issues","text":"<p>Is something broken? Or not acting how you would expect? Are we missing a feature that would make your life easier? We want to know about it!</p> <p>When reporting issues please include as much detail as possible about your operating system, Splink version, python version and which SQL backend you are using. Whenever possible, please also include a brief, self-contained code example that demonstrates the problem. It is particularly helpful if you can look through the existing issues and provide links to any related issues.</p>"},{"location":"CONTRIBUTING.html#contributing-to-documentation","title":"Contributing to documentation","text":"<p>Contributions to Splink are not limited to the code. Feedback and input on our documentation from a user's perspective is extremely valuable - even something as small as fixing a typo. More generally, if you are interested in starting to work on Splink, documentation is a great way to get those first commits!</p> <p>Behind the scenes, the Splink documentation is split into 2 parts:</p> <ul> <li>The Tutorials and Example Notebooks are stored in a separate repo - splink_demos</li> <li>Everything else is stored in the Splink repo either in:<ul> <li>the docs folder</li> <li>the Splink code itself. E.g. docstrings from linker.py feed directly into the Linker API docs.</li> </ul> </li> </ul> <p>There are some user restrictions on both the splink and splink_demos repos, so to make changes to either repo you will need to create a fork and then create a Pull Request which one of the Splink dev team will review.</p> <p>Shortcut</p> <p>If you are looking through the docs and find an issue, hit the  button on the top right corner of the page. This will take you to the underlying file on the Splink GitHub page.</p> <p>This is not supported for Tutorials or Examples as they are in a separate repo.</p> <p>For small changes, such as wording and typos, changes can be made directly in GitHub. However, for larger changes it may be worth cloning the relevant repo to your local machine. This way, you can build the docs site locally to check how the changes will look in the deployed doc site.</p> <p>We are trying to make our documentation as accessible to as many people as possible. If you find any problems with accessibility then please let us know by raising an issue, or feel free to put in a Pull Request with your suggested fixes.</p>"},{"location":"CONTRIBUTING.html#contributing-code","title":"Contributing code","text":"<p>Thanks for your interest in contributing code to Splink!</p> <p>There are a number of ways to get involved:</p> <ul> <li>Start work on an existing issue, there should be some with a <code>good first issue</code> flag which are a good place to start. </li> <li>Tackle a problem you have identified. If you have identified a feature or bug, the first step is to create a new issue to explain what you have identified and what you plan to implement, then you are free to fork the repo and get coding!</li> </ul> <p>In either case, we ask that you assign yourself to the relevant issue and open up a draft PR while you are working on your feature/bug-fix. This helps the Splink dev team keep track of developments and means we can start supporting you sooner!</p> <p>Small PRs</p> <p>In the Splink dev team, we believe that small Pull Requests make better code. They:</p> <ul> <li>are more focused</li> <li>increase understanding and clarity</li> <li>are easier (and quicker) to review</li> <li>get feedback quicker</li> </ul> <p>You can always add further PRs to build extra functionality. Starting out with a minimum viable product and iterating  makes for better software (in our opinion). It also helps get features out into the wild sooner with regular Splink releases every other Wednesday.</p> <p>When making code changes, we recommend: * Recreating Splink's virtual environment to best replicate the conditions in which Splink will be used in practice.  * Adding tests to ensure your code works as expected. These will be run through GitHub Actions when a PR is opened. * Linting to ensure that code is styled consistently.</p>"},{"location":"CONTRIBUTING.html#branching-strategy","title":"Branching Strategy","text":"<p>As mentioned above, we like to keep PRs small and our philosophy on branching reflects that. Splink does not work with a <code>dev</code> branch, so all branches/forks should be made from <code>master</code> in the first instance. </p> <p>For small PRs, simply branching from <code>master</code> and merging from there is perfectly fine. However, if you have a larger feature to add we tend to try and break these up into chunks. If you have a larger feature, please consider creating a simple minimum-viable feature and submit for review. Once this has been reviewed by the Splink dev team there are two options to consider:</p> <ol> <li>Merge minimal feature into master then create a new branch with additional features.</li> <li>Do not merge the initial feature branch and create additional feature branches from the reviewed branch.</li> </ol> <p>The best solution often depends on the specific feature being created and any other development work happening in that area of the codebase. If you are unsure, please ask the dev team for advice on how to best structure your changes in your initial PR and we can come to a decision together.</p>"},{"location":"SplinkDataFrame.html","title":"SplinkDataFrame API","text":"","tags":["API"]},{"location":"SplinkDataFrame.html#documentation-for-splinkdataframe-object","title":"Documentation for <code>SplinkDataFrame</code> object","text":"<p>Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like <code>as_pandas_dataframe()</code> and <code>as_record_dict()</code> to retrieve data</p> Source code in <code>splink/splink_dataframe.py</code> <pre><code>class SplinkDataFrame:\n    \"\"\"Abstraction over dataframe to handle basic operations like retrieving data and\n    retrieving column names, which need different implementations depending on whether\n    it's a spark dataframe, sqlite table etc.\n    Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data\n    \"\"\"\n\n    def __init__(\n        self,\n        templated_name: str,\n        physical_name: str,\n        linker: Linker,\n        metadata: dict = None,\n    ):\n        self.templated_name = templated_name\n        self.physical_name = physical_name\n        self.linker = linker\n        self._target_schema = \"splink\"\n        self.created_by_splink = False\n        self.sql_used_to_create = None\n        self.metadata = metadata or {}\n\n    @property\n    def columns(self):\n        pass\n\n    @property\n    def columns_escaped(self):\n        cols = self.columns\n        return [c.name for c in cols]\n\n    def validate(self):\n        pass\n\n    @property\n    def physical_and_template_names_equal(self):\n        return self.templated_name == self.physical_name\n\n    def _check_drop_table_created_by_splink(self, force_non_splink_table=False):\n        if not self.created_by_splink:\n            if not force_non_splink_table:\n                raise ValueError(\n                    f\"You've asked to drop table {self.physical_name} from your \"\n                    \"database which is not a table created by Splink.  If you really \"\n                    \"want to drop this table, you can do so by setting \"\n                    \"force_non_splink_table=True\"\n                )\n        logger.debug(\n            f\"Dropping table with templated name {self.templated_name} and \"\n            f\"physical name {self.physical_name}\"\n        )\n\n    def _drop_table_from_database(self, force_non_splink_table=False):\n        raise NotImplementedError(\n            \"_drop_table_from_database from database not \" \"implemented for this linker\"\n        )\n\n    def drop_table_from_database_and_remove_from_cache(\n        self, force_non_splink_table=False\n    ):\n        \"\"\"Drops the table from the underlying database, and removes it\n        from the (linker) cache.\n\n        By default this will fail if the table is not one created by Splink,\n        but this check can be overriden\n\n        Examples:\n            ```py\n            df_predict = linker.predict()\n            df_predict.drop_table_from_database_and_remove_from_cache()\n            # predictions table no longer in the database / cache\n            ```\n        Args:\n            force_non_splink_table (bool, optional): If True, skip check if the\n                table was created by Splink and always drop regardless. If False,\n                only drop if table was created by Splink. Defaults to False.\n\n        \"\"\"\n        self._drop_table_from_database(force_non_splink_table=force_non_splink_table)\n        self.linker._remove_splinkdataframe_from_cache(self)\n\n    def as_record_dict(self, limit=None):\n        \"\"\"Return the dataframe as a list of record dictionaries.\n\n        This can be computationally expensive if the dataframe is large.\n\n        Examples:\n            ```py\n            df_predict = linker.predict()\n            ten_edges = df_predict.as_record_dict(10)\n            ```\n        Args:\n            limit (int, optional): If provided, return this number of rows (equivalent\n            to a limit statement in SQL). Defaults to None, meaning return all rows\n\n        Returns:\n            list: a list of records, each of which is a dictionary\n        \"\"\"\n        raise NotImplementedError(\"as_record_dict not implemented for this linker\")\n\n    def as_pandas_dataframe(self, limit=None):\n        \"\"\"Return the dataframe as a pandas dataframe.\n\n        This can be computationally expensive if the dataframe is large.\n\n        Args:\n            limit (int, optional): If provided, return this number of rows (equivalent\n            to a limit statement in SQL). Defaults to None, meaning return all rows\n\n        Examples:\n            ```py\n            df_predict = linker.predict()\n            df_ten_edges = df_predict.as_pandas_dataframe(10)\n            ```\n        Returns:\n            pandas.DataFrame: pandas Dataframe\n        \"\"\"\n        import pandas as pd\n\n        return pd.DataFrame(self.as_record_dict(limit=limit))\n\n    def _repr_pretty_(self, p, cycle):\n        msg = (\n            f\"Table name in database: `{self.physical_name}`\\n\"\n            \"\\nTo retrieve records, you can call the following methods on this object:\"\n            \"\\n`.as_record_dict(limit=5)` or \"\n            \"`.as_pandas_dataframe(limit=5)`.\\n\"\n            \"\\nYou may omit the `limit` argument to return all records.\"\n            \"\\n\\nThis table represents the following splink entity: \"\n            f\"{self.templated_name}\"\n        )\n        p.text(msg)\n\n    def to_parquet(self, filepath, overwrite=False):\n        \"\"\"Save the dataframe in parquet format.\n\n        Examples:\n            ```py\n            df_predict = linker.predict()\n            df_predict.to_parquet(\"model_predictions.parquet\", overwrite=True)\n            ```\n        Args:\n            filepath (str): Filepath where csv will be saved.\n            overwrite (bool, optional): If True, overwrites file if it already exists.\n                Default is False.\n        \"\"\"\n        raise NotImplementedError(\"`to_parquet` not implemented for this linker\")\n\n    def to_csv(self, filepath, overwrite=False):\n        \"\"\"Save the dataframe in csv format.\n\n        Examples:\n            ```py\n            df_predict = linker.predict()\n            df_predict.to_csv(\"model_predictions.csv\", overwrite=True)\n            ```\n        Args:\n            filepath (str): Filepath where csv will be saved.\n            overwrite (bool, optional): If True, overwrites file if it already exists.\n                Default is False.\n        \"\"\"\n        raise NotImplementedError(\"`to_csv` not implemented for this linker\")\n\n    def check_file_exists(self, filepath):\n        p = Path(filepath)\n        if p.exists():\n            raise FileExistsError(\n                \"The filepath you've supplied already exists. Please use \"\n                \"either `overwrite = True` or manually move or delete the \"\n                \"existing file.\"\n            )\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.as_pandas_dataframe","title":"<code>as_pandas_dataframe(limit=None)</code>","text":"<p>Return the dataframe as a pandas dataframe.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>If provided, return this number of rows (equivalent</p> <code>None</code> <p>Examples:</p> <pre><code>df_predict = linker.predict()\ndf_ten_edges = df_predict.as_pandas_dataframe(10)\n</code></pre> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def as_pandas_dataframe(self, limit=None):\n    \"\"\"Return the dataframe as a pandas dataframe.\n\n    This can be computationally expensive if the dataframe is large.\n\n    Args:\n        limit (int, optional): If provided, return this number of rows (equivalent\n        to a limit statement in SQL). Defaults to None, meaning return all rows\n\n    Examples:\n        ```py\n        df_predict = linker.predict()\n        df_ten_edges = df_predict.as_pandas_dataframe(10)\n        ```\n    Returns:\n        pandas.DataFrame: pandas Dataframe\n    \"\"\"\n    import pandas as pd\n\n    return pd.DataFrame(self.as_record_dict(limit=limit))\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.as_record_dict","title":"<code>as_record_dict(limit=None)</code>","text":"<p>Return the dataframe as a list of record dictionaries.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>Examples:</p> <pre><code>df_predict = linker.predict()\nten_edges = df_predict.as_record_dict(10)\n</code></pre> <p>Returns:</p> Name Type Description <code>list</code> <p>a list of records, each of which is a dictionary</p> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def as_record_dict(self, limit=None):\n    \"\"\"Return the dataframe as a list of record dictionaries.\n\n    This can be computationally expensive if the dataframe is large.\n\n    Examples:\n        ```py\n        df_predict = linker.predict()\n        ten_edges = df_predict.as_record_dict(10)\n        ```\n    Args:\n        limit (int, optional): If provided, return this number of rows (equivalent\n        to a limit statement in SQL). Defaults to None, meaning return all rows\n\n    Returns:\n        list: a list of records, each of which is a dictionary\n    \"\"\"\n    raise NotImplementedError(\"as_record_dict not implemented for this linker\")\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.drop_table_from_database_and_remove_from_cache","title":"<code>drop_table_from_database_and_remove_from_cache(force_non_splink_table=False)</code>","text":"<p>Drops the table from the underlying database, and removes it from the (linker) cache.</p> <p>By default this will fail if the table is not one created by Splink, but this check can be overriden</p> <p>Examples:</p> <pre><code>df_predict = linker.predict()\ndf_predict.drop_table_from_database_and_remove_from_cache()\n# predictions table no longer in the database / cache\n</code></pre> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def drop_table_from_database_and_remove_from_cache(\n    self, force_non_splink_table=False\n):\n    \"\"\"Drops the table from the underlying database, and removes it\n    from the (linker) cache.\n\n    By default this will fail if the table is not one created by Splink,\n    but this check can be overriden\n\n    Examples:\n        ```py\n        df_predict = linker.predict()\n        df_predict.drop_table_from_database_and_remove_from_cache()\n        # predictions table no longer in the database / cache\n        ```\n    Args:\n        force_non_splink_table (bool, optional): If True, skip check if the\n            table was created by Splink and always drop regardless. If False,\n            only drop if table was created by Splink. Defaults to False.\n\n    \"\"\"\n    self._drop_table_from_database(force_non_splink_table=force_non_splink_table)\n    self.linker._remove_splinkdataframe_from_cache(self)\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.to_csv","title":"<code>to_csv(filepath, overwrite=False)</code>","text":"<p>Save the dataframe in csv format.</p> <p>Examples:</p> <pre><code>df_predict = linker.predict()\ndf_predict.to_csv(\"model_predictions.csv\", overwrite=True)\n</code></pre> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def to_csv(self, filepath, overwrite=False):\n    \"\"\"Save the dataframe in csv format.\n\n    Examples:\n        ```py\n        df_predict = linker.predict()\n        df_predict.to_csv(\"model_predictions.csv\", overwrite=True)\n        ```\n    Args:\n        filepath (str): Filepath where csv will be saved.\n        overwrite (bool, optional): If True, overwrites file if it already exists.\n            Default is False.\n    \"\"\"\n    raise NotImplementedError(\"`to_csv` not implemented for this linker\")\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.to_parquet","title":"<code>to_parquet(filepath, overwrite=False)</code>","text":"<p>Save the dataframe in parquet format.</p> <p>Examples:</p> <pre><code>df_predict = linker.predict()\ndf_predict.to_parquet(\"model_predictions.parquet\", overwrite=True)\n</code></pre> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def to_parquet(self, filepath, overwrite=False):\n    \"\"\"Save the dataframe in parquet format.\n\n    Examples:\n        ```py\n        df_predict = linker.predict()\n        df_predict.to_parquet(\"model_predictions.parquet\", overwrite=True)\n        ```\n    Args:\n        filepath (str): Filepath where csv will be saved.\n        overwrite (bool, optional): If True, overwrites file if it already exists.\n            Default is False.\n    \"\"\"\n    raise NotImplementedError(\"`to_parquet` not implemented for this linker\")\n</code></pre>","tags":["API"]},{"location":"blocking_rule_composition.html","title":"Blocking Rule Composition","text":"","tags":["API","blocking rule"]},{"location":"blocking_rule_composition.html#documentation-for-blocking_rule_composition-functions","title":"Documentation for <code>blocking_rule_composition</code> functions","text":"<p><code>blocking_composition</code> allows the merging of existing blocking rules by a logical SQL clause - <code>AND</code>, <code>OR</code> or <code>NOT</code>.</p> <p>This extends the functionality of our base blocking rules by allowing users to \"join\" existing comparisons by various SQL clauses.</p> <p>For example, <code>and_(block_on(\"first_name\"), block_on(\"surname\"))</code> creates a dual check for an exact match where both <code>first_name</code> and <code>surname</code> are equal.</p> <p>The detailed API for each of these are outlined below.</p>","tags":["API","blocking rule"]},{"location":"blocking_rule_composition.html#library-comparison-composition-apis","title":"Library comparison composition APIs","text":"","tags":["API","blocking rule"]},{"location":"blocking_rule_composition.html#splink.blocking_rule_composition.and_","title":"<code>and_(*brls, salting_partitions=1)</code>","text":"<p>Merge BlockingRules using logical \"AND\".</p> <p>Merge multiple BlockingRules into a single BlockingRule by merging their SQL conditions using a logical \"AND\".</p> <p>Parameters:</p> Name Type Description Default <code>*brls</code> <code>BlockingRule | dict | str</code> <p>BlockingRules or blocking rules in the string/dictionary format.</p> <code>()</code> <code>salting_partitions</code> <code>(optional, int)</code> <p>Whether to add salting to the blocking rule. More information on salting can be found within the docs. Salting is only valid for Spark.</p> <code>1</code> <p>Examples:</p>  DuckDB Spark Athena SQLitePostgreSQL <p>Simple exact rule composition with an <code>AND</code> clause </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    brl.exact_match_rule(\"surname\")\n)\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\"\n)\n</code></pre> <p>Simple exact rule composition with an <code>AND</code> clause </p><pre><code>import splink.spark.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    brl.exact_match_rule(\"surname\")\n)\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column, with additional salting (spark exclusive) <pre><code>import splink.spark.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n    salting_partitions=5\n)\n</code></pre> <p>Simple exact rule composition with an <code>AND</code> clause </p><pre><code>import splink.athena.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    brl.exact_match_rule(\"surname\")\n)\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.athena.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Simple exact rule composition with an <code>AND</code> clause </p><pre><code>import splink.sqlite.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    brl.exact_match_rule(\"surname\")\n)\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.sqlite.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.postgres.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    brl.exact_match_rule(\"surname\")\n)\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.postgres.blocking_rule_library as brl\nbrl.and_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Returns:</p> Name Type Description <code>BlockingRule</code> <code>BlockingRule</code> <p>A new BlockingRule with the merged SQL condition</p> Source code in <code>splink/blocking_rule_composition.py</code> <pre><code>def and_(\n    *brls: BlockingRule | dict | str,\n    salting_partitions=1,\n) -&gt; BlockingRule:\n    \"\"\"Merge BlockingRules using logical \"AND\".\n\n    Merge multiple BlockingRules into a single BlockingRule by\n    merging their SQL conditions using a logical \"AND\".\n\n\n    Args:\n        *brls (BlockingRule | dict | str): BlockingRules or\n            blocking rules in the string/dictionary format.\n        salting_partitions (optional, int): Whether to add salting\n            to the blocking rule. More information on salting can\n            be found within the docs. Salting is only valid for Spark.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple exact rule composition with an `AND` clause\n            ``` python\n            import splink.duckdb.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                brl.exact_match_rule(\"surname\")\n            )\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.duckdb.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            )\n            ```\n        === \":simple-apachespark: Spark\"\n            Simple exact rule composition with an `AND` clause\n            ``` python\n            import splink.spark.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                brl.exact_match_rule(\"surname\")\n            )\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column, with additional salting (spark exclusive)\n            ``` python\n            import splink.spark.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n                salting_partitions=5\n            )\n            ```\n        === \":simple-amazonaws: Athena\"\n            Simple exact rule composition with an `AND` clause\n            ``` python\n            import splink.athena.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                brl.exact_match_rule(\"surname\")\n            )\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.athena.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Simple exact rule composition with an `AND` clause\n            ``` python\n            import splink.sqlite.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                brl.exact_match_rule(\"surname\")\n            )\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.sqlite.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n        === \"PostgreSQL\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.postgres.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                brl.exact_match_rule(\"surname\")\n            )\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.postgres.blocking_rule_library as brl\n            brl.and_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n\n    Returns:\n        BlockingRule: A new BlockingRule with the merged\n            SQL condition\n    \"\"\"\n    return _br_merge(\n        *brls,\n        clause=\"AND\",\n        salting_partitions=salting_partitions,\n    )\n</code></pre>","tags":["API","blocking rule"]},{"location":"blocking_rule_composition.html#splink.blocking_rule_composition.not_","title":"<code>not_(*brls, salting_partitions=1)</code>","text":"<p>Invert a BlockingRule using \"NOT\".</p> <p>Returns a BlockingRule with the same SQL condition as the input, but prefixed with \"NOT\".</p> <p>Parameters:</p> Name Type Description Default <code>*brls</code> <code>BlockingRule | dict | str</code> <p>BlockingRules or blocking rules in the string/dictionary format.</p> <code>()</code> <code>salting_partitions</code> <code>(optional, int)</code> <p>Whether to add salting to the blocking rule. More information on salting can be found within the docs. Salting is only valid for Spark.</p> <code>1</code> <p>Examples:</p>  DuckDB Spark Athena SQLitePostgreSQL <p>Block where we do not have an exact match on first name </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.not_(brl.exact_match_rule(\"first_name\"))\n</code></pre> <p>Block where we do not have an exact match on first name </p><pre><code>import splink.spark.blocking_rule_library as brl\nbrl.not_(brl.exact_match_rule(\"first_name\"))\n</code></pre> <p>Block where we do not have an exact match on first name </p><pre><code>import splink.athena.blocking_rule_library as brl\nbrl.not_(brl.exact_match_rule(\"first_name\"))\n</code></pre> <p>Block where we do not have an exact match on first name </p><pre><code>import splink.sqlite.blocking_rule_library as brl\nbrl.not_(brl.exact_match_rule(\"first_name\"))\n</code></pre> <p>Block where we do not have an exact match on first name </p><pre><code>import splink.postgres.blocking_rule_library as brl\nbrl.not_(brl.exact_match_rule(\"first_name\"))\n</code></pre> <p>Returns:</p> Name Type Description <code>BlockingRule</code> <code>BlockingRule</code> <p>A new BlockingRule with the merged SQL condition</p> Source code in <code>splink/blocking_rule_composition.py</code> <pre><code>def not_(*brls: BlockingRule | dict | str, salting_partitions: int = 1) -&gt; BlockingRule:\n    \"\"\"Invert a BlockingRule using \"NOT\".\n\n    Returns a BlockingRule with the same SQL condition as the input,\n    but prefixed with \"NOT\".\n\n    Args:\n        *brls (BlockingRule | dict | str): BlockingRules or\n            blocking rules in the string/dictionary format.\n        salting_partitions (optional, int): Whether to add salting\n            to the blocking rule. More information on salting can\n            be found within the docs. Salting is only valid for Spark.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Block where we do *not* have an exact match on first name\n            ``` python\n            import splink.duckdb.blocking_rule_library as brl\n            brl.not_(brl.exact_match_rule(\"first_name\"))\n            ```\n        === \":simple-apachespark: Spark\"\n            Block where we do *not* have an exact match on first name\n            ``` python\n            import splink.spark.blocking_rule_library as brl\n            brl.not_(brl.exact_match_rule(\"first_name\"))\n            ```\n        === \":simple-amazonaws: Athena\"\n            Block where we do *not* have an exact match on first name\n            ``` python\n            import splink.athena.blocking_rule_library as brl\n            brl.not_(brl.exact_match_rule(\"first_name\"))\n            ```\n        === \":simple-sqlite: SQLite\"\n            Block where we do *not* have an exact match on first name\n            ``` python\n            import splink.sqlite.blocking_rule_library as brl\n            brl.not_(brl.exact_match_rule(\"first_name\"))\n            ```\n        === \"PostgreSQL\"\n            Block where we do *not* have an exact match on first name\n            ``` python\n            import splink.postgres.blocking_rule_library as brl\n            brl.not_(brl.exact_match_rule(\"first_name\"))\n            ```\n\n    Returns:\n        BlockingRule: A new BlockingRule with the merged\n            SQL condition\n    \"\"\"\n    if len(brls) == 0:\n        raise TypeError(\"You must provide at least one BlockingRule\")\n    elif len(brls) &gt; 1:\n        warnings.warning(\n            \"More than one BlockingRule entered for `NOT` composition. \"\n            \"This function only accepts one argument and will only use your \"\n            \"first BlockingRule.\",\n            SyntaxWarning,\n            stacklevel=2,\n        )\n\n    brls, sql_dialect, salt = _parse_blocking_rules(*brls)\n    br = brls[0]\n    blocking_rule = f\"NOT ({br.blocking_rule_sql})\"\n\n    br_dict = {\n        \"blocking_rule\": blocking_rule,\n        \"sql_dialect\": sql_dialect,\n    }\n\n    if salting_partitions &gt; 1:\n        salt = salting_partitions\n    if salt &gt; 1:\n        br_dict[\"salting_partitions\"] = salt\n\n    return blocking_rule_to_obj(br_dict)\n</code></pre>","tags":["API","blocking rule"]},{"location":"blocking_rule_composition.html#splink.blocking_rule_composition.or_","title":"<code>or_(*brls, salting_partitions=1)</code>","text":"<p>Merge BlockingRules using logical \"OR\".</p> <p>Merge multiple BlockingRules into a single BlockingRule by merging their SQL conditions using a logical \"OR\".</p> <p>Parameters:</p> Name Type Description Default <code>*brls</code> <code>BlockingRule | dict | str</code> <p>BlockingRules or blocking rules in the string/dictionary format.</p> <code>()</code> <code>salting_partitions</code> <code>(optional, int)</code> <p>Whether to add salting to the blocking rule. More information on salting can be found within the docs. Salting is only valid for Spark.</p> <code>1</code> <p>Examples:</p>  DuckDB Spark Athena SQLitePostgreSQL <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.or_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\"\n)\n</code></pre> <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.spark.blocking_rule_library as brl\nbrl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column, with additional salting (spark exclusive) <pre><code>import splink.spark.blocking_rule_library as brl\nbrl.or_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n    salting_partitions=5\n)\n</code></pre> <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.athena.blocking_rule_library as brl\nbrl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.athena.blocking_rule_library as brl\nbrl.or_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.sqlite.blocking_rule_library as brl\nbrl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.sqlite.blocking_rule_library as brl\nbrl.or_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Simple exact rule composition with an <code>OR</code> clause </p><pre><code>import splink.postgres.blocking_rule_library as brl\nbrl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n</code></pre> Composing a custom rule with an exact match on name and the year from a date of birth column <pre><code>import splink.postgres.blocking_rule_library as brl\nbrl.or_(\n    brl.exact_match_rule(\"first_name\"),\n    \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n)\n</code></pre> <p>Returns:</p> Name Type Description <code>BlockingRule</code> <code>BlockingRule</code> <p>A new BlockingRule with the merged SQL condition</p> Source code in <code>splink/blocking_rule_composition.py</code> <pre><code>def or_(\n    *brls: BlockingRule | dict | str,\n    salting_partitions: int = 1,\n) -&gt; BlockingRule:\n    \"\"\"Merge BlockingRules using logical \"OR\".\n\n    Merge multiple BlockingRules into a single BlockingRule by\n    merging their SQL conditions using a logical \"OR\".\n\n\n    Args:\n        *brls (BlockingRule | dict | str): BlockingRules or\n            blocking rules in the string/dictionary format.\n        salting_partitions (optional, int): Whether to add salting\n            to the blocking rule. More information on salting can\n            be found within the docs. Salting is only valid for Spark.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.duckdb.blocking_rule_library as brl\n            brl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.duckdb.blocking_rule_library as brl\n            brl.or_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            )\n            ```\n        === \":simple-apachespark: Spark\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.spark.blocking_rule_library as brl\n            brl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column, with additional salting (spark exclusive)\n            ``` python\n            import splink.spark.blocking_rule_library as brl\n            brl.or_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n                salting_partitions=5\n            )\n            ```\n        === \":simple-amazonaws: Athena\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.athena.blocking_rule_library as brl\n            brl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.athena.blocking_rule_library as brl\n            brl.or_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.sqlite.blocking_rule_library as brl\n            brl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.sqlite.blocking_rule_library as brl\n            brl.or_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n        === \"PostgreSQL\"\n            Simple exact rule composition with an `OR` clause\n            ``` python\n            import splink.postgres.blocking_rule_library as brl\n            brl.or_(brl.exact_match_rule(\"first_name\"), brl.exact_match_rule(\"surname\"))\n            ```\n            Composing a custom rule with an exact match on name and the year\n            from a date of birth column\n            ``` python\n            import splink.postgres.blocking_rule_library as brl\n            brl.or_(\n                brl.exact_match_rule(\"first_name\"),\n                \"substr(l.dob,1,4) = substr(r.dob,1,4)\",\n            )\n            ```\n\n    Returns:\n        BlockingRule: A new BlockingRule with the merged\n            SQL condition\n    \"\"\"\n    return _br_merge(\n        *brls,\n        clause=\"OR\",\n        salting_partitions=salting_partitions,\n    )\n</code></pre>","tags":["API","blocking rule"]},{"location":"blocking_rule_library.html","title":"Blocking Rule Library","text":"","tags":["API","blocking rule"]},{"location":"blocking_rule_library.html#documentation-for-blocking_rules_library","title":"Documentation for <code>blocking_rules_library</code>","text":"<p>The <code>blocking_rules_library</code> contains a series of pre-made blocking rules available for use in the construction of blocking rule strategies and em training blocks as described in this topic guide.</p> <p>These conform to a more performant standard that is outlined in detail here.</p> <p>The detailed API for each of these are outlined below.</p>","tags":["API","blocking rule"]},{"location":"blocking_rule_library.html#blocking-rule-apis","title":"Blocking Rule APIs","text":"<p>The <code>block_on</code> function generates blocking rules that facilitate efficient equi-joins based on the columns or SQL statements specified in the col_names argument. When multiple columns or SQL snippets are provided, the function generates a compound blocking rule, connecting individual match conditions with \"AND\" clauses.</p> <p>This function is designed for scenarios where you aim to achieve efficient yet straightforward blocking conditions based on one or more columns or SQL snippets.</p> <p>For more information on the intended use cases of <code>block_on</code>, please see the following discussion.</p> <p>Further information on equi-join conditions can be found here</p> <p>This function acts as a shorthand alias for the <code>brl.and_</code> syntax: </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.and_(brl.exact_match_rule, brl.exact_match_rule, ...)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>col_names</code> <code>list[str]</code> <p>A list of input columns or sql conditions you wish to create blocks on.</p> required <code>salting_partitions</code> <code>(optional, int)</code> <p>Whether to add salting to the blocking rule. More information on salting can be found within the docs. Salting is only valid for Spark.</p> <code>1</code> <p>Examples:</p>  DuckDB Spark Athena SQLitePostgreSQL <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblock_on(\"first_name\")  # check for exact matches on first name\nsql = \"substr(surname,1,2)\"\nblock_on([sql, \"surname\"])\n</code></pre> <pre><code>from splink.spark.blocking_rule_library import block_on\nblock_on(\"first_name\")  # check for exact matches on first name\nsql = \"substr(surname,1,2)\"\nblock_on([sql, \"surname\"], salting_partitions=1)\n</code></pre> <pre><code>from splink.athena.blocking_rule_library import block_on\nblock_on(\"first_name\")  # check for exact matches on first name\nsql = \"substr(surname,1,2)\"\nblock_on([sql, \"surname\"])\n</code></pre> <pre><code>from splink.sqlite.blocking_rule_library import block_on\nblock_on(\"first_name\")  # check for exact matches on first name\nsql = \"substr(surname,1,2)\"\nblock_on([sql, \"surname\"])\n</code></pre> <pre><code>from splink.postgres.blocking_rule_library import block_on\nblock_on(\"first_name\")  # check for exact matches on first name\nsql = \"substr(surname,1,2)\"\nblock_on([sql, \"surname\"])\n</code></pre> Source code in <code>splink/blocking_rules_library.py</code> <pre><code>def block_on(\n    _exact_match,\n    col_names: list[str],\n    salting_partitions: int = 1,\n) -&gt; BlockingRule:\n    \"\"\"The `block_on` function generates blocking rules that facilitate\n    efficient equi-joins based on the columns or SQL statements\n    specified in the col_names argument. When multiple columns or\n    SQL snippets are provided, the function generates a compound\n    blocking rule, connecting individual match conditions with\n    \"AND\" clauses.\n\n    This function is designed for scenarios where you aim to achieve\n    efficient yet straightforward blocking conditions based on one\n    or more columns or SQL snippets.\n\n    For more information on the intended use cases of `block_on`, please see\n    [the following discussion](https://github.com/moj-analytical-services/splink/issues/1376).\n\n    Further information on equi-join conditions can be found\n    [here](https://moj-analytical-services.github.io/splink/topic_guides/blocking/performance.html)\n\n    This function acts as a shorthand alias for the `brl.and_` syntax:\n    ```py\n    import splink.duckdb.blocking_rule_library as brl\n    brl.and_(brl.exact_match_rule, brl.exact_match_rule, ...)\n    ```\n\n    Args:\n        col_names (list[str]): A list of input columns or sql conditions\n            you wish to create blocks on.\n        salting_partitions (optional, int): Whether to add salting\n            to the blocking rule. More information on salting can\n            be found within the docs. Salting is only valid for Spark.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            from splink.duckdb.blocking_rule_library import block_on\n            block_on(\"first_name\")  # check for exact matches on first name\n            sql = \"substr(surname,1,2)\"\n            block_on([sql, \"surname\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            from splink.spark.blocking_rule_library import block_on\n            block_on(\"first_name\")  # check for exact matches on first name\n            sql = \"substr(surname,1,2)\"\n            block_on([sql, \"surname\"], salting_partitions=1)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            from splink.athena.blocking_rule_library import block_on\n            block_on(\"first_name\")  # check for exact matches on first name\n            sql = \"substr(surname,1,2)\"\n            block_on([sql, \"surname\"])\n            ```\n        === \":simple-sqlite: SQLite\"\n            ``` python\n            from splink.sqlite.blocking_rule_library import block_on\n            block_on(\"first_name\")  # check for exact matches on first name\n            sql = \"substr(surname,1,2)\"\n            block_on([sql, \"surname\"])\n            ```\n        === \"PostgreSQL\"\n            ``` python\n            from splink.postgres.blocking_rule_library import block_on\n            block_on(\"first_name\")  # check for exact matches on first name\n            sql = \"substr(surname,1,2)\"\n            block_on([sql, \"surname\"])\n            ```\n    \"\"\"  # noqa: E501\n\n    col_names = ensure_is_list(col_names)\n    em_rules = [_exact_match(col) for col in col_names]\n    return and_(*em_rules, salting_partitions=salting_partitions)\n</code></pre> <p>Represents an exact match blocking rule.</p> <p>DEPRECATED: <code>exact_match_rule</code> is deprecated. Please use <code>block_on</code> instead, which acts as a wrapper with additional functionality.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name, or a str represent a sql statement you'd like to match on. For example, <code>surname</code> or <code>\"substr(surname,1,2)\"</code> are both valid.</p> required <code>salting_partitions</code> <code>(optional, int)</code> <p>Whether to add salting to the blocking rule. More information on salting can be found within the docs. Salting is currently only valid for Spark.</p> <code>None</code> Source code in <code>splink/blocking_rules_library.py</code> <pre><code>def exact_match_rule(\n    col_name: str,\n    _sql_dialect: str,\n    salting_partitions: int = None,\n) -&gt; BlockingRule:\n    \"\"\"Represents an exact match blocking rule.\n\n    **DEPRECATED:**\n    `exact_match_rule` is deprecated. Please use `block_on`\n    instead, which acts as a wrapper with additional functionality.\n\n    Args:\n        col_name (str): Input column name, or a str represent a sql\n            statement you'd like to match on. For example, `surname` or\n            `\"substr(surname,1,2)\"` are both valid.\n        salting_partitions (optional, int): Whether to add salting\n            to the blocking rule. More information on salting can\n            be found within the docs. Salting is currently only valid\n            for Spark.\n    \"\"\"\n    warnings.warn(\n        \"`exact_match_rule` is deprecated; use `block_on`\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    syntax_tree = sqlglot.parse_one(col_name, read=_sql_dialect)\n\n    l_col = add_quotes_and_table_prefix(syntax_tree, \"l\").sql(_sql_dialect)\n    r_col = add_quotes_and_table_prefix(syntax_tree, \"r\").sql(_sql_dialect)\n\n    blocking_rule = f\"{l_col} = {r_col}\"\n\n    return blocking_rule_to_obj(\n        {\n            \"blocking_rule\": blocking_rule,\n            \"salting_partitions\": salting_partitions,\n            \"sql_dialect\": _sql_dialect,\n        }\n    )\n</code></pre>","tags":["API","blocking rule"]},{"location":"comparison.html","title":"Comparison","text":"","tags":["API"]},{"location":"comparison.html#documentation-for-comparison-object","title":"Documentation for <code>Comparison</code> object","text":"<p>Each Comparison defines how data from one or more input columns is compared to assess its similarity.</p> <p>For example, one Comparison may represent how similarity is assessed for a person's date of birth.  Others may represent the comparison of a person's name or location.</p> <p>The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name.</p> <p>A linking model thus usually contains several Comparisons.</p> <p>As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model.  This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field.</p> <p>Each Comparison contains two or more <code>ComparisonLevel</code>s which define the gradations of similarity between the input columns within the Comparison.</p> <p>For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons.</p> <p>To summarise:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> Source code in <code>splink/comparison.py</code> <pre><code>class Comparison:\n    \"\"\"Each Comparison defines how data from one or more input columns is\n    compared to assess its similarity.\n\n    For example, one Comparison may represent how similarity is assessed for a\n    person's date of birth.  Others may represent the comparison of a person's name or\n    location.\n\n    The method used to assess similarity will depend on the type of data -\n    for instance, the method used to assess similarity of a company's turnover would\n    be different to the method used to assess the similarity of a person's first name.\n\n    A linking model thus usually contains several Comparisons.\n\n    As far as possible, Comparisons should be configured to satisfy the assumption of\n    independece conditional on the true match status, a key assumption of the Fellegi\n    Sunter probabilistic linkage model.  This would be broken, for example, if a model\n    contained one Comparison for city, and another for postcode. Instead, in this\n    example, a single comparison should be modelled, which may to capture similarity\n    taking account of both the city and postcode field.\n\n    Each Comparison contains two or more `ComparisonLevel`s which define the gradations\n    of similarity between the input columns within the Comparison.\n\n    For example, for the date of birth Comparison there may be a ComparisonLevel for an\n    exact match, another for a one-character difference, and another for all other\n    comparisons.\n\n    To summarise:\n\n    ```\n    Data Linking Model\n    \u251c\u2500-- Comparison: Date of birth\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match\n    \u2502    \u251c\u2500-- ComparisonLevel: One character difference\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- Comparison: Name\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n    \u2502    \u251c\u2500-- etc.\n    ```\n\n    \"\"\"\n\n    def __init__(self, comparison_dict, settings_obj: Settings = None):\n        # Protected because we don't want to modify\n        self._comparison_dict = comparison_dict\n        comparison_level_list = comparison_dict[\"comparison_levels\"]\n        self.comparison_levels: list[ComparisonLevel] = []\n\n        # If comparison_levels are already of type ComparisonLevel, register\n        # the settings object on them\n        # otherwise turn the dictionaries into ComparisonLevel\n\n        for cl in comparison_level_list:\n            if isinstance(cl, ComparisonLevel):\n                cl.comparison = self\n            elif settings_obj is None:\n                cl = ComparisonLevel(cl, self)\n            else:\n                cl = ComparisonLevel(cl, self, sql_dialect=settings_obj._sql_dialect)\n\n            self.comparison_levels.append(cl)\n\n        self._settings_obj: Optional[Settings] = settings_obj\n\n        # Assign comparison vector values starting at highest level, count down to 0\n        num_levels = self._num_levels\n        counter = num_levels - 1\n\n        for level in self.comparison_levels:\n            if level.is_null_level:\n                level._comparison_vector_value = -1\n                level._max_level = False\n            else:\n                level._comparison_vector_value = counter\n                if counter == num_levels - 1:\n                    level._max_level = True\n                else:\n                    level._max_level = False\n                counter -= 1\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the Comparison which is independent\n        of the original e.g. modifying the copy will not affect the original.\n        This method implements ensures the Comparison can be deepcopied.\n        \"\"\"\n        cc = Comparison(self.as_dict(), self._settings_obj)\n        return cc\n\n    @property\n    def _num_levels(self):\n        return len([cl for cl in self.comparison_levels if not cl.is_null_level])\n\n    @property\n    def _comparison_levels_excluding_null(self):\n        return [cl for cl in self.comparison_levels if not cl.is_null_level]\n\n    @property\n    def _gamma_prefix(self):\n        return self._settings_obj._gamma_prefix\n\n    @property\n    def _retain_intermediate_calculation_columns(self):\n        return self._settings_obj._retain_intermediate_calculation_columns\n\n    @property\n    def _bf_column_name(self):\n        return f\"{self._settings_obj._bf_prefix}{self._output_column_name}\".replace(\n            \" \", \"_\"\n        )\n\n    @property\n    def _has_null_level(self):\n        return any([cl.is_null_level for cl in self.comparison_levels])\n\n    @property\n    def _bf_tf_adj_column_name(self):\n        bf = self._settings_obj._bf_prefix\n        tf = self._settings_obj._tf_prefix\n        cc_name = self._output_column_name\n        return f\"{bf}{tf}adj_{cc_name}\".replace(\" \", \"_\")\n\n    @property\n    def _has_tf_adjustments(self):\n        return any([cl._has_tf_adjustments for cl in self.comparison_levels])\n\n    @property\n    def _case_statement(self):\n        sqls = [\n            cl._when_then_comparison_vector_value_sql for cl in self.comparison_levels\n        ]\n        sql = \" \".join(sqls)\n        sql = f\"CASE {sql} END as {self._gamma_column_name}\"\n\n        return sql\n\n    @property\n    def _input_columns_used_by_case_statement(self):\n        cols = []\n        for cl in self.comparison_levels:\n            cols.extend(cl._input_columns_used_by_sql_condition)\n\n        # dedupe_preserving_order on input column\n        already_observed = []\n        deduped_cols = []\n        for col in cols:\n            if col.input_name not in already_observed:\n                deduped_cols.append(col)\n                already_observed.append(col.input_name)\n\n        return deduped_cols\n\n    @property\n    def _output_column_name(self):\n        if \"output_column_name\" in self._comparison_dict:\n            return self._comparison_dict[\"output_column_name\"]\n        else:\n            cols = self._input_columns_used_by_case_statement\n            cols = [c.input_name for c in cols]\n            if len(cols) == 1:\n                return cols[0]\n            else:\n                return f\"custom_{'_'.join(cols)}\"\n\n    @property\n    def _comparison_description(self):\n        if \"comparison_description\" in self._comparison_dict:\n            return self._comparison_dict[\"comparison_description\"]\n        else:\n            return self._output_column_name\n\n    @property\n    def _gamma_column_name(self):\n        return f\"{self._gamma_prefix}{self._output_column_name}\".replace(\" \", \"_\")\n\n    @property\n    def _tf_adjustment_input_col_names(self):\n        cols = [cl._tf_adjustment_input_column_name for cl in self.comparison_levels]\n        cols = [c for c in cols if c]\n\n        return cols\n\n    @property\n    def _columns_to_select_for_blocking(self):\n        cols = []\n        for cl in self.comparison_levels:\n            cols.extend(cl._columns_to_select_for_blocking)\n\n        return dedupe_preserving_order(cols)\n\n    @property\n    def _columns_to_select_for_comparison_vector_values(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r)\n\n        output_cols.append(self._case_statement)\n\n        for cl in self.comparison_levels:\n            if cl._has_tf_adjustments:\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _columns_to_select_for_bayes_factor_parts(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r)\n\n        output_cols.append(self._gamma_column_name)\n\n        for cl in self.comparison_levels:\n            if (\n                cl._has_tf_adjustments\n                and self._settings_obj._retain_intermediate_calculation_columns\n            ):\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r)\n\n        # Bayes factor case when statement\n        sqls = [cl._bayes_factor_sql for cl in self.comparison_levels]\n        sql = \" \".join(sqls)\n        sql = f\"CASE {sql} END as {self._bf_column_name} \"\n        output_cols.append(sql)\n\n        # tf adjustment case when statement\n\n        if self._has_tf_adjustments:\n            sqls = [cl._tf_adjustment_sql for cl in self.comparison_levels]\n            sql = \" \".join(sqls)\n            sql = f\"CASE {sql} END as {self._bf_tf_adj_column_name} \"\n            output_cols.append(sql)\n        output_cols.append(self._gamma_column_name)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _columns_to_select_for_predict(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r)\n\n        if (\n            self._settings_obj._training_mode\n            or self._settings_obj._retain_matching_columns\n        ):\n            output_cols.append(self._gamma_column_name)\n\n        for cl in self.comparison_levels:\n            if (\n                cl._has_tf_adjustments\n                and self._settings_obj._retain_intermediate_calculation_columns\n            ):\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r)\n\n        for _col in input_cols:\n            if self._settings_obj._retain_intermediate_calculation_columns:\n                output_cols.extend(self._match_weight_columns_to_multiply)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _match_weight_columns_to_multiply(self):\n        cols = []\n        cols.append(self._bf_column_name)\n        if self._has_tf_adjustments:\n            cols.append(self._bf_tf_adj_column_name)\n        return cols\n\n    @property\n    def _term_frequency_columns(self):\n        cols = set()\n        for cl in self.comparison_levels:\n            cols.add(cl.tf_adjustment_input_col_name)\n        return list(cols)\n\n    def as_dict(self):\n        d = {\n            \"output_column_name\": self._output_column_name,\n            \"comparison_levels\": [cl.as_dict() for cl in self.comparison_levels],\n        }\n        if \"comparison_description\" in self._comparison_dict:\n            d[\"comparison_description\"] = self._comparison_dict[\n                \"comparison_description\"\n            ]\n        return d\n\n    def _as_completed_dict(self):\n        return {\n            \"column_name\": self._output_column_name,\n            \"comparison_levels\": [\n                cl._as_completed_dict() for cl in self.comparison_levels\n            ],\n            \"input_columns_used_by_case_statement\": [\n                c.input_name for c in self._input_columns_used_by_case_statement\n            ],\n        }\n\n    @property\n    def _has_estimated_m_values(self):\n        return all(cl._has_estimated_m_values for cl in self.comparison_levels)\n\n    @property\n    def _has_estimated_u_values(self):\n        return all(cl._has_estimated_u_values for cl in self.comparison_levels)\n\n    @property\n    def _all_m_are_trained(self):\n        return all(cl._m_is_trained for cl in self.comparison_levels)\n\n    @property\n    def _all_u_are_trained(self):\n        return all(cl._u_is_trained for cl in self.comparison_levels)\n\n    @property\n    def _some_m_are_trained(self):\n        return any(cl._m_is_trained for cl in self._comparison_levels_excluding_null)\n\n    @property\n    def _some_u_are_trained(self):\n        return any(cl._u_is_trained for cl in self._comparison_levels_excluding_null)\n\n    @property\n    def _is_trained_message(self):\n        messages = []\n        if self._all_m_are_trained and self._all_u_are_trained:\n            return None\n\n        if not self._some_u_are_trained:\n            messages.append(\"no u values are trained\")\n        elif self._some_u_are_trained and not self._all_u_are_trained:\n            messages.append(\"some u values are not trained\")\n\n        if not self._some_m_are_trained:\n            messages.append(\"no m values are trained\")\n        elif self._some_m_are_trained and not self._all_m_are_trained:\n            messages.append(\"some m values are not trained\")\n\n        message = \", \".join(messages)\n        message = f\"    - {self._output_column_name} ({message}).\"\n        return message\n\n    @property\n    def _is_trained(self):\n        return self._all_m_are_trained and self._all_u_are_trained\n\n    @property\n    def _as_detailed_records(self):\n        records = []\n        for cl in self.comparison_levels:\n            record = {}\n            record[\"comparison_name\"] = self._output_column_name\n            record = {**record, **cl._as_detailed_record}\n            records.append(record)\n        return records\n\n    @property\n    def _parameter_estimates_as_records(self):\n        records = []\n        for cl in self.comparison_levels:\n            new_records = cl._parameter_estimates_as_records\n            for r in new_records:\n                r[\"comparison_name\"] = self._output_column_name\n            records.extend(new_records)\n\n        return records\n\n    def _get_comparison_level_by_comparison_vector_value(\n        self, value\n    ) -&gt; ComparisonLevel:\n        for cl in self.comparison_levels:\n            if cl._comparison_vector_value == value:\n                return cl\n        raise ValueError(f\"No comparison level with comparison vector value {value}\")\n\n    def __repr__(self):\n        return (\n            f\"&lt;Comparison {self._comparison_description} with \"\n            f\"{self._num_levels} levels at {hex(id(self))}&gt;\"\n        )\n\n    @property\n    def _not_trained_messages(self):\n        msgs = []\n\n        cname = self._output_column_name\n\n        header = f\"Comparison: '{cname}':\\n\"\n\n        msg_template = \"{header}    {m_or_u} values not fully trained\"\n\n        if not self._all_m_are_trained:\n            msgs.append(msg_template.format(header=header, m_or_u=\"m\"))\n        if not self._all_u_are_trained:\n            msgs.append(msg_template.format(header=header, m_or_u=\"u\"))\n\n        return msgs\n\n    @property\n    def _comparison_level_description_list(self):\n        cl_template = \"    - '{label}' with SQL rule: {sql}\\n\"\n\n        comp_levels = [\n            cl_template.format(\n                cvv=cl._comparison_vector_value,\n                label=cl.label_for_charts,\n                sql=cl.sql_condition,\n            )\n            for cl in self.comparison_levels\n        ]\n        comp_levels = \"\".join(comp_levels)\n        return comp_levels\n\n    @property\n    def _human_readable_description_succinct(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name for c in self._input_columns_used_by_case_statement]\n        )\n\n        comp_levels = self._comparison_level_description_list\n\n        if \"comparison_description\" in self._comparison_dict:\n            main_desc = (\n                f\"of {input_cols}\\nDescription: '{self._comparison_description}'\"\n            )\n        else:\n            main_desc = f\"of {input_cols}\"\n\n        desc = f\"Comparison {main_desc}\\nComparison levels:\\n{comp_levels}\"\n        return desc\n\n    @property\n    def human_readable_description(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name for c in self._input_columns_used_by_case_statement]\n        )\n\n        comp_levels = self._comparison_level_description_list\n\n        if \"comparison_description\" in self._comparison_dict:\n            main_desc = f\"'{self._comparison_description}' of {input_cols}\"\n        else:\n            main_desc = f\"of {input_cols}\"\n\n        desc = (\n            f\"Comparison {main_desc}.\\n\"\n            \"Similarity is assessed using the following \"\n            f\"ComparisonLevels:\\n{comp_levels}\"\n        )\n\n        return desc\n\n    def match_weights_chart(self, as_dict=False):\n        \"\"\"Display a chart of comparison levels of the comparison\"\"\"\n        from .charts import comparison_match_weights_chart\n\n        records = self._as_detailed_records\n        return comparison_match_weights_chart(records, as_dict=as_dict)\n</code></pre>","tags":["API"]},{"location":"comparison.html#splink.comparison.Comparison.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the Comparison which is independent of the original e.g. modifying the copy will not affect the original. This method implements ensures the Comparison can be deepcopied.</p> Source code in <code>splink/comparison.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the Comparison which is independent\n    of the original e.g. modifying the copy will not affect the original.\n    This method implements ensures the Comparison can be deepcopied.\n    \"\"\"\n    cc = Comparison(self.as_dict(), self._settings_obj)\n    return cc\n</code></pre>","tags":["API"]},{"location":"comparison.html#splink.comparison.Comparison.match_weights_chart","title":"<code>match_weights_chart(as_dict=False)</code>","text":"<p>Display a chart of comparison levels of the comparison</p> Source code in <code>splink/comparison.py</code> <pre><code>def match_weights_chart(self, as_dict=False):\n    \"\"\"Display a chart of comparison levels of the comparison\"\"\"\n    from .charts import comparison_match_weights_chart\n\n    records = self._as_detailed_records\n    return comparison_match_weights_chart(records, as_dict=as_dict)\n</code></pre>","tags":["API"]},{"location":"comparison_helpers.html","title":"Comparison Helpers","text":"","tags":["API","comparisons"]},{"location":"comparison_helpers.html#documentation-for-comparison_helpers-functions","title":"Documentation for <code>comparison_helpers</code> functions","text":"<p>The <code>comparison_helpers</code> functions are a set of functions to help users create better comparisons by helping them understand string comparators (fuzzy matching) and phonetic matching.</p> <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#comparison-helpers-api","title":"Comparison Helpers API","text":"","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.comparator_score","title":"<code>comparator_score(str1, str2, decimal_places=2)</code>","text":"<p>Helper function to give the similarity between two strings for the string comparators in splink.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nch.comparator_score(\"Richard\", \"iRchard\")\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def comparator_score(str1, str2, decimal_places=2):\n    \"\"\"Helper function to give the similarity between two strings for\n    the string comparators in splink.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        ch.comparator_score(\"Richard\", \"iRchard\")\n        ```\n    \"\"\"\n    con = duckdb.connect()\n\n    sql = f\"\"\"\n        select\n        '{str1}' as string1,\n        '{str2}' as string2,\n        {comparator_cols_sql.format(\n            comparison1 = 'string1',\n            comparison2 = 'string2',\n            decimal_places=decimal_places\n        )}\n    \"\"\"\n    return con.execute(sql).fetch_df()\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.comparator_score_chart","title":"<code>comparator_score_chart(list, col1, col2)</code>","text":"<p>Helper function returning a heatmap showing the sting similarity scores and string distances for a list of strings.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nlist = {\n        \"string1\": [\"Stephen\", \"Stephen\", \"Stephen\"],\n        \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n        }\n\nch.comparator_score_chart(list, \"string1\", \"string2\")\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def comparator_score_chart(list, col1, col2):\n    \"\"\"Helper function returning a heatmap showing the sting similarity\n    scores and string distances for a list of strings.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        list = {\n                \"string1\": [\"Stephen\", \"Stephen\", \"Stephen\"],\n                \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n                }\n\n        ch.comparator_score_chart(list, \"string1\", \"string2\")\n        ```\n    \"\"\"\n\n    df = comparator_score_df(list, col1, col2)\n\n    df[\"strings_to_compare\"] = df[\"string1\"] + \", \" + df[\"string2\"]\n\n    df_long = pd.melt(\n        df,\n        id_vars=[\"strings_to_compare\"],\n        value_vars=[\n            \"jaro_similarity\",\n            \"jaro_winkler_similarity\",\n            \"jaccard_similarity\",\n            \"levenshtein_distance\",\n            \"damerau_levenshtein_distance\",\n        ],\n        var_name=\"comparator\",\n        value_name=\"score\",\n    )\n\n    similarity_df = df_long[df_long[\"comparator\"].str.contains(\"similarity\")]\n    similarity_df.loc[:, \"comparator\"] = similarity_df[\"comparator\"].str.replace(\n        \"_similarity\", \"\"\n    )\n    similarity_records = similarity_df.to_json(orient=\"records\")\n    distance_df = df_long[df_long[\"comparator\"].str.contains(\"distance\")]\n    distance_df.loc[:, \"comparator\"] = distance_df[\"comparator\"].str.replace(\n        \"_distance\", \"\"\n    )\n    distance_records = distance_df.to_json(orient=\"records\")\n\n    return _comparator_score_chart(similarity_records, distance_records)\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.comparator_score_df","title":"<code>comparator_score_df(list, col1, col2, decimal_places=2)</code>","text":"<p>Helper function returning a dataframe showing the string similarity scores and string distances for a list of strings.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nlist = {\n        \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n        \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n        }\n\nch.comparator_score_df(list, \"string1\", \"string2\")\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def comparator_score_df(list, col1, col2, decimal_places=2):\n    \"\"\"Helper function returning a dataframe showing the string similarity\n    scores and string distances for a list of strings.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        list = {\n                \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n                \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n                }\n\n        ch.comparator_score_df(list, \"string1\", \"string2\")\n        ```\n    \"\"\"\n    duckdb.connect()\n\n    list = pd.DataFrame(list)\n\n    sql = f\"\"\"\n        select\n        {col1}, {col2},\n        {comparator_cols_sql.format(\n            comparison1 = col1,\n            comparison2 = col2,\n            decimal_places=decimal_places\n        )},\n        from list\n    \"\"\"\n\n    return duckdb.sql(sql).df()\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.comparator_score_threshold_chart","title":"<code>comparator_score_threshold_chart(list, col1, col2, similarity_threshold=None, distance_threshold=None)</code>","text":"<p>Helper function returning a heatmap showing the sting similarity scores and string distances for a list of strings given a threshold.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nlist = {\n        \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n        \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n        }\n\nch.comparator_score_threshold_chart(data,\n                         \"string1\", \"string2\",\n                         similarity_threshold=0.8,\n                         distance_threshold=2)\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def comparator_score_threshold_chart(\n    list, col1, col2, similarity_threshold=None, distance_threshold=None\n):\n    \"\"\"Helper function returning a heatmap showing the sting similarity\n    scores and string distances for a list of strings given a threshold.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        list = {\n                \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n                \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n                }\n\n        ch.comparator_score_threshold_chart(data,\n                                 \"string1\", \"string2\",\n                                 similarity_threshold=0.8,\n                                 distance_threshold=2)\n        ```\n    \"\"\"\n    df = comparator_score_df(list, col1, col2)\n\n    df[\"strings_to_compare\"] = df[\"string1\"] + \", \" + df[\"string2\"]\n\n    df_long = pd.melt(\n        df,\n        id_vars=[\"strings_to_compare\"],\n        value_vars=[\n            \"jaro_similarity\",\n            \"jaro_winkler_similarity\",\n            \"jaccard_similarity\",\n            \"levenshtein_distance\",\n            \"damerau_levenshtein_distance\",\n        ],\n        var_name=\"comparator\",\n        value_name=\"score\",\n    )\n\n    similarity_df = df_long.loc[df_long[\"comparator\"].str.contains(\"similarity\"), :]\n    similarity_df[\"comparator\"] = similarity_df[\"comparator\"].str.replace(\n        \"_similarity\", \"\"\n    )\n    similarity_records = similarity_df.to_json(orient=\"records\")\n    distance_df = df_long.loc[df_long[\"comparator\"].str.contains(\"distance\"), :]\n    distance_df[\"comparator\"] = distance_df[\"comparator\"].str.replace(\"_distance\", \"\")\n    distance_records = distance_df.to_json(orient=\"records\")\n\n    return _comparator_score_threshold_chart(\n        similarity_records, distance_records, similarity_threshold, distance_threshold\n    )\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.phonetic_match_chart","title":"<code>phonetic_match_chart(list, col1, col2)</code>","text":"<p>Helper function returning a heatmap showing the phonetic transform and matches for a list of strings given a threshold.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nlist = {\n        \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n        \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n        }\n\nch.comparator_score_threshold_chart(list,\n                         \"string1\", \"string2\",\n                         similarity_threshold=0.8,\n                         distance_threshold=2)\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def phonetic_match_chart(list, col1, col2):\n    \"\"\"Helper function returning a heatmap showing the phonetic transform and\n    matches for a list of strings given a threshold.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        list = {\n                \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n                \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n                }\n\n        ch.comparator_score_threshold_chart(list,\n                                 \"string1\", \"string2\",\n                                 similarity_threshold=0.8,\n                                 distance_threshold=2)\n        ```\n    \"\"\"\n\n    df = phonetic_transform_df(list, \"string1\", \"string2\")\n\n    df[\"strings_to_compare\"] = df[\"string1\"] + \", \" + df[\"string2\"]\n\n    df_long = pd.melt(\n        df,\n        id_vars=[\"strings_to_compare\"],\n        value_vars=[\n            \"metaphone\",\n            \"dmetaphone\",\n            \"soundex\",\n        ],\n        var_name=\"phonetic\",\n        value_name=\"transform\",\n    )\n    df_long[\"match\"] = df_long[\"transform\"].apply(lambda x: x[0] == x[1])\n\n    records = df_long.to_json(orient=\"records\")\n\n    return _phonetic_match_chart(records)\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.phonetic_transform","title":"<code>phonetic_transform(string)</code>","text":"<p>Helper function to give the phonetic transformation of two strings with Soundex, Metaphone and Double Metaphone.</p> <p>Examples:</p> <pre><code>phonetic_transform(\"Richard\", \"iRchard\")\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def phonetic_transform(string):\n    \"\"\"Helper function to give the phonetic transformation of two strings with\n    Soundex, Metaphone and Double Metaphone.\n\n    Examples:\n        ```py\n        phonetic_transform(\"Richard\", \"iRchard\")\n        ```\n    \"\"\"\n    transforms = {}\n\n    # Soundex Transform\n    soundex_transform = phonetics.soundex(string)\n    transforms[\"soundex\"] = soundex_transform\n\n    # Metaphone distance\n    metaphone_transform = phonetics.metaphone(string)\n    transforms[\"metaphone\"] = metaphone_transform\n\n    # Metaphone distance\n    dmetaphone_transform = phonetics.dmetaphone(string)\n    transforms[\"dmetaphone\"] = dmetaphone_transform\n\n    return transforms\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_helpers.html#splink.comparison_helpers.phonetic_transform_df","title":"<code>phonetic_transform_df(list, col1, col2)</code>","text":"<p>Helper function returning a dataframe showing the phonetic transforms for a list of strings.</p> <p>Examples:</p> <pre><code>import splink.comparison_helpers as ch\n\nlist = {\n        \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n        \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n        }\n\nch.phonetic_match_chart(list, \"string1\", \"string2\")\n</code></pre> Source code in <code>splink/comparison_helpers.py</code> <pre><code>def phonetic_transform_df(list, col1, col2):\n    \"\"\"Helper function returning a dataframe showing the phonetic transforms\n    for a list of strings.\n\n    Examples:\n        ```py\n        import splink.comparison_helpers as ch\n\n        list = {\n                \"string1\": [\"Stephen\", \"Stephen\",\"Stephen\"],\n                \"string2\": [\"Stephen\", \"Steven\", \"Stephan\"],\n                }\n\n        ch.phonetic_match_chart(list, \"string1\", \"string2\")\n        ```\n    \"\"\"\n\n    df = pd.DataFrame(list)\n\n    df[f\"soundex_{col1}\"] = df.apply(lambda row: phonetics.soundex(row[col1]), axis=1)\n    df[f\"soundex_{col2}\"] = df.apply(lambda row: phonetics.soundex(row[col2]), axis=1)\n    df[f\"metaphone_{col1}\"] = df.apply(\n        lambda row: phonetics.metaphone(row[col1]), axis=1\n    )\n    df[f\"metaphone_{col2}\"] = df.apply(\n        lambda row: phonetics.metaphone(row[col2]), axis=1\n    )\n    df[f\"dmetaphone_{col1}\"] = df.apply(\n        lambda row: phonetics.dmetaphone(row[col1]), axis=1\n    )\n    df[f\"dmetaphone_{col2}\"] = df.apply(\n        lambda row: phonetics.dmetaphone(row[col2]), axis=1\n    )\n\n    df[\"soundex\"] = df.apply(\n        lambda x: [x[f\"soundex_{col1}\"], x[f\"soundex_{col2}\"]], axis=1\n    )\n    df[\"metaphone\"] = df.apply(\n        lambda x: [x[f\"metaphone_{col1}\"], x[f\"metaphone_{col2}\"]], axis=1\n    )\n    df[\"dmetaphone\"] = df.apply(\n        lambda x: [x[f\"dmetaphone_{col1}\"], x[f\"dmetaphone_{col2}\"]], axis=1\n    )\n\n    phonetic_df = df[[col1, col2, \"soundex\", \"metaphone\", \"dmetaphone\"]]\n\n    return phonetic_df\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level.html","title":"Comparison Level","text":"","tags":["API","comparisons"]},{"location":"comparison_level.html#documentation-for-comparisonlevel-object","title":"Documentation for <code>ComparisonLevel</code> object","text":"<p>Each ComparisonLevel defines a gradation (category) of similarity within a <code>Comparison</code>.</p> <p>For example, a <code>Comparison</code> that uses the first_name and surname columns may define three <code>ComparisonLevel</code>s:     An exact match on first name and surname     First name and surname have a JaroWinkler score of above 0.95     All other comparisons</p> <p>The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name.</p> <p>To summarise:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler &gt; 0.95\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- etc.\n</code></pre> Source code in <code>splink/comparison_level.py</code> <pre><code>class ComparisonLevel:\n    \"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a\n    `Comparison`.\n\n    For example, a `Comparison` that uses the first_name and surname columns may\n    define three `ComparisonLevel`s:\n        An exact match on first name and surname\n        First name and surname have a JaroWinkler score of above 0.95\n        All other comparisons\n\n    The method used to assess similarity will depend on the type of data - for\n    instance, the method used to assess similarity of a company's turnover would be\n    different to the method used to assess the similarity of a person's first name.\n\n    To summarise:\n\n    ```\n    Data Linking Model\n    \u251c\u2500-- Comparison: Name\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname\n    \u2502    \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler &gt; 0.95\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- Comparison: Date of birth\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match\n    \u2502    \u251c\u2500-- ComparisonLevel: One character difference\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- etc.\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        level_dict,\n        comparison: Comparison = None,\n        sql_dialect: str = None,\n    ):\n        # Protected, because we don't want to modify the original dict\n        self._level_dict = level_dict\n\n        self.comparison: Comparison = comparison\n        if not hasattr(self, \"_sql_dialect\"):\n            self._sql_dialect = sql_dialect\n\n        self._sql_condition = self._level_dict[\"sql_condition\"]\n        self._is_null_level = self._level_dict_val_else_default(\"is_null_level\")\n        self._tf_adjustment_weight = self._level_dict_val_else_default(\n            \"tf_adjustment_weight\"\n        )\n\n        self._tf_minimum_u_value = self._level_dict_val_else_default(\n            \"tf_minimum_u_value\"\n        )\n\n        # Private values controlled with getter/setter\n        self._m_probability = self._level_dict.get(\"m_probability\")\n        self._u_probability = self._level_dict.get(\"u_probability\")\n\n        # These will be set when the ComparisonLevel is passed into a Comparison\n        self._comparison_vector_value: int = None\n        self._max_level: bool = None\n\n        # Enable the level to 'know' when it's been trained\n        self._trained_m_probabilities: list = []\n        self._trained_u_probabilities: list = []\n        # controls warnings from model training - ensures we only send once\n        self._m_warning_sent = False\n        self._u_warning_sent = False\n\n        self._validate()\n\n    @property\n    def sql_dialect(self):\n        return self._sql_dialect\n\n    @property\n    def is_null_level(self) -&gt; bool:\n        return self._is_null_level\n\n    @property\n    def sql_condition(self) -&gt; str:\n        return self._sql_condition\n\n    def _level_dict_val_else_default(self, key):\n        val = self._level_dict.get(key)\n        if not val:\n            val = default_value_from_schema(key, \"comparison_level\")\n        return val\n\n    @property\n    def _tf_adjustment_input_column(self):\n        val = self._level_dict_val_else_default(\"tf_adjustment_column\")\n        if val:\n            return InputColumn(val, sql_dialect=self.sql_dialect)\n        else:\n            return None\n\n    @property\n    def _tf_adjustment_input_column_name(self):\n        input_column = self._tf_adjustment_input_column\n        if input_column:\n            return input_column.unquote().name\n\n    @property\n    def _has_comparison(self):\n        from .comparison import Comparison\n\n        return isinstance(self.comparison, Comparison)\n\n    @property\n    def m_probability(self):\n        if self.is_null_level:\n            return None\n        if self._m_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return 1e-6\n        if self._m_probability is None and self._has_comparison:\n            vals = _default_m_values(self.comparison._num_levels)\n            return vals[self._comparison_vector_value]\n        return self._m_probability\n\n    @m_probability.setter\n    def m_probability(self, value):\n        if self.is_null_level:\n            raise AttributeError(\"Cannot set m_probability when is_null_level is true\")\n        if value == LEVEL_NOT_OBSERVED_TEXT:\n            cc_n = self.comparison._output_column_name\n            cl_n = self.label_for_charts\n            if not self._m_warning_sent:\n                logger.warning(\n                    \"WARNING:\\n\"\n                    f\"Level {cl_n} on comparison {cc_n} not observed in dataset, \"\n                    \"unable to train m value\\n\"\n                )\n                self._m_warning_sent = True\n\n        self._m_probability = value\n\n    @property\n    def u_probability(self):\n        if self.is_null_level:\n            return None\n        if self._u_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return 1e-6\n        if self._u_probability is None:\n            vals = _default_u_values(self.comparison._num_levels)\n            return vals[self._comparison_vector_value]\n        return self._u_probability\n\n    @u_probability.setter\n    def u_probability(self, value):\n        if self.is_null_level:\n            raise AttributeError(\"Cannot set u_probability when is_null_level is true\")\n        if value == LEVEL_NOT_OBSERVED_TEXT:\n            cc_n = self.comparison._output_column_name\n            cl_n = self.label_for_charts\n            if not self._u_warning_sent:\n                logger.warning(\n                    \"WARNING:\\n\"\n                    f\"Level {cl_n} on comparison {cc_n} not observed in dataset, \"\n                    \"unable to train u value\\n\"\n                )\n                self._u_warning_sent = True\n        self._u_probability = value\n\n    @property\n    def _m_probability_description(self):\n        if self.m_probability is not None:\n            return (\n                \"Amongst matching record comparisons, \"\n                f\"{self.m_probability:.2%} of records are in the \"\n                f\"{self.label_for_charts.lower()} comparison level\"\n            )\n\n    @property\n    def _u_probability_description(self):\n        if self.u_probability is not None:\n            return (\n                \"Amongst non-matching record comparisons, \"\n                f\"{self.u_probability:.2%} of records are in the \"\n                f\"{self.label_for_charts.lower()} comparison level\"\n            )\n\n    def _add_trained_u_probability(self, val, desc=\"no description given\"):\n        self._trained_u_probabilities.append(\n            {\"probability\": val, \"description\": desc, \"m_or_u\": \"u\"}\n        )\n\n    def _add_trained_m_probability(self, val, desc=\"no description given\"):\n        self._trained_m_probabilities.append(\n            {\"probability\": val, \"description\": desc, \"m_or_u\": \"m\"}\n        )\n\n    @property\n    def _has_estimated_u_values(self):\n        if self.is_null_level:\n            return True\n        vals = [r[\"probability\"] for r in self._trained_u_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        return len(vals) &gt; 0\n\n    @property\n    def _has_estimated_m_values(self):\n        if self.is_null_level:\n            return True\n        vals = [r[\"probability\"] for r in self._trained_m_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        return len(vals) &gt; 0\n\n    @property\n    def _has_estimated_values(self):\n        return self._has_estimated_m_values and self._has_estimated_u_values\n\n    @property\n    def _trained_m_median(self):\n        vals = [r[\"probability\"] for r in self._trained_m_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        if len(vals) == 0:\n            return None\n        return median(vals)\n\n    @property\n    def _trained_u_median(self):\n        vals = [r[\"probability\"] for r in self._trained_u_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        if len(vals) == 0:\n            return None\n        return median(vals)\n\n    @property\n    def _m_is_trained(self):\n        if self.is_null_level:\n            return True\n        if self._m_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return False\n        if self._m_probability is None:\n            return False\n        return True\n\n    @property\n    def _u_is_trained(self):\n        if self.is_null_level:\n            return True\n        if self._u_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return False\n        if self._u_probability is None:\n            return False\n        return True\n\n    @property\n    def _is_trained(self):\n        return self._m_is_trained and self._u_is_trained\n\n    @property\n    def _bayes_factor(self):\n        if self.is_null_level:\n            return 1.0\n        if self.m_probability is None or self.u_probability is None:\n            return None\n        elif self.u_probability == 0:\n            return math.inf\n        else:\n            return self.m_probability / self.u_probability\n\n    @property\n    def _log2_bayes_factor(self):\n        if self.is_null_level:\n            return 0.0\n        else:\n            return math.log2(self._bayes_factor)\n\n    @property\n    def _bayes_factor_description(self):\n        text = (\n            f\"If comparison level is `{self.label_for_charts.lower()}` \"\n            \"then comparison is\"\n        )\n        if self._bayes_factor == math.inf:\n            return f\"{text} certain to be a match\"\n        elif self._bayes_factor == 0.0:\n            return f\"{text} impossible to be a match\"\n        elif self._bayes_factor &gt;= 1.0:\n            return f\"{text} {self._bayes_factor:,.2f} times more likely to be a match\"\n        else:\n            mult = 1 / self._bayes_factor\n            return f\"{text}  {mult:,.2f} times less likely to be a match\"\n\n    @property\n    def label_for_charts(self):\n        return self._level_dict.get(\n            \"label_for_charts\", str(self._comparison_vector_value)\n        )\n\n    @property\n    def _label_for_charts_no_duplicates(self):\n        if self._has_comparison:\n            labels = []\n            for cl in self.comparison.comparison_levels:\n                labels.append(cl.label_for_charts)\n\n        if len(labels) == len(set(labels)):\n            return self.label_for_charts\n\n        # Make label unique\n        cvv = str(self._comparison_vector_value)\n        label = self._level_dict[\"label_for_charts\"]\n        return f\"{cvv}. {label}\"\n\n    @property\n    def _is_else_level(self):\n        if self.sql_condition.strip().upper() == \"ELSE\":\n            return True\n\n    @property\n    def _has_tf_adjustments(self):\n        col = self._level_dict.get(\"tf_adjustment_column\")\n        return col is not None\n\n    def _validate_sql(self):\n        sql = self.sql_condition\n        if self._is_else_level:\n            return True\n        dialect = self.sql_dialect\n        # TODO: really self._sql_dialect_ should always be set, something gets\n        # messed up during the deepcopy()ing of a Comparison\n        if dialect is None:\n            dialect = \"spark\"\n        try:\n            sqlglot.parse_one(sql, read=dialect)\n        except sqlglot.ParseError as e:\n            raise ValueError(f\"Error parsing sql_statement:\\n{sql}\") from e\n\n        return True\n\n    @property\n    def _input_columns_used_by_sql_condition(self) -&gt; list[InputColumn]:\n        # returns e.g. InputColumn(first_name), InputColumn(surname)\n\n        if self._is_else_level:\n            return []\n\n        cols = get_columns_used_from_sql(self.sql_condition, dialect=self.sql_dialect)\n        # Parsed order seems to be roughly in reverse order of apearance\n        cols = cols[::-1]\n\n        cols = [re.sub(r\"_L$|_R$\", \"\", c, flags=re.IGNORECASE) for c in cols]\n        cols = dedupe_preserving_order(cols)\n\n        input_cols = []\n        for c in cols:\n            # We could have tf adjustments for surname on a dmeta_surname column\n            # If so, we want to set the tf adjustments against the surname col,\n            # not the dmeta_surname one\n\n            input_cols.append(InputColumn(c, sql_dialect=self.sql_dialect))\n\n        return input_cols\n\n    @property\n    def _columns_to_select_for_blocking(self):\n        # e.g. l.first_name as first_name_l, r.first_name as first_name_r\n        output_cols = []\n        cols = self._input_columns_used_by_sql_condition\n\n        for c in cols:\n            output_cols.extend(c.l_r_names_as_l_r)\n            if self._tf_adjustment_input_column:\n                output_cols.extend(self._tf_adjustment_input_column.l_r_tf_names_as_l_r)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _when_then_comparison_vector_value_sql(self):\n        # e.g. when first_name_l = first_name_r then 1\n        if not hasattr(self, \"_comparison_vector_value\"):\n            raise ValueError(\n                \"Cannot get the 'when .. then ...' sql expression because \"\n                \"this comparison level does not belong to a parent Comparison. \"\n                \"The comparison_vector_value is only defined in the \"\n                \"context of a list of ComparisonLevels within a Comparison.\"\n            )\n        if self._is_else_level:\n            return f\"{self.sql_condition} {self._comparison_vector_value}\"\n        else:\n            return f\"WHEN {self.sql_condition} THEN {self._comparison_vector_value}\"\n\n    @property\n    def _is_exact_match(self):\n        if self._is_else_level:\n            return False\n\n        sql_syntax_tree = sqlglot.parse_one(\n            self.sql_condition.lower(), read=self.sql_dialect\n        )\n        sql_cnf = simplify(normalize(sql_syntax_tree))\n\n        exprs = _get_and_subclauses(sql_cnf)\n        for expr in exprs:\n            if not _is_exact_match(expr):\n                return False\n        return True\n\n    @property\n    def _exact_match_colnames(self):\n        sql_syntax_tree = sqlglot.parse_one(\n            self.sql_condition.lower(), read=self.sql_dialect\n        )\n        sql_cnf = simplify(normalize(sql_syntax_tree))\n\n        exprs = _get_and_subclauses(sql_cnf)\n        for expr in exprs:\n            if not _is_exact_match(expr):\n                raise ValueError(\n                    \"sql_cond not an exact match so can't get exact match column name\"\n                )\n\n        cols = []\n        for expr in exprs:\n            col = _exact_match_colname(expr)\n            cols.append(col)\n        return cols\n\n    @property\n    def _u_probability_corresponding_to_exact_match(self):\n        levels = self.comparison.comparison_levels\n\n        # Find a level with a single exact match colname\n        # which is equal to the tf adjustment input colname\n\n        for level in levels:\n            if not level._is_exact_match:\n                continue\n            colnames = level._exact_match_colnames\n            if len(colnames) != 1:\n                continue\n            if colnames[0] == self._tf_adjustment_input_column_name.lower():\n                return level.u_probability\n        raise ValueError(\n            \"Could not find an exact match level for \"\n            f\"{self._tf_adjustment_input_column_name}.\"\n            \"\\nAn exact match level is required to make a term frequency adjustment \"\n            \"on a comparison level that is not an exact match.\"\n        )\n\n    @property\n    def _bayes_factor_sql(self):\n        bayes_factor = (\n            self._bayes_factor if self._bayes_factor != math.inf else \"'Infinity'\"\n        )\n        sql = f\"\"\"\n        WHEN\n        {self.comparison._gamma_column_name} = {self._comparison_vector_value}\n        THEN cast({bayes_factor} as float8)\n        \"\"\"\n        return dedent(sql)\n\n    @property\n    def _tf_adjustment_sql(self):\n        gamma_column_name = self.comparison._gamma_column_name\n        gamma_colname_value_is_this_level = (\n            f\"{gamma_column_name} = {self._comparison_vector_value}\"\n        )\n\n        # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment\n        if self._comparison_vector_value == -1:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as float8)\"\n        elif not self._has_tf_adjustments:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as float8)\"\n        elif self._tf_adjustment_weight == 0:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as float8)\"\n        elif self._is_else_level:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as float8)\"\n        else:\n            tf_adj_col = self._tf_adjustment_input_column\n\n            coalesce_l_r = f\"coalesce({tf_adj_col.tf_name_l}, {tf_adj_col.tf_name_r})\"\n            coalesce_r_l = f\"coalesce({tf_adj_col.tf_name_r}, {tf_adj_col.tf_name_l})\"\n\n            tf_adjustment_exists = f\"{coalesce_l_r} is not null\"\n            u_prob_exact_match = self._u_probability_corresponding_to_exact_match\n\n            # Using coalesce protects against one of the tf adjustments being null\n            # Which would happen if the user provided their own tf adjustment table\n            # That didn't contain some of the values in this data\n\n            # In this case rather than taking the greater of the two, we take\n            # whichever value exists\n\n            if self._tf_minimum_u_value == 0.0:\n                divisor_sql = f\"\"\"\n                (CASE\n                    WHEN {coalesce_l_r} &gt;= {coalesce_r_l}\n                    THEN {coalesce_l_r}\n                    ELSE {coalesce_r_l}\n                END)\n                \"\"\"\n            else:\n                # This sql works correctly even when the tf_minimum_u_value is 0.0\n                # but is less efficient to execute, hence the above if statement\n                divisor_sql = f\"\"\"\n                (CASE\n                    WHEN {coalesce_l_r} &gt;= {coalesce_r_l}\n                    AND {coalesce_l_r} &gt; cast({self._tf_minimum_u_value} as float8)\n                        THEN {coalesce_l_r}\n                    WHEN {coalesce_r_l}  &gt; cast({self._tf_minimum_u_value} as float8)\n                        THEN {coalesce_r_l}\n                    ELSE cast({self._tf_minimum_u_value} as float8)\n                END)\n                \"\"\"\n\n            sql = f\"\"\"\n            WHEN  {gamma_colname_value_is_this_level} then\n                (CASE WHEN {tf_adjustment_exists}\n                THEN\n                POW(\n                    cast({u_prob_exact_match} as float8) /{divisor_sql},\n                    cast({self._tf_adjustment_weight} as float8)\n                )\n                ELSE cast(1 as float8)\n                END)\n            \"\"\"\n        return dedent(sql).strip()\n\n    def as_dict(self):\n        \"The minimal representation of this level to use as an input to Splink\"\n        output = {}\n\n        output[\"sql_condition\"] = self.sql_condition\n\n        if self._level_dict.get(\"label_for_charts\"):\n            output[\"label_for_charts\"] = self.label_for_charts\n\n        if self._m_probability and self._m_is_trained:\n            output[\"m_probability\"] = self.m_probability\n\n        if self._u_probability and self._u_is_trained:\n            output[\"u_probability\"] = self.u_probability\n\n        if self._has_tf_adjustments:\n            output[\"tf_adjustment_column\"] = self._tf_adjustment_input_column.input_name\n            if self._tf_adjustment_weight != 0:\n                output[\"tf_adjustment_weight\"] = self._tf_adjustment_weight\n\n        if self.is_null_level:\n            output[\"is_null_level\"] = True\n\n        return output\n\n    def _as_completed_dict(self):\n        comp_dict = self.as_dict()\n        comp_dict[\"comparison_vector_value\"] = self._comparison_vector_value\n        return comp_dict\n\n    @property\n    def _as_detailed_record(self):\n        \"A detailed representation of this level to describe it in charting outputs\"\n        output = {}\n        output[\"sql_condition\"] = self.sql_condition\n        output[\"label_for_charts\"] = self._label_for_charts_no_duplicates\n\n        output[\"m_probability\"] = self.m_probability\n        output[\"u_probability\"] = self.u_probability\n\n        output[\"m_probability_description\"] = self._m_probability_description\n        output[\"u_probability_description\"] = self._u_probability_description\n\n        output[\"has_tf_adjustments\"] = self._has_tf_adjustments\n        if self._has_tf_adjustments:\n            output[\"tf_adjustment_column\"] = self._tf_adjustment_input_column.input_name\n        else:\n            output[\"tf_adjustment_column\"] = None\n        output[\"tf_adjustment_weight\"] = self._tf_adjustment_weight\n\n        output[\"is_null_level\"] = self.is_null_level\n        output[\"bayes_factor\"] = self._bayes_factor\n        output[\"log2_bayes_factor\"] = self._log2_bayes_factor\n        output[\"comparison_vector_value\"] = self._comparison_vector_value\n        output[\"max_comparison_vector_value\"] = self.comparison._num_levels - 1\n        output[\"bayes_factor_description\"] = self._bayes_factor_description\n\n        return output\n\n    @property\n    def _parameter_estimates_as_records(self):\n        output_records = []\n\n        cl_record = self._as_detailed_record\n        trained_values = self._trained_u_probabilities + self._trained_m_probabilities\n        for trained_value in trained_values:\n            record = {}\n            record[\"m_or_u\"] = trained_value[\"m_or_u\"]\n            p = trained_value[\"probability\"]\n            record[\"estimated_probability\"] = p\n            record[\"estimate_description\"] = trained_value[\"description\"]\n            if p is not None and p != LEVEL_NOT_OBSERVED_TEXT and p &gt; 0.0 and p &lt; 1.0:\n                record[\"estimated_probability_as_log_odds\"] = math.log2(p / (1 - p))\n            else:\n                record[\"estimated_probability_as_log_odds\"] = None\n\n            record[\"sql_condition\"] = cl_record[\"sql_condition\"]\n            record[\"comparison_level_label\"] = cl_record[\"label_for_charts\"]\n            record[\"comparison_vector_value\"] = cl_record[\"comparison_vector_value\"]\n            output_records.append(record)\n\n        return output_records\n\n    def _validate(self):\n        self._validate_sql()\n\n    def _abbreviated_sql(self, cutoff=75):\n        sql = self.sql_condition\n        return (sql[:cutoff] + \"...\") if len(sql) &gt; cutoff else sql\n\n    def __repr__(self):\n        return f\"&lt;{self._human_readable_succinct}&gt;\"\n\n    @property\n    def _human_readable_succinct(self):\n        sql = self._abbreviated_sql(75)\n        return f\"Comparison level '{self.label_for_charts}' using SQL rule: {sql}\"\n\n    @property\n    def human_readable_description(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name for c in self._input_columns_used_by_sql_condition]\n        )\n        desc = (\n            f\"Comparison level: {self.label_for_charts} of {input_cols}\\n\"\n            \"Assesses similarity between pairwise comparisons of the input columns \"\n            f\"using the following rule\\n{self.sql_condition}\"\n        )\n\n        return desc\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.as_dict","title":"<code>as_dict()</code>","text":"<p>The minimal representation of this level to use as an input to Splink</p> Source code in <code>splink/comparison_level.py</code> <pre><code>def as_dict(self):\n    \"The minimal representation of this level to use as an input to Splink\"\n    output = {}\n\n    output[\"sql_condition\"] = self.sql_condition\n\n    if self._level_dict.get(\"label_for_charts\"):\n        output[\"label_for_charts\"] = self.label_for_charts\n\n    if self._m_probability and self._m_is_trained:\n        output[\"m_probability\"] = self.m_probability\n\n    if self._u_probability and self._u_is_trained:\n        output[\"u_probability\"] = self.u_probability\n\n    if self._has_tf_adjustments:\n        output[\"tf_adjustment_column\"] = self._tf_adjustment_input_column.input_name\n        if self._tf_adjustment_weight != 0:\n            output[\"tf_adjustment_weight\"] = self._tf_adjustment_weight\n\n    if self.is_null_level:\n        output[\"is_null_level\"] = True\n\n    return output\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html","title":"Comparison Composition","text":"","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#documentation-for-comparison_level_composition-functions","title":"Documentation for <code>comparison_level_composition</code> functions","text":"<p><code>comparison_composition</code> allows the merging of existing comparison levels by a logical SQL clause - <code>OR</code>, <code>AND</code> or <code>NOT</code>.</p> <p>This extends the functionality of our base comparison levels by allowing users to \"join\" existing comparisons by various SQL clauses.</p> <p>For example, <code>or_(null_level(\"first_name\"), null_level(\"surname\"))</code> creates a check for nulls in either <code>first_name</code> or <code>surname</code>, rather than restricting the user to a single column.</p> <p>The Splink comparison level composition functions available for each SQL dialect are as given in this table:</p>  DuckDB  Spark  Athena  SQLite  PostgreSql and_ \u2713 \u2713 \u2713 \u2713 \u2713 not_ \u2713 \u2713 \u2713 \u2713 \u2713 or_ \u2713 \u2713 \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#library-comparison-composition-apis","title":"Library comparison composition APIs","text":"","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.and_","title":"<code>and_(*clls, label_for_charts=None, m_probability=None, is_null_level=None)</code>","text":"<p>Merge ComparisonLevels using logical \"AND\".</p> <p>Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"AND\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*clls</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevels or comparison level dictionaries to merge</p> <code>()</code> <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - <code>label_1 AND label_2</code></p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>is_null_level</code> <code>bool</code> <p>If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite <p>Simple null level composition with an <code>AND</code> clause </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.duckdb.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.spark.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.athena.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A new ComparisonLevel with the merged SQL condition</p> Source code in <code>splink/comparison_level_composition.py</code> <pre><code>def and_(\n    *clls: ComparisonLevel | dict,\n    label_for_charts=None,\n    m_probability=None,\n    is_null_level=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Merge ComparisonLevels using logical \"AND\".\n\n    Merge multiple ComparisonLevels into a single ComparisonLevel by\n    merging their SQL conditions using a logical \"AND\".\n\n    By default, we generate a new `label_for_charts` for the new ComparisonLevel.\n    You can override this, and any other ComparisonLevel attributes, by passing\n    them as keyword arguments.\n\n    Args:\n        *clls (ComparisonLevel | dict): ComparisonLevels or comparison\n            level dictionaries to merge\n        label_for_charts (str, optional): A label for this comparson level,\n            which will appear on charts as a reminder of what the level represents.\n            Defaults to a composition of - `label_1 AND label_2`\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n        is_null_level (bool, optional): If true, m and u values will not be\n            estimated and instead the match weight will be zero for this column.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple null level composition with an `AND` clause\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-apachespark: Spark\"\n            Simple null level composition with an `AND` clause\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-amazonaws: Athena\"\n            Simple null level composition with an `AND` clause\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-sqlite: SQLite\"\n            Simple null level composition with an `AND` clause\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n\n    Returns:\n        ComparisonLevel: A new ComparisonLevel with the merged\n            SQL condition\n    \"\"\"\n    return _cl_merge(\n        *clls,\n        clause=\"AND\",\n        label_for_charts=label_for_charts,\n        m_probability=m_probability,\n        is_null_level=is_null_level,\n    )\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.not_","title":"<code>not_(cll, label_for_charts=None, m_probability=None)</code>","text":"<p>Negate a ComparisonLevel.</p> <p>Returns a ComparisonLevel with the same SQL condition as the input, but prefixed with \"NOT\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>cll</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevel or comparison level dictionary</p> required <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite <p>Not an exact match on first name </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.duckdb.comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre> <p>Not an exact match on first name </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.spark.comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre> <p>Not an exact match on first name </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.athena.comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre> <p>Not an exact match on first name </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.sqlite.comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre> <p>Returns:</p> Type Description <code>ComparisonLevel</code> <p>ComparisonLevel A new ComparisonLevel with the negated SQL condition and label_for_charts</p> Source code in <code>splink/comparison_level_composition.py</code> <pre><code>def not_(\n    cll: ComparisonLevel | dict,\n    label_for_charts: str | None = None,\n    m_probability: float | None = None,\n) -&gt; ComparisonLevel:\n    \"\"\"Negate a ComparisonLevel.\n\n    Returns a ComparisonLevel with the same SQL condition as the input,\n    but prefixed with \"NOT\".\n\n    By default, we generate a new `label_for_charts` for the new ComparisonLevel.\n    You can override this, and any other ComparisonLevel attributes, by passing\n    them as keyword arguments.\n\n    Args:\n        cll (ComparisonLevel | dict): ComparisonLevel or comparison\n            level dictionary\n        label_for_charts (str, optional): A label for this comparson level,\n            which will appear on charts as a reminder of what the level represents.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            *Not* an exact match on first name\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.not_(cll.exact_match(\"first_name\"))\n            ```\n            Find all exact matches *not* on the first of January\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            dob_first_jan =  {\n               \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n               \"label_for_charts\": \"Date is 1st Jan\",\n            }\n            exact_match_not_first_jan = cll.and_(\n                cll.exact_match_level(\"dob\"),\n                cll.not_(dob_first_jan),\n                label_for_charts = \"Exact match and not the 1st Jan\"\n            )\n            ```\n        === \":simple-apachespark: Spark\"\n            *Not* an exact match on first name\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.not_(cll.exact_match(\"first_name\"))\n            ```\n            Find all exact matches *not* on the first of January\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            dob_first_jan =  {\n               \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n               \"label_for_charts\": \"Date is 1st Jan\",\n            }\n            exact_match_not_first_jan = cll.and_(\n                cll.exact_match_level(\"dob\"),\n                cll.not_(dob_first_jan),\n                label_for_charts = \"Exact match and not the 1st Jan\"\n            )\n            ```\n        === \":simple-amazonaws: Athena\"\n            *Not* an exact match on first name\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.not_(cll.exact_match(\"first_name\"))\n            ```\n            Find all exact matches *not* on the first of January\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            dob_first_jan =  {\n               \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n               \"label_for_charts\": \"Date is 1st Jan\",\n            }\n            exact_match_not_first_jan = cll.and_(\n                cll.exact_match_level(\"dob\"),\n                cll.not_(dob_first_jan),\n                label_for_charts = \"Exact match and not the 1st Jan\"\n            )\n            ```\n        === \":simple-sqlite: SQLite\"\n            *Not* an exact match on first name\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.not_(cll.exact_match(\"first_name\"))\n            ```\n            Find all exact matches *not* on the first of January\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            dob_first_jan =  {\n               \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n               \"label_for_charts\": \"Date is 1st Jan\",\n            }\n            exact_match_not_first_jan = cll.and_(\n                cll.exact_match_level(\"dob\"),\n                cll.not_(dob_first_jan),\n                label_for_charts = \"Exact match and not the 1st Jan\"\n            )\n            ```\n\n    Returns:\n        ComparisonLevel\n            A new ComparisonLevel with the negated SQL condition and label_for_charts\n    \"\"\"\n    cls, sql_dialect = _parse_comparison_levels(cll)\n    cl = cls[0]\n    result = {}\n    result[\"sql_condition\"] = f\"NOT ({cl.sql_condition})\"\n\n    # Invert if is_null_level.\n    # If NOT is_null_level, then we don't know if the inverted level is null or not\n    if not cl.is_null_level:\n        result[\"is_null_level\"] = False\n\n    result[\"label_for_charts\"] = (\n        label_for_charts if label_for_charts else f\"NOT ({cl.label_for_charts})\"\n    )\n\n    if m_probability:\n        result[\"m_probability\"] = m_probability\n\n    return ComparisonLevel(result, sql_dialect=sql_dialect)\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.or_","title":"<code>or_(*clls, label_for_charts=None, m_probability=None, is_null_level=None)</code>","text":"<p>Merge ComparisonLevels using logical \"OR\".</p> <p>Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"OR\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*clls</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevels or comparison level dictionaries to merge</p> <code>()</code> <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - <code>label_1 OR label_2</code></p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>is_null_level</code> <code>bool</code> <p>If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite <p>Simple null level composition with an <code>OR</code> clause </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.duckdb.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.spark.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.athena.comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A new ComparisonLevel with the merged SQL condition</p> Source code in <code>splink/comparison_level_composition.py</code> <pre><code>def or_(\n    *clls: ComparisonLevel | dict,\n    label_for_charts: str | None = None,\n    m_probability: float | None = None,\n    is_null_level: bool | None = None,\n) -&gt; ComparisonLevel:\n    \"\"\"Merge ComparisonLevels using logical \"OR\".\n\n    Merge multiple ComparisonLevels into a single ComparisonLevel by\n    merging their SQL conditions using a logical \"OR\".\n\n    By default, we generate a new `label_for_charts` for the new ComparisonLevel.\n    You can override this, and any other ComparisonLevel attributes, by passing\n    them as keyword arguments.\n\n    Args:\n        *clls (ComparisonLevel | dict): ComparisonLevels or comparison\n            level dictionaries to merge\n        label_for_charts (str, optional): A label for this comparson level,\n            which will appear on charts as a reminder of what the level represents.\n            Defaults to a composition of - `label_1 OR label_2`\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n        is_null_level (bool, optional): If true, m and u values will not be\n            estimated and instead the match weight will be zero for this column.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple null level composition with an `OR` clause\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-apachespark: Spark\"\n            Simple null level composition with an `OR` clause\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-amazonaws: Athena\"\n            Simple null level composition with an `OR` clause\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n            Composing a levenshtein level with a custom `contains` level\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            misspelling = cll.levenshtein_level(\"name\", 1)\n            contains = {\n                \"sql_condition\": \"(contains(name_l, name_r) OR \" \\\n                \"contains(name_r, name_l))\"\n            }\n            merged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n            ```\n            ```python\n            merged.as_dict()\n            ```\n            &gt;{\n            &gt; sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) ' \\\n            &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',\n            &gt;  'label_for_charts': 'Spelling error'\n            &gt;}\n        === \":simple-sqlite: SQLite\"\n            Simple null level composition with an `OR` clause\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n            ```\n\n    Returns:\n        ComparisonLevel: A new ComparisonLevel with the merged\n            SQL condition\n    \"\"\"\n\n    return _cl_merge(\n        *clls,\n        clause=\"OR\",\n        label_for_charts=label_for_charts,\n        m_probability=m_probability,\n        is_null_level=is_null_level,\n    )\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level_library.html","title":"Comparison Level Library","text":"","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#documentation-for-comparison_level_library","title":"Documentation for <code>comparison_level_library</code>","text":"<p>The <code>comparison_level_library</code> contains pre-made comparison levels available for use to construct custom comparisons as described in this topic guide. However, not every comparison level is available for every Splink-compatible SQL backend.</p> <p>The pre-made Splink comparison levels available for each SQL dialect are as given in this table:</p>  DuckDB  Spark  Athena  SQLite  PostgreSql array_intersect_level \u2713 \u2713 \u2713 \u2713 columns_reversed_level \u2713 \u2713 \u2713 \u2713 \u2713 damerau_levenshtein_level \u2713 \u2713 \u2713 datediff_level \u2713 \u2713 \u2713 \u2713 distance_function_level \u2713 \u2713 \u2713 \u2713 \u2713 distance_in_km_level \u2713 \u2713 \u2713 \u2713 else_level \u2713 \u2713 \u2713 \u2713 \u2713 exact_match_level \u2713 \u2713 \u2713 \u2713 \u2713 jaccard_level \u2713 \u2713 jaro_level \u2713 \u2713 \u2713 jaro_winkler_level \u2713 \u2713 \u2713 levenshtein_level \u2713 \u2713 \u2713 \u2713 \u2713 null_level \u2713 \u2713 \u2713 \u2713 \u2713 percentage_difference_level \u2713 \u2713 \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#library-comparison-level-apis","title":"Library comparison level APIs","text":"<p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class NullLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name,\n        valid_string_pattern: str = None,\n        invalid_dates_as_null: bool = False,\n        valid_string_regex: str = None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents comparisons level where one or both sides of the comparison\n        contains null values so the similarity cannot be evaluated.\n        Assumed to have a partial match weight of zero (null effect\n        on overall match weight)\n        Args:\n            col_name (str): Input column name\n            valid_string_pattern (str): pattern (regex or otherwise) that if not\n                matched will result in column being treated as a null.\n            invalid_dates_as_null (bool): If True, set all invalid dates to null.\n                The \"correct\" format of a date is set by valid_string_pattern.\n                Defaults to false.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Simple null comparison level\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.null_level(\"name\")\n                ```\n                Null comparison level including strings that do not match\n                a given regex pattern\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Simple null level\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.null_level(\"name\")\n                ```\n                Null comparison level including strings that do not match\n                a given regex pattern\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Simple null level\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.null_level(\"name\")\n                ```\n                Null comparison level including strings that do not match\n                a given regex pattern\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Simple null level\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.null_level(\"name\")\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Simple null level\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.null_level(\"name\")\n                ```\n        Returns:\n            ComparisonLevel: Comparison level for null entries\n        \"\"\"\n\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"valid_string_regex\". Deprecation warning added in 3.9.6\n        if valid_string_pattern is not None and valid_string_regex is not None:\n            # user supplied both\n            raise TypeError(\"Just use valid_string_pattern\")\n        elif valid_string_pattern is not None:\n            # user is doing it correctly\n            pass\n        elif valid_string_regex is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"valid_string_regex is deprecated; use valid_string_pattern\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            valid_string_pattern = valid_string_regex\n\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n        col_name_l, col_name_r = col.name_l, col.name_r\n\n        if invalid_dates_as_null:\n            # See https://github.com/moj-analytical-services/splink/pull/1939\n            col_name_l = self._valid_date_function(col_name_l, valid_string_pattern)\n            col_name_r = self._valid_date_function(col_name_r, valid_string_pattern)\n            sql = f\"\"\"{col_name_l} IS NULL OR {col_name_r} IS NULL\"\"\"\n        elif valid_string_pattern:\n            col_name_l = self._regex_extract_function(col_name_l, valid_string_pattern)\n            col_name_r = self._regex_extract_function(col_name_r, valid_string_pattern)\n            sql = f\"\"\"{col_name_l} IS NULL OR {col_name_r} IS NULL OR\n                      {col_name_l} = '' OR {col_name_r} = '' \"\"\"\n        else:\n            sql = f\"{col_name_l} IS NULL OR {col_name_r} IS NULL\"\n\n        level_dict = {\n            \"sql_condition\": sql,\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        }\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class ExactMatchLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name,\n        regex_extract: str = None,\n        set_to_lowercase: bool = False,\n        m_probability=None,\n        term_frequency_adjustments=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level where there is an exact match,\n\n        Args:\n            col_name (str): Input column name\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            m_probability (float, optional): Starting value for m probability\n                Defaults to None.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            include_colname_in_charts_label (bool, optional): If True, include col_name\n                in chart labels (e.g. linker.match_weights_chart())\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n                include_colname_in_charts_label=True\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Simple Exact match level\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.exact_match_level(\"name\")\n                ```\n                Exact match level with term-frequency adjustments\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n                ```\n                Exact match level on a substring of col_name as\n                 determined by a regular expression\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Simple Exact match level\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.exact_match_level(\"name\")\n                ```\n                Exact match level with term-frequency adjustments\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n                ```\n                Exact match level on a substring of col_name as\n                 determined by a regular expression\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Simple Exact match level\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.exact_match_level(\"name\")\n                ```\n                Exact match level with term-frequency adjustments\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n                ```\n                Exact match level on a substring of col_name as\n                 determined by a regular expression\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Simple Exact match level\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.exact_match_level(\"name\")\n                ```\n                Exact match level with term-frequency adjustments\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n            === \":simple-postgresql: PostgreSql\"\n                Simple Exact match level\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.exact_match_level(\"name\")\n                ```\n                Exact match level with term-frequency adjustments\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n                ```\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n        if include_colname_in_charts_label:\n            label_suffix = f\" {col_name}\"\n        elif manual_col_name_for_charts_label:\n            label_suffix = f\" {manual_col_name_for_charts_label}\"\n        else:\n            label_suffix = \"\"\n\n        col_name_l, col_name_r = col.name_l, col.name_r\n\n        if set_to_lowercase:\n            col_name_l = f\"lower({col_name_l})\"\n            col_name_r = f\"lower({col_name_r})\"\n\n        if regex_extract:\n            col_name_l = self._regex_extract_function(col_name_l, regex_extract)\n            col_name_r = self._regex_extract_function(col_name_r, regex_extract)\n\n        sql_cond = f\"{col_name_l} = {col_name_r}\"\n        level_dict = {\n            \"sql_condition\": sql_cond,\n            \"label_for_charts\": f\"Exact match{label_suffix}\",\n        }\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n        if term_frequency_adjustments:\n            level_dict[\"tf_adjustment_column\"] = col_name\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class ElseLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level for all cases which have not been\n        considered by preceding comparison levels,\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.else_level()\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.else_level()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.else_level()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.else_level()\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.else_level()\n                ```\n        \"\"\"\n        if isinstance(m_probability, str):\n            raise ValueError(\n                \"You provided a string for the value of m probability when it should \"\n                \"be numeric.  Perhaps you passed a column name.  Note that you do \"\n                \"not need to pass a column name into the else level.\"\n            )\n        level_dict = {\n            \"sql_condition\": \"ELSE\",\n            \"label_for_charts\": \"All other comparisons\",\n        }\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n        super().__init__(level_dict)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class DistanceFunctionLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name: str,\n        distance_function_name: str,\n        distance_threshold: int | float,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        higher_is_more_similar: bool = True,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level using a user-provided distance function,\n        where the similarity\n\n        Args:\n            col_name (str): Input column name\n            distance_function_name (str): The name of the distance function\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            higher_is_more_similar (bool): If True, a higher value of the\n                distance function indicates a higher similarity (e.g. jaro_winkler).\n                If false, a higher value indicates a lower similarity\n                (e.g. levenshtein).\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label.\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Apply the `levenshtein` function to a comparison level\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.distance_function_level(\"name\",\n                                            \"levenshtein\",\n                                            2,\n                                            False)\n                ```\n            === \":simple-apachespark: Spark\"\n                Apply the `levenshtein` function to a comparison level\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.distance_function_level(\"name\",\n                                            \"levenshtein\",\n                                            2,\n                                            False)\n                ```\n            === \":simple-amazonaws: Athena\"\n                Apply the `levenshtein_distance` function to a comparison level\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.distance_function_level(\"name\",\n                                            \"levenshtein_distance\",\n                                            2,\n                                            False)\n                ```\n            === \":simple-sqlite: SQLite\"\n                Apply the `levenshtein` function to a comparison level\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.distance_function_level(\"name\",\n                                            \"levenshtein\",\n                                            2,\n                                            False)\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Apply the `levenshtein` function to a comparison level\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.distance_function_level(\"name\",\n                                            \"levenshtein\",\n                                            2,\n                                            False)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level for a given distance function\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n        if higher_is_more_similar:\n            operator = \"&gt;=\"\n        else:\n            operator = \"&lt;=\"\n\n        col_name_l, col_name_r = col.name_l, col.name_r\n\n        if set_to_lowercase:\n            col_name_l = f\"lower({col_name_l})\"\n            col_name_r = f\"lower({col_name_r})\"\n\n        if regex_extract:\n            col_name_l = self._regex_extract_function(col_name_l, regex_extract)\n            col_name_r = self._regex_extract_function(col_name_r, regex_extract)\n\n        sql_cond = (\n            f\"{distance_function_name}({col_name_l}, {col_name_r}) \"\n            f\"{operator} {distance_threshold}\"\n        )\n\n        if include_colname_in_charts_label:\n            if manual_col_name_for_charts_label:\n                col_name = manual_col_name_for_charts_label\n\n            label_suffix = f\" {col_name}\"\n        else:\n            label_suffix = \"\"\n\n        chart_label = (\n            f\"{distance_function_name.capitalize()}{label_suffix} {operator} \"\n            f\"{distance_threshold}\"\n        )\n\n        level_dict = {\n            \"sql_condition\": sql_cond,\n            \"label_for_charts\": chart_label,\n        }\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n\n    @property\n    def _distance_level(self):\n        raise NotImplementedError(\"Distance function not supported in this dialect\")\n</code></pre> <p>             Bases: <code>DistanceFunctionLevelBase</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class LevenshteinLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: int,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level using a levenshtein distance function,\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1)\n                ```\n\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1 on a subtring of name column as determined by a regular\n                expression.\n                ```python\n                import splink.duckdb.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1)\n                ```\n\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1 on a subtring of name column as determined by a regular\n                expression.\n                ```python\n                import splink.spark.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1)\n                ```\n\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1 on a subtring of name column as determined by a regular\n                expression.\n                ```python\n                import splink.athena.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1)\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Comparison level with levenshtein distance score less than (or equal\n                 to) 1\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.levenshtein_level(\"name\", 1)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                levenshtein similarity\n        \"\"\"\n        super().__init__(\n            col_name,\n            distance_function_name=self._levenshtein_name,\n            distance_threshold=distance_threshold,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=False,\n            include_colname_in_charts_label=include_colname_in_charts_label,\n            m_probability=m_probability,\n        )\n</code></pre> <p>             Bases: <code>DistanceFunctionLevelBase</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class DamerauLevenshteinLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: int,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level using a damerau-levenshtein distance\n        function,\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level with damerau-levenshtein distance score less than\n                (or equal to) 1\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.damerau_levenshtein_level(\"name\", 1)\n                ```\n\n                Comparison level with damerau-levenshtein distance score less than\n                (or equal to) 1 on a subtring of name column as determined by a regular\n                expression.\n                ```python\n                import splink.duckdb.comparison_level_library as cll\n                cll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level with damerau-levenshtein distance score less than\n                (or equal to) 1\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.damerau_levenshtein_level(\"name\", 1)\n                ```\n\n                Comparison level with damerau-levenshtein distance score less than\n                (or equal to) 1 on a subtring of name column as determined by a regular\n                expression.\n                ```python\n                import splink.spark.comparison_level_library as cll\n                cll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Comparison level with damerau-levenshtein distance score less than\n                (or equal to) 1\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.damerau_levenshtein_level(\"name\", 1)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                Damerau-Levenshtein similarity\n        \"\"\"\n        super().__init__(\n            col_name,\n            distance_function_name=self._damerau_levenshtein_name,\n            distance_threshold=distance_threshold,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=False,\n            include_colname_in_charts_label=include_colname_in_charts_label,\n            m_probability=m_probability,\n        )\n</code></pre> <p>             Bases: <code>DistanceFunctionLevelBase</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class JaroLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: float,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ):\n        \"\"\"Represents a comparison using the jaro distance function\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level with jaro score greater than 0.9\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaro_level(\"name\", 0.9)\n                ```\n                Comparison level with a jaro score greater than 0.9 on a substring\n                of name column as determined by a regular expression.\n\n                ```python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level with jaro score greater than 0.9\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.jaro_level(\"name\", 0.9)\n                ```\n                Comparison level with a jaro score greater than 0.9 on a substring\n                of name column as determined by a regular expression.\n\n                ```python\n                import splink.spark.comparison_level_library as cll\n                cll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Comparison level with jaro score greater than 0.9\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.jaro_level(\"name\", 0.9)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                jaro similarity\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            self._jaro_name,\n            distance_threshold=distance_threshold,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_colname_in_charts_label=include_colname_in_charts_label,\n            m_probability=m_probability,\n        )\n</code></pre> <p>             Bases: <code>DistanceFunctionLevelBase</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class JaroWinklerLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: float,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level using the jaro winkler distance function\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level with jaro-winkler score greater than 0.9\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaro_winkler_level(\"name\", 0.9)\n                ```\n                Comparison level with jaro-winkler score greater than 0.9 on a\n                substring of name column as determined by a regular expression.\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level with jaro-winkler score greater than 0.9\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.jaro_winkler_level(\"name\", 0.9)\n                ```\n                Comparison level with jaro-winkler score greater than 0.9 on a\n                substring of name column as determined by a regular expression.\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Comparison level with jaro-winkler score greater than 0.9\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.jaro_winkler_level(\"name\", 0.9)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                jaro winkler similarity\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            self._jaro_winkler_name,\n            distance_threshold=distance_threshold,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_colname_in_charts_label=include_colname_in_charts_label,\n            m_probability=m_probability,\n        )\n\n    @property\n    def _jaro_winkler_name(self):\n        raise NotImplementedError(\n            \"Jaro-winkler function name not defined on base class\"\n        )\n</code></pre> <p>             Bases: <code>DistanceFunctionLevelBase</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class JaccardLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: int | float,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        include_colname_in_charts_label=False,\n        manual_col_name_for_charts_label=None,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level using a jaccard distance function\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            include_colname_in_charts_label (bool, optional): If True, includes\n                col_name in charts label\n            manual_col_name_for_charts_label (str, optional): string to include as\n                 column name in chart label. Acts as a manual overwrite of the\n                 colname when include_colname_in_charts_label is True.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level with jaccard score greater than 0.9\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaccard_level(\"name\", 0.9)\n                ```\n                Comparison level with jaccard score greater than 0.9 on a\n                substring of name column as determined by a regular expression.\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level with jaccard score greater than 0.9\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.jaccard_level(\"name\", 0.9)\n                ```\n                Comparison level with jaccard score greater than 0.9 on a\n                substring of name column as determined by a regular expression.\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the jaccard similarity\n        \"\"\"\n        super().__init__(\n            col_name,\n            self._jaccard_name,\n            distance_threshold=distance_threshold,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_colname_in_charts_label=include_colname_in_charts_label,\n            m_probability=m_probability,\n        )\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class ColumnsReversedLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name_1: str,\n        col_name_2: str,\n        regex_extract: str = None,\n        set_to_lowercase=False,\n        m_probability=None,\n        tf_adjustment_column=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level where the columns are reversed.  For example,\n        if surname is in the forename field and vice versa\n\n        Args:\n            col_name_1 (str): First column, e.g. forename\n            col_name_2 (str): Second column, e.g. surname\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n            tf_adjustment_column (str, optional): Column to use for term frequency\n                adjustments if an exact match is observed. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Comparison level on first_name and surname columns reversed\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\", \"surname\")\n                ```\n                Comparison level on first_name and surname column reversed\n                on a substring of each column as determined by a regular expression.\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\",\n                                           \"surname\",\n                                           regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Comparison level on first_name and surname columns reversed\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\", \"surname\")\n                ```\n                Comparison level on first_name and surname column reversed\n                on a substring of each column as determined by a regular expression.\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\",\n                                           \"surname\",\n                                           regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Comparison level on first_name and surname columns reversed\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\", \"surname\")\n                ```\n                Comparison level on first_name and surname column reversed\n                on a substring of each column as determined by a regular expression.\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\",\n                                           \"surname\",\n                                           regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Comparison level on first_name and surname columns reversed\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\", \"surname\")\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.columns_reversed_level(\"first_name\", \"surname\")\n                ```\n\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the exact match of two\n                columns.\n        \"\"\"\n\n        col_1 = InputColumn(col_name_1, sql_dialect=self._sql_dialect)\n        col_2 = InputColumn(col_name_2, sql_dialect=self._sql_dialect)\n\n        col_1_l, col_1_r = col_1.name_l, col_1.name_r\n        col_2_l, col_2_r = col_2.name_l, col_2.name_r\n\n        if set_to_lowercase:\n            col_1_l = f\"lower({col_1_l})\"\n            col_1_r = f\"lower({col_1_r})\"\n            col_2_l = f\"lower({col_2_l})\"\n            col_2_r = f\"lower({col_2_r})\"\n\n        if regex_extract:\n            col_1_l = self._regex_extract_function(col_1_l, regex_extract)\n            col_1_r = self._regex_extract_function(col_1_r, regex_extract)\n            col_2_l = self._regex_extract_function(col_2_l, regex_extract)\n            col_2_r = self._regex_extract_function(col_2_r, regex_extract)\n\n        s = f\"{col_1_l} = {col_2_r} and \" f\"{col_1_r} = {col_2_l}\"\n        level_dict = {\n            \"sql_condition\": s,\n            \"label_for_charts\": \"Exact match on reversed cols\",\n        }\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        if tf_adjustment_column:\n            level_dict[\"tf_adjustment_column\"] = tf_adjustment_column\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class DistanceInKMLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        lat_col: str,\n        long_col: str,\n        km_threshold: int | float,\n        not_null: bool = False,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Use the haversine formula to transform comparisons of lat,lngs\n        into distances measured in kilometers\n\n        Arguments:\n            lat_col (str): The name of a latitude column or the respective array\n                or struct column column containing the information\n                For example: long_lat['lat'] or long_lat[0]\n            long_col (str): The name of a longitudinal column or the respective array\n                or struct column column containing the information, plus an index.\n                For example: long_lat['long'] or long_lat[1]\n            km_threshold (int): The total distance in kilometers to evaluate your\n                comparisons against\n            not_null (bool): If true, remove any . This is only necessary if you are not\n                capturing nulls elsewhere in your comparison level.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.distance_in_km_level(\"lat_col\",\n                                        \"long_col\",\n                                        km_threshold=5)\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.distance_in_km_level(\"lat_col\",\n                                        \"long_col\",\n                                        km_threshold=5)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.distance_in_km_level(\"lat_col\",\n                                        \"long_col\",\n                                        km_threshold=5)\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.distance_in_km_level(\"lat_col\",\n                                        \"long_col\",\n                                        km_threshold=5)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the distance between\n                two coordinates\n        \"\"\"\n\n        lat = InputColumn(lat_col, sql_dialect=self._sql_dialect)\n        long = InputColumn(long_col, sql_dialect=self._sql_dialect)\n        lat_l, lat_r = lat.names_l_r\n        long_l, long_r = long.names_l_r\n\n        distance_km_sql = f\"\"\"\n        {great_circle_distance_km_sql(lat_l, lat_r, long_l, long_r)} &lt;= {km_threshold}\n        \"\"\"\n\n        if not_null:\n            null_sql = \" AND \".join(\n                [f\"{c} is not null\" for c in [lat_r, lat_l, long_l, long_r]]\n            )\n            distance_km_sql = f\"({null_sql}) AND {distance_km_sql}\"\n\n        level_dict = {\n            \"sql_condition\": distance_km_sql,\n            \"label_for_charts\": f\"Distance less than {km_threshold}km\",\n        }\n\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class PercentageDifferenceLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name: str,\n        percentage_distance_threshold: float,\n        m_probability=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level based around the percentage difference between\n        two numbers.\n\n        Note: the percentage is calculated by dividing the absolute difference between\n        the values by the largest value\n\n        Args:\n            col_name (str): Input column name\n            percentage_distance_threshold (float): Percentage difference threshold for\n                the comparison level\n            m_probability (float, optional): Starting value for m probability. Defaults\n                to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.percentage_difference_level(\"value\", 0.5)\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.percentage_difference_level(\"value\", 0.5)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.percentage_difference_level(\"value\", 0.5)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ``` python\n                import splink.sqlite.comparison_level_library as cll\n                cll.percentage_difference_level(\"value\", 0.5)\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.percentage_difference_level(\"value\", 0.5)\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the percentage difference\n                between two values\n\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n        s = f\"\"\"(abs({col.name_l} - {col.name_r})/\n            (case\n                when {col.name_r} &gt; {col.name_l}\n                then {col.name_r}\n                else {col.name_l}\n            end))\n            &lt; {percentage_distance_threshold}\"\"\"\n\n        level_dict = {\n            \"sql_condition\": s,\n            \"label_for_charts\": f\"&lt; {percentage_distance_threshold:,.2%} diff\",\n        }\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class ArrayIntersectLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name,\n        m_probability=None,\n        term_frequency_adjustments=False,\n        min_intersection=1,\n        include_colname_in_charts_label=False,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level based around the size of an intersection of\n        arrays\n\n        Args:\n            col_name (str): Input column name\n            m_probability (float, optional): Starting value for m probability. Defaults\n                to None.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            min_intersection (int, optional): The minimum cardinality of the\n                intersection of arrays for this comparison level. Defaults to 1\n            include_colname_in_charts_label (bool, optional): Should the charts label\n                contain the column name? Defaults to False\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.array_intersect_level(\"name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.array_intersect_level(\"name\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.array_intersect_level(\"name\")\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.array_intersect_level(\"name\")\n                ```\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the size of intersection\n                of arrays\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n        size_array_intersection = (\n            f\"{self._size_array_intersect_function(col.name_l, col.name_r)}\"\n        )\n        sql = f\"{size_array_intersection} &gt;= {min_intersection}\"\n\n        label_prefix = (\n            f\"{col_name} arrays\" if include_colname_in_charts_label else \"Arrays\"\n        )\n        if min_intersection == 1:\n            label = f\"{label_prefix} intersect\"\n        else:\n            label = f\"{label_prefix} intersect size &gt;= {min_intersection}\"\n\n        level_dict = {\"sql_condition\": sql, \"label_for_charts\": label}\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n        if term_frequency_adjustments:\n            level_dict[\"tf_adjustment_column\"] = col_name\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n\n    @property\n    def _size_array_intersect_function(self):\n        raise NotImplementedError(\"Intersect function not defined on base class\")\n</code></pre> <p>             Bases: <code>ComparisonLevel</code></p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>class DatediffLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        date_col: str,\n        date_threshold: int,\n        date_metric: str = \"day\",\n        m_probability=None,\n        cast_strings_to_date=False,\n        date_format=None,\n    ) -&gt; ComparisonLevel:\n        \"\"\"Represents a comparison level based around the difference between dates\n        within a column\n\n        Arguments:\n            date_col (str): Input column name\n            date_threshold (int): The total difference in time between two given\n                dates. This is used in tandem with `date_metric` to determine .\n                If you are using `year` as your metric, then a value of 1 would\n                require that your dates lie within 1 year of one another.\n            date_metric (str): The unit of time with which to measure your\n                `date_threshold`.\n                Your metric should be one of `day`, `month` or `year`.\n                Defaults to `day`.\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n            cast_strings_to_date (bool, optional): Set to true and adjust\n                date_format param when input dates are strings to enable\n                date-casting. Defaults to False.\n            date_format (str, optional): Format of input dates if date-strings\n                are given. Must be consistent across record pairs. If None\n                (the default), downstream functions for each backend assign\n                date_format to ISO 8601 format (yyyy-mm-dd).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Date Difference comparison level at threshold 1 year\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.datediff_level(\"date\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"\n                                    )\n                ```\n                Date Difference comparison with date-casting and unspecified\n                date_format (default = %Y-%m-%d)\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True\n                                    )\n                ```\n                Date Difference comparison with date-casting and specified date_format\n                ``` python\n                import splink.duckdb.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True,\n                                    date_format='%d/%m/%Y'\n                                    )\n                ```\n            === \":simple-apachespark: Spark\"\n                Date Difference comparison level at threshold 1 year\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.datediff_level(\"date\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"\n                                    )\n                ```\n                Date Difference comparison with date-casting and unspecified\n                date_format (default = %Y-%m-%d)\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True\n                                    )\n                ```\n                Date Difference comparison with date-casting and specified date_format\n                ``` python\n                import splink.spark.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True,\n                                    date_format='%d/%m/%Y'\n                                    )\n                ```\n            === \":simple-amazonaws: Athena\"\n                Date Difference comparison level at threshold 1 year\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.datediff_level(\"date\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"\n                                    )\n                ```\n                Date Difference comparison with date-casting and unspecified\n                date_format (default = %Y-%m-%d)\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True\n                                    )\n                ```\n                Date Difference comparison with date-casting and specified date_format\n                ``` python\n                import splink.athena.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True,\n                                    date_format='%d/%m/%Y'\n                                    )\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Date Difference comparison level at threshold 1 year\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.datediff_level(\"date\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"\n                                    )\n                ```\n                Date Difference comparison with date-casting and unspecified\n                date_format (default = yyyy-MM-dd)\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True\n                                    )\n                ```\n                Date Difference comparison with date-casting and specified date_format\n                ``` python\n                import splink.postgres.comparison_level_library as cll\n                cll.datediff_level(\"dob\",\n                                    date_threshold=3,\n                                    date_metric='month',\n                                    cast_strings_to_date=True,\n                                    date_format='dd/MM/yyyy'\n                                    )\n                ```\n        Returns:\n            ComparisonLevel: A comparison level that evaluates whether two dates fall\n                within a given interval.\n        \"\"\"\n\n        date = InputColumn(date_col, sql_dialect=self._sql_dialect)\n        date_l, date_r = date.names_l_r\n\n        datediff_sql = self._datediff_function(\n            date_l,\n            date_r,\n            date_threshold,\n            date_metric,\n            cast_strings_to_date,\n            date_format,\n        )\n        label = f\"Within {date_threshold} {date_metric}\"\n        if date_threshold &gt; 1:\n            label += \"s\"\n\n        level_dict = {\n            \"sql_condition\": datediff_sql,\n            \"label_for_charts\": label,\n        }\n\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n\n    @property\n    def _datediff_function(self):\n        raise NotImplementedError(\"Datediff function not defined on base class\")\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.NullLevelBase.__init__","title":"<code>__init__(col_name, valid_string_pattern=None, invalid_dates_as_null=False, valid_string_regex=None)</code>","text":"<p>Represents comparisons level where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Args:     col_name (str): Input column name     valid_string_pattern (str): pattern (regex or otherwise) that if not         matched will result in column being treated as a null.     invalid_dates_as_null (bool): If True, set all invalid dates to null.         The \"correct\" format of a date is set by valid_string_pattern.         Defaults to false.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Simple null comparison level </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n</code></pre> <p>Simple null level </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.spark.comparison_level_library as cll\ncll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n</code></pre> <p>Simple null level </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.athena.comparison_level_library as cll\ncll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n</code></pre> <p>Simple null level </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> <p>Simple null level </p><pre><code>import splink.postgres.comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name,\n    valid_string_pattern: str = None,\n    invalid_dates_as_null: bool = False,\n    valid_string_regex: str = None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents comparisons level where one or both sides of the comparison\n    contains null values so the similarity cannot be evaluated.\n    Assumed to have a partial match weight of zero (null effect\n    on overall match weight)\n    Args:\n        col_name (str): Input column name\n        valid_string_pattern (str): pattern (regex or otherwise) that if not\n            matched will result in column being treated as a null.\n        invalid_dates_as_null (bool): If True, set all invalid dates to null.\n            The \"correct\" format of a date is set by valid_string_pattern.\n            Defaults to false.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple null comparison level\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.null_level(\"name\")\n            ```\n            Null comparison level including strings that do not match\n            a given regex pattern\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Simple null level\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.null_level(\"name\")\n            ```\n            Null comparison level including strings that do not match\n            a given regex pattern\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Simple null level\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.null_level(\"name\")\n            ```\n            Null comparison level including strings that do not match\n            a given regex pattern\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.null_level(\"name\", valid_string_pattern=\"^[A-Z]{1,7}$\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Simple null level\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.null_level(\"name\")\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Simple null level\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.null_level(\"name\")\n            ```\n    Returns:\n        ComparisonLevel: Comparison level for null entries\n    \"\"\"\n\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"valid_string_regex\". Deprecation warning added in 3.9.6\n    if valid_string_pattern is not None and valid_string_regex is not None:\n        # user supplied both\n        raise TypeError(\"Just use valid_string_pattern\")\n    elif valid_string_pattern is not None:\n        # user is doing it correctly\n        pass\n    elif valid_string_regex is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"valid_string_regex is deprecated; use valid_string_pattern\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        valid_string_pattern = valid_string_regex\n\n    col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n    col_name_l, col_name_r = col.name_l, col.name_r\n\n    if invalid_dates_as_null:\n        # See https://github.com/moj-analytical-services/splink/pull/1939\n        col_name_l = self._valid_date_function(col_name_l, valid_string_pattern)\n        col_name_r = self._valid_date_function(col_name_r, valid_string_pattern)\n        sql = f\"\"\"{col_name_l} IS NULL OR {col_name_r} IS NULL\"\"\"\n    elif valid_string_pattern:\n        col_name_l = self._regex_extract_function(col_name_l, valid_string_pattern)\n        col_name_r = self._regex_extract_function(col_name_r, valid_string_pattern)\n        sql = f\"\"\"{col_name_l} IS NULL OR {col_name_r} IS NULL OR\n                  {col_name_l} = '' OR {col_name_r} = '' \"\"\"\n    else:\n        sql = f\"{col_name_l} IS NULL OR {col_name_r} IS NULL\"\n\n    level_dict = {\n        \"sql_condition\": sql,\n        \"label_for_charts\": \"Null\",\n        \"is_null_level\": True,\n    }\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ExactMatchLevelBase.__init__","title":"<code>__init__(col_name, regex_extract=None, set_to_lowercase=False, m_probability=None, term_frequency_adjustments=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None)</code>","text":"<p>Represents a comparison level where there is an exact match,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability Defaults to None.</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, include col_name in chart labels (e.g. linker.match_weights_chart())</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True. include_colname_in_charts_label=True</p> <code>None</code> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name,\n    regex_extract: str = None,\n    set_to_lowercase: bool = False,\n    m_probability=None,\n    term_frequency_adjustments=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level where there is an exact match,\n\n    Args:\n        col_name (str): Input column name\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        m_probability (float, optional): Starting value for m probability\n            Defaults to None.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        include_colname_in_charts_label (bool, optional): If True, include col_name\n            in chart labels (e.g. linker.match_weights_chart())\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n            include_colname_in_charts_label=True\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Simple Exact match level\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.exact_match_level(\"name\")\n            ```\n            Exact match level with term-frequency adjustments\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n            ```\n            Exact match level on a substring of col_name as\n             determined by a regular expression\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Simple Exact match level\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.exact_match_level(\"name\")\n            ```\n            Exact match level with term-frequency adjustments\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n            ```\n            Exact match level on a substring of col_name as\n             determined by a regular expression\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Simple Exact match level\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.exact_match_level(\"name\")\n            ```\n            Exact match level with term-frequency adjustments\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n            ```\n            Exact match level on a substring of col_name as\n             determined by a regular expression\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Simple Exact match level\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.exact_match_level(\"name\")\n            ```\n            Exact match level with term-frequency adjustments\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n        === \":simple-postgresql: PostgreSql\"\n            Simple Exact match level\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.exact_match_level(\"name\")\n            ```\n            Exact match level with term-frequency adjustments\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.exact_match_level(\"name\", term_frequency_adjustments=True)\n            ```\n    \"\"\"\n    col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n    if include_colname_in_charts_label:\n        label_suffix = f\" {col_name}\"\n    elif manual_col_name_for_charts_label:\n        label_suffix = f\" {manual_col_name_for_charts_label}\"\n    else:\n        label_suffix = \"\"\n\n    col_name_l, col_name_r = col.name_l, col.name_r\n\n    if set_to_lowercase:\n        col_name_l = f\"lower({col_name_l})\"\n        col_name_r = f\"lower({col_name_r})\"\n\n    if regex_extract:\n        col_name_l = self._regex_extract_function(col_name_l, regex_extract)\n        col_name_r = self._regex_extract_function(col_name_r, regex_extract)\n\n    sql_cond = f\"{col_name_l} = {col_name_r}\"\n    level_dict = {\n        \"sql_condition\": sql_cond,\n        \"label_for_charts\": f\"Exact match{label_suffix}\",\n    }\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n    if term_frequency_adjustments:\n        level_dict[\"tf_adjustment_column\"] = col_name\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ElseLevelBase.__init__","title":"<code>__init__(m_probability=None)</code>","text":"<p>Represents a comparison level for all cases which have not been considered by preceding comparison levels,</p> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.else_level()\n</code></pre> <pre><code>import splink.spark.comparison_level_library as cll\ncll.else_level()\n</code></pre> <pre><code>import splink.athena.comparison_level_library as cll\ncll.else_level()\n</code></pre> <p>``` python import splink.sqlite.comparison_level_library as cll cll.else_level()</p> <pre><code>import splink.postgres.comparison_level_library as cll\ncll.else_level()\n</code></pre> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level for all cases which have not been\n    considered by preceding comparison levels,\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.else_level()\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.else_level()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.else_level()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.else_level()\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.else_level()\n            ```\n    \"\"\"\n    if isinstance(m_probability, str):\n        raise ValueError(\n            \"You provided a string for the value of m probability when it should \"\n            \"be numeric.  Perhaps you passed a column name.  Note that you do \"\n            \"not need to pass a column name into the else level.\"\n        )\n    level_dict = {\n        \"sql_condition\": \"ELSE\",\n        \"label_for_charts\": \"All other comparisons\",\n    }\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n    super().__init__(level_dict)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceFunctionLevelBase.__init__","title":"<code>__init__(col_name, distance_function_name, distance_threshold, regex_extract=None, set_to_lowercase=False, higher_is_more_similar=True, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison level using a user-provided distance function, where the similarity</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_function_name</code> <code>str</code> <p>The name of the distance function</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>higher_is_more_similar</code> <code>bool</code> <p>If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein).</p> <code>True</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label.</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Apply the <code>levenshtein</code> function to a comparison level </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre> <p>Apply the <code>levenshtein</code> function to a comparison level </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre> <p>Apply the <code>levenshtein_distance</code> function to a comparison level </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein_distance\",\n                            2,\n                            False)\n</code></pre> <p>Apply the <code>levenshtein</code> function to a comparison level </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre> <p>Apply the <code>levenshtein</code> function to a comparison level </p><pre><code>import splink.postgres.comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level for a given distance function</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_function_name: str,\n    distance_threshold: int | float,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    higher_is_more_similar: bool = True,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level using a user-provided distance function,\n    where the similarity\n\n    Args:\n        col_name (str): Input column name\n        distance_function_name (str): The name of the distance function\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        higher_is_more_similar (bool): If True, a higher value of the\n            distance function indicates a higher similarity (e.g. jaro_winkler).\n            If false, a higher value indicates a lower similarity\n            (e.g. levenshtein).\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label.\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Apply the `levenshtein` function to a comparison level\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.distance_function_level(\"name\",\n                                        \"levenshtein\",\n                                        2,\n                                        False)\n            ```\n        === \":simple-apachespark: Spark\"\n            Apply the `levenshtein` function to a comparison level\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.distance_function_level(\"name\",\n                                        \"levenshtein\",\n                                        2,\n                                        False)\n            ```\n        === \":simple-amazonaws: Athena\"\n            Apply the `levenshtein_distance` function to a comparison level\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.distance_function_level(\"name\",\n                                        \"levenshtein_distance\",\n                                        2,\n                                        False)\n            ```\n        === \":simple-sqlite: SQLite\"\n            Apply the `levenshtein` function to a comparison level\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.distance_function_level(\"name\",\n                                        \"levenshtein\",\n                                        2,\n                                        False)\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Apply the `levenshtein` function to a comparison level\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.distance_function_level(\"name\",\n                                        \"levenshtein\",\n                                        2,\n                                        False)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level for a given distance function\n    \"\"\"\n    col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n    if higher_is_more_similar:\n        operator = \"&gt;=\"\n    else:\n        operator = \"&lt;=\"\n\n    col_name_l, col_name_r = col.name_l, col.name_r\n\n    if set_to_lowercase:\n        col_name_l = f\"lower({col_name_l})\"\n        col_name_r = f\"lower({col_name_r})\"\n\n    if regex_extract:\n        col_name_l = self._regex_extract_function(col_name_l, regex_extract)\n        col_name_r = self._regex_extract_function(col_name_r, regex_extract)\n\n    sql_cond = (\n        f\"{distance_function_name}({col_name_l}, {col_name_r}) \"\n        f\"{operator} {distance_threshold}\"\n    )\n\n    if include_colname_in_charts_label:\n        if manual_col_name_for_charts_label:\n            col_name = manual_col_name_for_charts_label\n\n        label_suffix = f\" {col_name}\"\n    else:\n        label_suffix = \"\"\n\n    chart_label = (\n        f\"{distance_function_name.capitalize()}{label_suffix} {operator} \"\n        f\"{distance_threshold}\"\n    )\n\n    level_dict = {\n        \"sql_condition\": sql_cond,\n        \"label_for_charts\": chart_label,\n    }\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison level using a levenshtein distance function,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Comparison level with levenshtein distance score less than (or equal  to) 1 </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 </p><pre><code>import splink.postgres.comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the levenshtein similarity</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold: int,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level using a levenshtein distance function,\n\n    Args:\n        col_name (str): Input column name\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1)\n            ```\n\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1 on a subtring of name column as determined by a regular\n            expression.\n            ```python\n            import splink.duckdb.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1)\n            ```\n\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1 on a subtring of name column as determined by a regular\n            expression.\n            ```python\n            import splink.spark.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1)\n            ```\n\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1 on a subtring of name column as determined by a regular\n            expression.\n            ```python\n            import splink.athena.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1)\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Comparison level with levenshtein distance score less than (or equal\n             to) 1\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.levenshtein_level(\"name\", 1)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the\n            levenshtein similarity\n    \"\"\"\n    super().__init__(\n        col_name,\n        distance_function_name=self._levenshtein_name,\n        distance_threshold=distance_threshold,\n        regex_extract=regex_extract,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=False,\n        include_colname_in_charts_label=include_colname_in_charts_label,\n        m_probability=m_probability,\n    )\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DamerauLevenshteinLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison level using a damerau-levenshtein distance function,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 on a subtring of name column as determined by a regular expression. </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1)\n</code></pre> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 on a subtring of name column as determined by a regular expression. </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the Damerau-Levenshtein similarity</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold: int,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level using a damerau-levenshtein distance\n    function,\n\n    Args:\n        col_name (str): Input column name\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level with damerau-levenshtein distance score less than\n            (or equal to) 1\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.damerau_levenshtein_level(\"name\", 1)\n            ```\n\n            Comparison level with damerau-levenshtein distance score less than\n            (or equal to) 1 on a subtring of name column as determined by a regular\n            expression.\n            ```python\n            import splink.duckdb.comparison_level_library as cll\n            cll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level with damerau-levenshtein distance score less than\n            (or equal to) 1\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.damerau_levenshtein_level(\"name\", 1)\n            ```\n\n            Comparison level with damerau-levenshtein distance score less than\n            (or equal to) 1 on a subtring of name column as determined by a regular\n            expression.\n            ```python\n            import splink.spark.comparison_level_library as cll\n            cll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Comparison level with damerau-levenshtein distance score less than\n            (or equal to) 1\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.damerau_levenshtein_level(\"name\", 1)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the\n            Damerau-Levenshtein similarity\n    \"\"\"\n    super().__init__(\n        col_name,\n        distance_function_name=self._damerau_levenshtein_name,\n        distance_threshold=distance_threshold,\n        regex_extract=regex_extract,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=False,\n        include_colname_in_charts_label=include_colname_in_charts_label,\n        m_probability=m_probability,\n    )\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison using the jaro distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Comparison level with jaro score greater than 0.9 </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9)\n</code></pre> Comparison level with a jaro score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with jaro score greater than 0.9 </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9)\n</code></pre> Comparison level with a jaro score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.spark.comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with jaro score greater than 0.9 </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaro similarity</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold: float,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n):\n    \"\"\"Represents a comparison using the jaro distance function\n\n    Args:\n        col_name (str): Input column name\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level with jaro score greater than 0.9\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaro_level(\"name\", 0.9)\n            ```\n            Comparison level with a jaro score greater than 0.9 on a substring\n            of name column as determined by a regular expression.\n\n            ```python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level with jaro score greater than 0.9\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.jaro_level(\"name\", 0.9)\n            ```\n            Comparison level with a jaro score greater than 0.9 on a substring\n            of name column as determined by a regular expression.\n\n            ```python\n            import splink.spark.comparison_level_library as cll\n            cll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Comparison level with jaro score greater than 0.9\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.jaro_level(\"name\", 0.9)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the\n            jaro similarity\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        self._jaro_name,\n        distance_threshold=distance_threshold,\n        regex_extract=regex_extract,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_colname_in_charts_label=include_colname_in_charts_label,\n        m_probability=m_probability,\n    )\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison level using the jaro winkler distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Comparison level with jaro-winkler score greater than 0.9 </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9)\n</code></pre> Comparison level with jaro-winkler score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with jaro-winkler score greater than 0.9 </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9)\n</code></pre> Comparison level with jaro-winkler score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.spark.comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with jaro-winkler score greater than 0.9 </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaro winkler similarity</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold: float,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level using the jaro winkler distance function\n\n    Args:\n        col_name (str): Input column name\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level with jaro-winkler score greater than 0.9\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaro_winkler_level(\"name\", 0.9)\n            ```\n            Comparison level with jaro-winkler score greater than 0.9 on a\n            substring of name column as determined by a regular expression.\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level with jaro-winkler score greater than 0.9\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.jaro_winkler_level(\"name\", 0.9)\n            ```\n            Comparison level with jaro-winkler score greater than 0.9 on a\n            substring of name column as determined by a regular expression.\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Comparison level with jaro-winkler score greater than 0.9\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.jaro_winkler_level(\"name\", 0.9)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the\n            jaro winkler similarity\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        self._jaro_winkler_name,\n        distance_threshold=distance_threshold,\n        regex_extract=regex_extract,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_colname_in_charts_label=include_colname_in_charts_label,\n        m_probability=m_probability,\n    )\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaccardLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, manual_col_name_for_charts_label=None, m_probability=None)</code>","text":"<p>Represents a comparison level using a jaccard distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>manual_col_name_for_charts_label</code> <code>str</code> <p>string to include as  column name in chart label. Acts as a manual overwrite of the  colname when include_colname_in_charts_label is True.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaccard similarity</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold: int | float,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    include_colname_in_charts_label=False,\n    manual_col_name_for_charts_label=None,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level using a jaccard distance function\n\n    Args:\n        col_name (str): Input column name\n        distance_threshold (Union[int, float]): The threshold to use to assess\n            similarity\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        include_colname_in_charts_label (bool, optional): If True, includes\n            col_name in charts label\n        manual_col_name_for_charts_label (str, optional): string to include as\n             column name in chart label. Acts as a manual overwrite of the\n             colname when include_colname_in_charts_label is True.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level with jaccard score greater than 0.9\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaccard_level(\"name\", 0.9)\n            ```\n            Comparison level with jaccard score greater than 0.9 on a\n            substring of name column as determined by a regular expression.\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level with jaccard score greater than 0.9\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.jaccard_level(\"name\", 0.9)\n            ```\n            Comparison level with jaccard score greater than 0.9 on a\n            substring of name column as determined by a regular expression.\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the jaccard similarity\n    \"\"\"\n    super().__init__(\n        col_name,\n        self._jaccard_name,\n        distance_threshold=distance_threshold,\n        regex_extract=regex_extract,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_colname_in_charts_label=include_colname_in_charts_label,\n        m_probability=m_probability,\n    )\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ColumnsReversedLevelBase.__init__","title":"<code>__init__(col_name_1, col_name_2, regex_extract=None, set_to_lowercase=False, m_probability=None, tf_adjustment_column=None)</code>","text":"<p>Represents a comparison level where the columns are reversed.  For example, if surname is in the forename field and vice versa</p> <p>Parameters:</p> Name Type Description Default <code>col_name_1</code> <code>str</code> <p>First column, e.g. forename</p> required <code>col_name_2</code> <code>str</code> <p>Second column, e.g. surname</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>tf_adjustment_column</code> <code>str</code> <p>Column to use for term frequency adjustments if an exact match is observed. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Comparison level on first_name and surname columns reversed </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level on first_name and surname columns reversed </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.spark.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level on first_name and surname columns reversed </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.athena.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level on first_name and surname columns reversed </p><pre><code>import splink.sqlite.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> <pre><code>import splink.postgres.comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the exact match of two columns.</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name_1: str,\n    col_name_2: str,\n    regex_extract: str = None,\n    set_to_lowercase=False,\n    m_probability=None,\n    tf_adjustment_column=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level where the columns are reversed.  For example,\n    if surname is in the forename field and vice versa\n\n    Args:\n        col_name_1 (str): First column, e.g. forename\n        col_name_2 (str): Second column, e.g. surname\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n        tf_adjustment_column (str, optional): Column to use for term frequency\n            adjustments if an exact match is observed. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Comparison level on first_name and surname columns reversed\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\", \"surname\")\n            ```\n            Comparison level on first_name and surname column reversed\n            on a substring of each column as determined by a regular expression.\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\",\n                                       \"surname\",\n                                       regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Comparison level on first_name and surname columns reversed\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\", \"surname\")\n            ```\n            Comparison level on first_name and surname column reversed\n            on a substring of each column as determined by a regular expression.\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\",\n                                       \"surname\",\n                                       regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Comparison level on first_name and surname columns reversed\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\", \"surname\")\n            ```\n            Comparison level on first_name and surname column reversed\n            on a substring of each column as determined by a regular expression.\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\",\n                                       \"surname\",\n                                       regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Comparison level on first_name and surname columns reversed\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\", \"surname\")\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.columns_reversed_level(\"first_name\", \"surname\")\n            ```\n\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the exact match of two\n            columns.\n    \"\"\"\n\n    col_1 = InputColumn(col_name_1, sql_dialect=self._sql_dialect)\n    col_2 = InputColumn(col_name_2, sql_dialect=self._sql_dialect)\n\n    col_1_l, col_1_r = col_1.name_l, col_1.name_r\n    col_2_l, col_2_r = col_2.name_l, col_2.name_r\n\n    if set_to_lowercase:\n        col_1_l = f\"lower({col_1_l})\"\n        col_1_r = f\"lower({col_1_r})\"\n        col_2_l = f\"lower({col_2_l})\"\n        col_2_r = f\"lower({col_2_r})\"\n\n    if regex_extract:\n        col_1_l = self._regex_extract_function(col_1_l, regex_extract)\n        col_1_r = self._regex_extract_function(col_1_r, regex_extract)\n        col_2_l = self._regex_extract_function(col_2_l, regex_extract)\n        col_2_r = self._regex_extract_function(col_2_r, regex_extract)\n\n    s = f\"{col_1_l} = {col_2_r} and \" f\"{col_1_r} = {col_2_l}\"\n    level_dict = {\n        \"sql_condition\": s,\n        \"label_for_charts\": \"Exact match on reversed cols\",\n    }\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n\n    if tf_adjustment_column:\n        level_dict[\"tf_adjustment_column\"] = tf_adjustment_column\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceInKMLevelBase.__init__","title":"<code>__init__(lat_col, long_col, km_threshold, not_null=False, m_probability=None)</code>","text":"<p>Use the haversine formula to transform comparisons of lat,lngs into distances measured in kilometers</p> <p>Parameters:</p> Name Type Description Default <code>lat_col</code> <code>str</code> <p>The name of a latitude column or the respective array or struct column column containing the information For example: long_lat['lat'] or long_lat[0]</p> required <code>long_col</code> <code>str</code> <p>The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long_lat['long'] or long_lat[1]</p> required <code>km_threshold</code> <code>int</code> <p>The total distance in kilometers to evaluate your comparisons against</p> required <code>not_null</code> <code>bool</code> <p>If true, remove any . This is only necessary if you are not capturing nulls elsewhere in your comparison level.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena PostgreSql <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <pre><code>import splink.spark.comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <pre><code>import splink.athena.comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <pre><code>import splink.postgres.comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the distance between two coordinates</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    lat_col: str,\n    long_col: str,\n    km_threshold: int | float,\n    not_null: bool = False,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Use the haversine formula to transform comparisons of lat,lngs\n    into distances measured in kilometers\n\n    Arguments:\n        lat_col (str): The name of a latitude column or the respective array\n            or struct column column containing the information\n            For example: long_lat['lat'] or long_lat[0]\n        long_col (str): The name of a longitudinal column or the respective array\n            or struct column column containing the information, plus an index.\n            For example: long_lat['long'] or long_lat[1]\n        km_threshold (int): The total distance in kilometers to evaluate your\n            comparisons against\n        not_null (bool): If true, remove any . This is only necessary if you are not\n            capturing nulls elsewhere in your comparison level.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.distance_in_km_level(\"lat_col\",\n                                    \"long_col\",\n                                    km_threshold=5)\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.distance_in_km_level(\"lat_col\",\n                                    \"long_col\",\n                                    km_threshold=5)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.distance_in_km_level(\"lat_col\",\n                                    \"long_col\",\n                                    km_threshold=5)\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.distance_in_km_level(\"lat_col\",\n                                    \"long_col\",\n                                    km_threshold=5)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the distance between\n            two coordinates\n    \"\"\"\n\n    lat = InputColumn(lat_col, sql_dialect=self._sql_dialect)\n    long = InputColumn(long_col, sql_dialect=self._sql_dialect)\n    lat_l, lat_r = lat.names_l_r\n    long_l, long_r = long.names_l_r\n\n    distance_km_sql = f\"\"\"\n    {great_circle_distance_km_sql(lat_l, lat_r, long_l, long_r)} &lt;= {km_threshold}\n    \"\"\"\n\n    if not_null:\n        null_sql = \" AND \".join(\n            [f\"{c} is not null\" for c in [lat_r, lat_l, long_l, long_r]]\n        )\n        distance_km_sql = f\"({null_sql}) AND {distance_km_sql}\"\n\n    level_dict = {\n        \"sql_condition\": distance_km_sql,\n        \"label_for_charts\": f\"Distance less than {km_threshold}km\",\n    }\n\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.PercentageDifferenceLevelBase.__init__","title":"<code>__init__(col_name, percentage_distance_threshold, m_probability=None)</code>","text":"<p>Represents a comparison level based around the percentage difference between two numbers.</p> <p>Note: the percentage is calculated by dividing the absolute difference between the values by the largest value</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>percentage_distance_threshold</code> <code>float</code> <p>Percentage difference threshold for the comparison level</p> required <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.spark.comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.athena.comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.sqlite.comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.postgres.comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the percentage difference between two values</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    percentage_distance_threshold: float,\n    m_probability=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level based around the percentage difference between\n    two numbers.\n\n    Note: the percentage is calculated by dividing the absolute difference between\n    the values by the largest value\n\n    Args:\n        col_name (str): Input column name\n        percentage_distance_threshold (float): Percentage difference threshold for\n            the comparison level\n        m_probability (float, optional): Starting value for m probability. Defaults\n            to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.percentage_difference_level(\"value\", 0.5)\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.percentage_difference_level(\"value\", 0.5)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.percentage_difference_level(\"value\", 0.5)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ``` python\n            import splink.sqlite.comparison_level_library as cll\n            cll.percentage_difference_level(\"value\", 0.5)\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.percentage_difference_level(\"value\", 0.5)\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the percentage difference\n            between two values\n\n    \"\"\"\n    col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n    s = f\"\"\"(abs({col.name_l} - {col.name_r})/\n        (case\n            when {col.name_r} &gt; {col.name_l}\n            then {col.name_r}\n            else {col.name_l}\n        end))\n        &lt; {percentage_distance_threshold}\"\"\"\n\n    level_dict = {\n        \"sql_condition\": s,\n        \"label_for_charts\": f\"&lt; {percentage_distance_threshold:,.2%} diff\",\n    }\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ArrayIntersectLevelBase.__init__","title":"<code>__init__(col_name, m_probability=None, term_frequency_adjustments=False, min_intersection=1, include_colname_in_charts_label=False)</code>","text":"<p>Represents a comparison level based around the size of an intersection of arrays</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>min_intersection</code> <code>int</code> <p>The minimum cardinality of the intersection of arrays for this comparison level. Defaults to 1</p> <code>1</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>Should the charts label contain the column name? Defaults to False</p> <code>False</code> <p>Examples:</p>  DuckDB Spark Athena PostgreSql <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <pre><code>import splink.spark.comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <pre><code>import splink.athena.comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <pre><code>import splink.postgres.comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the size of intersection of arrays</p> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    col_name,\n    m_probability=None,\n    term_frequency_adjustments=False,\n    min_intersection=1,\n    include_colname_in_charts_label=False,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level based around the size of an intersection of\n    arrays\n\n    Args:\n        col_name (str): Input column name\n        m_probability (float, optional): Starting value for m probability. Defaults\n            to None.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        min_intersection (int, optional): The minimum cardinality of the\n            intersection of arrays for this comparison level. Defaults to 1\n        include_colname_in_charts_label (bool, optional): Should the charts label\n            contain the column name? Defaults to False\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.array_intersect_level(\"name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.array_intersect_level(\"name\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.array_intersect_level(\"name\")\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.array_intersect_level(\"name\")\n            ```\n\n    Returns:\n        ComparisonLevel: A comparison level that evaluates the size of intersection\n            of arrays\n    \"\"\"\n    col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n\n    size_array_intersection = (\n        f\"{self._size_array_intersect_function(col.name_l, col.name_r)}\"\n    )\n    sql = f\"{size_array_intersection} &gt;= {min_intersection}\"\n\n    label_prefix = (\n        f\"{col_name} arrays\" if include_colname_in_charts_label else \"Arrays\"\n    )\n    if min_intersection == 1:\n        label = f\"{label_prefix} intersect\"\n    else:\n        label = f\"{label_prefix} intersect size &gt;= {min_intersection}\"\n\n    level_dict = {\"sql_condition\": sql, \"label_for_charts\": label}\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n    if term_frequency_adjustments:\n        level_dict[\"tf_adjustment_column\"] = col_name\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DatediffLevelBase.__init__","title":"<code>__init__(date_col, date_threshold, date_metric='day', m_probability=None, cast_strings_to_date=False, date_format=None)</code>","text":"<p>Represents a comparison level based around the difference between dates within a column</p> <p>Parameters:</p> Name Type Description Default <code>date_col</code> <code>str</code> <p>Input column name</p> required <code>date_threshold</code> <code>int</code> <p>The total difference in time between two given dates. This is used in tandem with <code>date_metric</code> to determine . If you are using <code>year</code> as your metric, then a value of 1 would require that your dates lie within 1 year of one another.</p> required <code>date_metric</code> <code>str</code> <p>The unit of time with which to measure your <code>date_threshold</code>. Your metric should be one of <code>day</code>, <code>month</code> or <code>year</code>. Defaults to <code>day</code>.</p> <code>'day'</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to true and adjust date_format param when input dates are strings to enable date-casting. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd).</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena PostgreSql <p>Date Difference comparison level at threshold 1 year </p><pre><code>import splink.duckdb.comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.duckdb.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y'\n                    )\n</code></pre> <p>Date Difference comparison level at threshold 1 year </p><pre><code>import splink.spark.comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.spark.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.spark.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y'\n                    )\n</code></pre> <p>Date Difference comparison level at threshold 1 year </p><pre><code>import splink.athena.comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.athena.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.athena.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y'\n                    )\n</code></pre> <p>Date Difference comparison level at threshold 1 year </p><pre><code>import splink.postgres.comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = yyyy-MM-dd) <pre><code>import splink.postgres.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.postgres.comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='dd/MM/yyyy'\n                    )\n</code></pre> Source code in <code>splink/comparison_level_library.py</code> <pre><code>def __init__(\n    self,\n    date_col: str,\n    date_threshold: int,\n    date_metric: str = \"day\",\n    m_probability=None,\n    cast_strings_to_date=False,\n    date_format=None,\n) -&gt; ComparisonLevel:\n    \"\"\"Represents a comparison level based around the difference between dates\n    within a column\n\n    Arguments:\n        date_col (str): Input column name\n        date_threshold (int): The total difference in time between two given\n            dates. This is used in tandem with `date_metric` to determine .\n            If you are using `year` as your metric, then a value of 1 would\n            require that your dates lie within 1 year of one another.\n        date_metric (str): The unit of time with which to measure your\n            `date_threshold`.\n            Your metric should be one of `day`, `month` or `year`.\n            Defaults to `day`.\n        m_probability (float, optional): Starting value for m probability.\n            Defaults to None.\n        cast_strings_to_date (bool, optional): Set to true and adjust\n            date_format param when input dates are strings to enable\n            date-casting. Defaults to False.\n        date_format (str, optional): Format of input dates if date-strings\n            are given. Must be consistent across record pairs. If None\n            (the default), downstream functions for each backend assign\n            date_format to ISO 8601 format (yyyy-mm-dd).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Date Difference comparison level at threshold 1 year\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.datediff_level(\"date\",\n                                date_threshold=1,\n                                date_metric=\"year\"\n                                )\n            ```\n            Date Difference comparison with date-casting and unspecified\n            date_format (default = %Y-%m-%d)\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True\n                                )\n            ```\n            Date Difference comparison with date-casting and specified date_format\n            ``` python\n            import splink.duckdb.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True,\n                                date_format='%d/%m/%Y'\n                                )\n            ```\n        === \":simple-apachespark: Spark\"\n            Date Difference comparison level at threshold 1 year\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.datediff_level(\"date\",\n                                date_threshold=1,\n                                date_metric=\"year\"\n                                )\n            ```\n            Date Difference comparison with date-casting and unspecified\n            date_format (default = %Y-%m-%d)\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True\n                                )\n            ```\n            Date Difference comparison with date-casting and specified date_format\n            ``` python\n            import splink.spark.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True,\n                                date_format='%d/%m/%Y'\n                                )\n            ```\n        === \":simple-amazonaws: Athena\"\n            Date Difference comparison level at threshold 1 year\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.datediff_level(\"date\",\n                                date_threshold=1,\n                                date_metric=\"year\"\n                                )\n            ```\n            Date Difference comparison with date-casting and unspecified\n            date_format (default = %Y-%m-%d)\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True\n                                )\n            ```\n            Date Difference comparison with date-casting and specified date_format\n            ``` python\n            import splink.athena.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True,\n                                date_format='%d/%m/%Y'\n                                )\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Date Difference comparison level at threshold 1 year\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.datediff_level(\"date\",\n                                date_threshold=1,\n                                date_metric=\"year\"\n                                )\n            ```\n            Date Difference comparison with date-casting and unspecified\n            date_format (default = yyyy-MM-dd)\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True\n                                )\n            ```\n            Date Difference comparison with date-casting and specified date_format\n            ``` python\n            import splink.postgres.comparison_level_library as cll\n            cll.datediff_level(\"dob\",\n                                date_threshold=3,\n                                date_metric='month',\n                                cast_strings_to_date=True,\n                                date_format='dd/MM/yyyy'\n                                )\n            ```\n    Returns:\n        ComparisonLevel: A comparison level that evaluates whether two dates fall\n            within a given interval.\n    \"\"\"\n\n    date = InputColumn(date_col, sql_dialect=self._sql_dialect)\n    date_l, date_r = date.names_l_r\n\n    datediff_sql = self._datediff_function(\n        date_l,\n        date_r,\n        date_threshold,\n        date_metric,\n        cast_strings_to_date,\n        date_format,\n    )\n    label = f\"Within {date_threshold} {date_metric}\"\n    if date_threshold &gt; 1:\n        label += \"s\"\n\n    level_dict = {\n        \"sql_condition\": datediff_sql,\n        \"label_for_charts\": label,\n    }\n\n    if m_probability:\n        level_dict[\"m_probability\"] = m_probability\n\n    super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_library.html","title":"Comparison Library","text":"","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#documentation-for-comparison_library","title":"Documentation for <code>comparison_library</code>","text":"<p>The <code>comparison_library</code> contains pre-made comparisons available for use directly as described in this topic guide. However, not every comparison is available for every Splink-compatible SQL backend.</p> <p>The pre-made Splink comparisons available for each SQL dialect are as given in this table:</p>  DuckDB  Spark  Athena  SQLite  PostgreSql array_intersect_at_sizes \u2713 \u2713 \u2713 \u2713 damerau_levenshtein_at_thresholds \u2713 \u2713 \u2713 datediff_at_thresholds \u2713 \u2713 \u2713 \u2713 distance_function_at_thresholds \u2713 \u2713 \u2713 \u2713 \u2713 distance_in_km_at_thresholds \u2713 \u2713 \u2713 \u2713 exact_match \u2713 \u2713 \u2713 \u2713 \u2713 jaccard_at_thresholds \u2713 \u2713 jaro_at_thresholds \u2713 \u2713 \u2713 jaro_winkler_at_thresholds \u2713 \u2713 \u2713 levenshtein_at_thresholds \u2713 \u2713 \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#library-comparison-apis","title":"Library comparison APIs","text":"<p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class ExactMatchBase(Comparison):\n    def __init__(\n        self,\n        col_name,\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_else=None,\n        include_colname_in_charts_label=False,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with two levels:\n\n        - Exact match\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            term_frequency_adjustments (bool, optional): If True, term frequency\n                adjustments will be made on the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n            include_colname_in_charts_label: If true, append col name to label for\n                charts.  Defaults to False.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with exact match level\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.exact_match(\"first_name\")\n                ```\n                Create comparison with exact match level based on a\n                substring of first_name as determined by a regular expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with exact match level\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.exact_match(\"first_name\")\n                ```\n                Create comparison with exact match level based on a\n                substring of first_name as determined by a regular expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Create comparison with exact match level\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.exact_match(\"first_name\")\n                ```\n                Create comparison with exact match level based on a\n                substring of first_name as determined by a regular expression\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Create comparison with exact match level\n                ``` python\n                import splink.sqlite.comparison_library as cl\n                cl.exact_match(\"first_name\")\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Create comparison with exact match level\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.exact_match(\"first_name\")\n                ```\n\n        Returns:\n            Comparison: A comparison for exact match that can be included in the Splink\n                settings dictionary\n        \"\"\"\n\n        comparison_dict = {\n            \"comparison_description\": \"Exact match vs. anything else\",\n            \"comparison_levels\": [\n                self._null_level(col_name, valid_string_pattern),\n                self._exact_match_level(\n                    col_name,\n                    regex_extract=regex_extract,\n                    set_to_lowercase=set_to_lowercase,\n                    term_frequency_adjustments=term_frequency_adjustments,\n                    m_probability=m_probability_exact_match,\n                    include_colname_in_charts_label=include_colname_in_charts_label,\n                ),\n                self._else_level(m_probability=m_probability_else),\n            ],\n        }\n        super().__init__(comparison_dict)\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class DistanceFunctionAtThresholdsBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        distance_function_name: str,\n        distance_threshold_or_thresholds: float | list,\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        higher_is_more_similar: bool = True,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_thres: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with a user-provided distance\n        function used to assess middle similarity levels.\n\n        The user-provided distance function must exist in the SQL backend.\n\n        An example of the output with default arguments and setting\n        `distance_function_name` to `jaccard` and\n        `distance_threshold_or_thresholds = [0.9,0.7]` would be\n\n        - Exact match\n        - Jaccard distance &lt;= 0.9\n        - Jaccard distance &lt;= 0.7\n        - Anything else\n\n        Note: distance_function_at_thresholds() is primarily used in the\n        backend to create the out-of-the-box cl.XXX_at_thresholds() functions\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_function_name (str): The name of the distance function.\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to [1, 2].\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            set_to_lowercase (bool): If True, sets all entries to lowercase.\n            higher_is_more_similar (bool): If True, a higher value of the distance\n                function indicates a higher similarity (e.g. jaro_winkler).\n                If false, a higher value indicates a lower similarity\n                (e.g. levenshtein).\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_thres (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Apply the `jaccard` function in a comparison with levels 0.9 and 0.7\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'jaccard',\n                                   distance_threshold_or_thresholds = [0.9, 0.7]\n                                   )\n                ```\n                Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n                a substring of name column as determined by a regular expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'jaccard',\n                                   distance_threshold_or_thresholds = [0.9, 0.7],\n                                   regex_extract=\"^[A-Z]{1,4}\n                                   )\n                ```\n            === \":simple-apachespark: Spark\"\n                Apply the `jaccard` function in a comparison with levels 0.9 and 0.7\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'jaccard',\n                                   distance_threshold_or_thresholds = [0.9, 0.7]\n                                   )\n                ```\n                Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n                a substring of name column as determined by a regular expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'jaccard',\n                                   distance_threshold_or_thresholds = [0.9, 0.7],\n                                   regex_extract=\"^[A-Z]{1,4}\n                                   )\n                ```\n            === \":simple-amazonaws: Athena\"\n                Apply the `levenshtein_distance` function in a comparison with\n                levels 1 and 2\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'levenshtein_distance',\n                                   distance_threshold_or_thresholds = [1, 2],\n                                   higher_is_more_similar = False\n                                   )\n                ```\n                Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n                a substring of name column as determined by a regular expression\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'jaccard',\n                                   distance_threshold_or_thresholds = [0.9, 0.7],\n                                   regex_extract=\"^[A-Z]{1,4}\n                                   )\n                ```\n            === \":simple-sqlite: SQLite\"\n                Apply the `levenshtein` function in a comparison with\n                levels 1 and 2\n                ``` python\n                import splink.sqlite.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'levenshtein',\n                                   distance_threshold_or_thresholds = [1, 2],\n                                   higher_is_more_similar = False\n                                   )\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Apply the `levenshtein` function in a comparison with\n                levels 1 and 2\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.distance_function_at_thresholds(\"name\",\n                                   distance_function_name = 'levenshtein',\n                                   distance_threshold_or_thresholds = [1, 2],\n                                   higher_is_more_similar = False\n                                   )\n                ```\n                ```\n\n        Returns:\n            Comparison: A comparison for a chosen distance function similarity that\n                can be included in the Splink settings dictionary.\n        \"\"\"\n        # Validate user inputs\n\n        distance_threshold_or_thresholds = ensure_is_iterable(\n            distance_threshold_or_thresholds\n        )\n\n        comparison_at_thresholds_error_logger(\n            distance_function_name, distance_threshold_or_thresholds\n        )\n\n        comparison_levels = []\n        comparison_levels.append(self._null_level(col_name, valid_string_pattern))\n        if include_exact_match_level:\n            level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability=m_probability_exact_match,\n            )\n            comparison_levels.append(level)\n\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name,\n            distance_threshold_or_thresholds,\n            regex_extract,\n            set_to_lowercase,\n            m_probability_or_probabilities_thres,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct comparison description\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        threshold_desc = distance_threshold_description(\n            col_name, distance_function_name, distance_threshold_or_thresholds\n        )\n        comparison_desc += threshold_desc\n\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _is_distance_subclass(self):\n        return False\n</code></pre> <p>             Bases: <code>DistanceFunctionAtThresholdsBase</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class LevenshteinAtThresholdsBase(DistanceFunctionAtThresholdsBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold_or_thresholds: int | list = [1, 2],\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_lev: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with the levenshtein distance used to\n        assess middle similarity levels.\n\n        An example of the output with default arguments and setting\n        `distance_threshold_or_thresholds = [1,2]` would be\n\n        - Exact match\n        - Levenshtein distance &lt;= 1\n        - Levenshtein distance &lt;= 2\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to [1, 2].\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_lev (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the thresholds specified for given function. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                on a substring of name column as determined by a regular expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                on a substring of name column as determined by a regular expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                on a substring of name column as determined by a regular expression\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Create comparison with levenshtein match levels with distance &lt;=1\n                and &lt;=2\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                ```\n\n        Returns:\n            Comparison: A comparison for Levenshtein similarity that can be included\n                in the Splink settings dictionary.\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            distance_function_name=self._levenshtein_name,\n            distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n            regex_extract=regex_extract,\n            valid_string_pattern=valid_string_pattern,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=False,\n            include_exact_match_level=include_exact_match_level,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability_exact_match=m_probability_exact_match,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n            m_probability_else=m_probability_else,\n        )\n\n    @property\n    def _is_distance_subclass(self):\n        return True\n</code></pre> <p>             Bases: <code>DistanceFunctionAtThresholdsBase</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class DamerauLevenshteinAtThresholdsBase(DistanceFunctionAtThresholdsBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold_or_thresholds: int | list = 1,\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_dl: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with the damerau-levenshtein distance\n        used to assess middle similarity levels.\n\n        An example of the output with default arguments and setting\n        `distance_threshold_or_thresholds = [1]` would be\n\n        - Exact match\n        - Damerau-Levenshtein distance &lt;= 1\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to 1.\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (_type_, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified for given function. Defaults to None.\n            m_probability_else (_type_, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with damerau-levenshtein match levels with\n                distance &lt;= 1, 2\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with damerau-levenshtein match levels with\n                distance &lt;= 1\n                on a substring of name column as determined by a regular expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                                     [1,2],\n                                                     regex_extract=\"^A|B\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with damerau-levenshtein match levels with\n                distance &lt;= 1, 2\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with damerau-evenshtein match levels with\n                distance &lt;= 1, 2\n                on a substring of name column as determined by a regular expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                                     [1,2],\n                                                     regex_extract=\"^A|B\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Create comparison with damerau-levenshtein match levels with\n                distance &lt;= 1, 2\n                ``` python\n                import splink.sqlite.comparison_library as cl\n                cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n                ```\n\n        Returns:\n            Comparison: A comparison for Damerau-Levenshtein similarity that can be\n            included in the Splink settings dictionary.\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            distance_function_name=self._levenshtein_name,\n            distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n            regex_extract=regex_extract,\n            valid_string_pattern=valid_string_pattern,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=False,\n            include_exact_match_level=include_exact_match_level,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability_exact_match=m_probability_exact_match,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n            m_probability_else=m_probability_else,\n        )\n\n    @property\n    def _is_distance_subclass(self):\n        return True\n</code></pre> <p>             Bases: <code>DistanceFunctionAtThresholdsBase</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class JaccardAtThresholdsBase(DistanceFunctionAtThresholdsBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_jac: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with the jaccard distance used to\n        assess middle similarity levels.\n\n        An example of the output with default arguments and setting\n        `distance_threshold_or_thresholds = [0.9,0.7]` would be\n\n        - Exact match\n        - Jaccard distance &lt;= 0.9\n        - Jaccard distance &lt;= 0.7\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to [0.9, 0.7].\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_jac (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the thresholds specified for given function. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with jaccard match levels with similarity score &gt;=0.9\n                and &gt;=0.7\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaccard_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with jaccard match levels with similarity score &gt;=0.9\n                and &gt;=0.7 on a substring of name column as determined by a regular\n                expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with jaccard match levels with similarity score &gt;=0.9\n                and &gt;=0.7\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaccard_at_thresholds(\"first_name\", [1,2])\n                ```\n                Create comparison with jaccard match levels with similarity score &gt;=0.9\n                and &gt;=0.7 on a substring of name column as determined by a regular\n                expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n                ```\n\n        Returns:\n            Comparison: A comparison for Jaccard similarity that can be included\n                in the Splink settings dictionary.\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            distance_function_name=self._jaccard_name,\n            distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n            regex_extract=regex_extract,\n            valid_string_pattern=valid_string_pattern,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_exact_match_level=include_exact_match_level,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability_exact_match=m_probability_exact_match,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jac,\n            m_probability_else=m_probability_else,\n        )\n\n    @property\n    def _is_distance_subclass(self):\n        return True\n</code></pre> <p>             Bases: <code>DistanceFunctionAtThresholdsBase</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class JaroAtThresholdsBase(DistanceFunctionAtThresholdsBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_jar: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with the jaro distance used to\n        assess middle similarity levels.\n\n        An example of the output with default arguments and setting\n        `distance_threshold_or_thresholds = [0.9, 0.7]` would be\n\n        - Exact match\n        - Jaro distance &lt;= 0.9\n        - Jaro distance &lt;= 0.7\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to [0.9, 0.7].\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_jar (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the thresholds specified for given function. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with jaro match levels with similarity score &gt;=0.9\n                and &gt;=0.7\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n                Create comparison with jaro match levels with similarity score &gt;=0.9\n                and &gt;=0.7 on a substring of name column as determined by a regular\n                expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with jaro match levels with similarity score &gt;=0.9\n                and &gt;=0.7\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n                Create comparison with jaro match levels with similarity score &gt;=0.9\n                and &gt;=0.7 on a substring of name column as determined by a regular\n                expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                Create comparison with jaro match levels with similarity score &gt;=0.9\n                and &gt;=0.7\n                ``` python\n                import splink.sqlite.comparison_library as cl\n                cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n\n        Returns:\n            Comparison:\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            distance_function_name=self._jaro_name,\n            distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n            regex_extract=regex_extract,\n            valid_string_pattern=valid_string_pattern,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_exact_match_level=include_exact_match_level,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability_exact_match=m_probability_exact_match,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jar,\n            m_probability_else=m_probability_else,\n        )\n\n    @property\n    def _is_distance_subclass(self):\n        return True\n</code></pre> <p>             Bases: <code>DistanceFunctionAtThresholdsBase</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class JaroWinklerAtThresholdsBase(DistanceFunctionAtThresholdsBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n        regex_extract: str = None,\n        valid_string_pattern: str = None,\n        set_to_lowercase: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_jw: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in `col_name` with the jaro_winkler distance used to\n        assess middle similarity levels.\n\n        An example of the output with default arguments and setting\n        `distance_threshold_or_thresholds = [0.9, 0.7]` would be\n\n        - Exact match\n        - Jaro-Winkler distance &lt;= 0.9\n        - Jaro-Winkler distance &lt;= 0.7\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare\n            distance_threshold_or_thresholds (Union[int, list], optional): The\n                threshold(s) to use for the middle similarity level(s).\n                Defaults to [0.9, 0.7].\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            valid_string_pattern (str): regular expression pattern that if not\n                matched will result in column being treated as a null.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_jw (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the thresholds specified for given function. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Create comparison with jaro_winkler match levels with similarity\n                score &gt;= 0.9 and &gt;=0.7\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n                Create comparison with jaro_winkler match levels with similarity\n                score =&gt;0.9 and &gt;=0.7 on a substring of name column as determined by\n                a regular expression\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.jaro_winkler_at_thresholds(\"first_name\",\n                                              [0.9, 0.7],\n                                              regex_extract=\"^[A-Z]\"\n                                              )\n                ```\n            === \":simple-apachespark: Spark\"\n                Create comparison with jaro_winkler match levels with similarity\n                score &gt;=0.9 and &gt;=0.7\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n                Create comparison with jaro_winkler match levels with similarity\n                score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined\n                by a regular expression\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.jaro_winkler_at_thresholds(\"first_name\",\n                                              [0.9, 0.7],\n                                              regex_extract=\"^[A-Z]\"\n                                              )\n                ```\n            === \":simple-sqlite: SQLite\"\n                Create comparison with jaro_winkler match levels with similarity\n                score &gt;=0.9 and &gt;=0.7\n                ``` python\n                import splink.sqlite.comparison_library as cl\n                cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n                ```\n\n        Returns:\n            Comparison: A comparison for Jaro Winkler similarity that can be included\n                in the Splink settings dictionary.\n        \"\"\"\n\n        super().__init__(\n            col_name,\n            distance_function_name=self._jaro_winkler_name,\n            distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n            regex_extract=regex_extract,\n            valid_string_pattern=valid_string_pattern,\n            set_to_lowercase=set_to_lowercase,\n            higher_is_more_similar=True,\n            include_exact_match_level=include_exact_match_level,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability_exact_match=m_probability_exact_match,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jw,\n            m_probability_else=m_probability_else,\n        )\n\n    @property\n    def _is_distance_subclass(self):\n        return True\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class ArrayIntersectAtSizesBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        size_or_sizes: int | list = [1],\n        m_probability_or_probabilities_sizes: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in array column `col_name` with various\n        intersection sizes to assess similarity levels.\n\n        An example of the output with default arguments and setting\n        `size_or_sizes = [3, 1]` would be\n\n        - Intersection has at least 3 elements\n        - Intersection has at least 1 element (i.e. 1 or 2)\n        - Anything else (i.e. empty intersection)\n\n        Args:\n            col_name (str): The name of the column to compare\n            size_or_sizes (Union[int, list], optional): The size(s) of intersection\n                to use for the non-'else' similarity level(s). Should be in\n                descending order. Defaults to [1].\n            m_probability_or_probabilities_sizes (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the sizes specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n                ```\n\n        Returns:\n            Comparison: A comparison for the intersection of arrays that can be included\n                in the Splink settings dictionary.\n        \"\"\"\n\n        sizes = ensure_is_iterable(size_or_sizes)\n        if len(sizes) == 0:\n            raise ValueError(\n                \"`size_or_sizes` must have at least one element, so that Comparison \"\n                \"has more than just an 'else' level\"\n            )\n        if any(size &lt;= 0 for size in sizes):\n            raise ValueError(\"All entries of `size_or_sizes` must be postive\")\n\n        if m_probability_or_probabilities_sizes is None:\n            m_probability_or_probabilities_sizes = [None] * len(sizes)\n        m_probabilities = ensure_is_iterable(m_probability_or_probabilities_sizes)\n\n        comparison_levels = []\n        comparison_levels.append(self._null_level(col_name))\n\n        for size_intersect, m_prob in zip(sizes, m_probabilities):\n            level = self._array_intersect_level(\n                col_name, m_probability=m_prob, min_intersection=size_intersect\n            )\n            comparison_levels.append(level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        comparison_desc = \"\"\n\n        size_desc = \", \".join([str(s) for s in sizes])\n        plural = \"\" if len(sizes) == 1 else \"s\"\n        comparison_desc += (\n            f\"Array intersection at minimum size{plural} {size_desc} vs. \"\n        )\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _array_intersect_level(self):\n        raise NotImplementedError(\"Intersect level not defined on base class\")\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class DatediffAtThresholdsBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        date_thresholds: int | list = [1],\n        date_metrics: str | list = [\"year\"],\n        cast_strings_to_date=False,\n        date_format: str = None,\n        invalid_dates_as_null: bool = False,\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_dat: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the data in the date column `col_name` with various\n        date thresholds and metrics to assess similarity levels.\n\n        An example of the output with default arguments and settings\n        `date_thresholds = [1]` and `date_metrics = ['day']` would be\n        - The two input dates are within 1 day of one another\n        - Anything else (i.e. all other dates lie outside this range)\n\n        `date_thresholds` and `date_metrics` should be used in conjunction\n        with one another.\n        For example, `date_thresholds = [10, 12, 15]` with\n        `date_metrics = ['day', 'month', 'year']` would result in the following checks:\n\n        - The two dates are within 10 days of one another\n        - The two dates are within 12 months of one another\n        - And the two dates are within 15 years of one another\n\n        Args:\n            col_name (str): The name of the date column to compare.\n            date_thresholds (Union[int, list], optional): The size(s) of given date\n                thresholds, to assess whether two dates fall within a given time\n                interval.\n                These values can be any integer value and should be used in tandem with\n                `date_metrics`.\n            date_metrics (Union[str, list], optional): The unit of time you wish your\n                `date_thresholds` to be measured against.\n                Metrics should be one of `day`, `month` or `year`.\n            cast_strings_to_date (bool, optional): Set to True to\n                enable date-casting when input dates are strings. Also adjust\n                date_format if date-strings are not in (yyyy-mm-dd) format.\n                Defaults to False.\n            date_format(str, optional): Format of input dates if date-strings\n                are given. Must be consistent across record pairs. If None\n                (the default), downstream functions for each backend assign\n                date_format to ISO 8601 format (yyyy-mm-dd).\n                Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB and Athena\n                when invalid_dates_as_null=True\n            invalid_dates_as_null (bool, optional): assign any dates that do not adhere\n                to date_format to the null level. Defaults to False.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (_type_, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_dat (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the sizes specified. Defaults to None.\n            m_probability_else (_type_, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Date Difference comparison at thresholds 10 days, 12 months and 15 years\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds = [10, 12, 15],\n                                            date_metrics = ['day', 'month', 'year']\n                                            )\n                ```\n\n                Datediff comparison with date-casting and unspecified date_format\n                (default = %Y-%m-%d)\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True\n                                            )\n                ```\n                Datediff comparison with date-casting and specified (non-default)\n                date_format\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            date_format='%d/%m/%Y'\n                                            )\n                ```\n                Datediff comparison with date-casting and invalid dates set to null\n                ```py\n                import splink.duckdb.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            invalid_dates_as_null=True\n                                            )\n                ```\n            === \":simple-apachespark: Spark\"\n                Date Difference comparison at thresholds 10 days, 12 months and 15 years\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds = [10, 12, 15],\n                                            date_metrics = ['day', 'month', 'year']\n                                            )\n                ```\n\n                Datediff comparison with date-casting and unspecified date_format\n                (default = %Y-%m-%d)\n                ``` python\n                    import splink.spark.comparison_library as cl\n                    cl.datediff_at_thresholds(\"date\",\n                                                date_thresholds=[1,5],\n                                                date_metrics = [\"day\", \"year\"],\n                                                cast_strings_to_date=True\n                                                )\n                ```\n\n                Datediff comparison with date-casting and specified (non-default)\n                date_format\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            date_format='%d/%m/%Y'\n                                            )\n                ```\n                 Datediff comparison with date-casting and invalid dates set to null\n                ```py\n                import splink.spark.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            invalid_dates_as_null=True\n                                            )\n                ```\n            === \"\":simple-amazonaws: Athena\"\n                Date Difference comparison at thresholds 10 days, 12 months and 15 years\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds = [10, 12, 15],\n                                            date_metrics = ['day', 'month', 'year']\n                                            )\n                ```\n\n                Datediff comparison with date-casting and unspecified date_format\n                (default = %Y-%m-%d)\n                ``` python\n                    import splink.athena.comparison_library as cl\n                    cl.datediff_at_thresholds(\"date\",\n                                                date_thresholds=[1,5],\n                                                date_metrics = [\"day\", \"year\"],\n                                                cast_strings_to_date=True\n                                                )\n                ```\n\n                Datediff comparison with date-casting and specified (non-default)\n                date_format\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            date_format='%d/%m/%Y'\n                                            )\n                ```\n                 Datediff comparison with date-casting and invalid dates set to null\n                ```py\n                import splink.athena.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            invalid_dates_as_null=True\n                                            )\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                Date Difference comparison at thresholds 10 days, 12 months and 15 years\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds = [10, 12, 15],\n                                            date_metrics = ['day', 'month', 'year']\n                                            )\n                ```\n\n                Datediff comparison with date-casting and unspecified date_format\n                (default = yyyy-MM-dd)\n                ``` python\n                    import splink.postgres.comparison_library as cl\n                    cl.datediff_at_thresholds(\"date\",\n                                                date_thresholds=[1,5],\n                                                date_metrics = [\"day\", \"year\"],\n                                                cast_strings_to_date=True\n                                                )\n                ```\n\n                Datediff comparison with date-casting and specified (non-default)\n                date_format\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True,\n                                            date_format='dd/MM/yyyy'\n                                            )\n                ```\n\n        Returns:\n            Comparison: A comparison for Datediff that can be included in the Splink\n                settings dictionary.\n        \"\"\"\n\n        thresholds = ensure_is_iterable(date_thresholds)\n        metrics = ensure_is_iterable(date_metrics)\n\n        # Validate user inputs\n        comparison_at_thresholds_error_logger(\"datediff\", date_thresholds)\n        datediff_error_logger(thresholds, metrics)\n\n        if m_probability_or_probabilities_dat is None:\n            m_probability_or_probabilities_dat = [None] * len(thresholds)\n        m_probabilities = ensure_is_iterable(m_probability_or_probabilities_dat)\n\n        comparison_levels = []\n        comparison_levels.append(\n            self._null_level(\n                col_name,\n                invalid_dates_as_null=invalid_dates_as_null,\n                valid_string_pattern=date_format,\n            )\n        )\n\n        if include_exact_match_level:\n            level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match,\n            )\n            comparison_levels.append(level)\n\n        for date_thres, date_metr, m_prob in zip(thresholds, metrics, m_probabilities):\n            level = self._datediff_level(\n                col_name,\n                date_threshold=date_thres,\n                date_metric=date_metr,\n                m_probability=m_prob,\n                cast_strings_to_date=cast_strings_to_date,\n                date_format=date_format,\n            )\n            comparison_levels.append(level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        thres_desc = \", \".join(\n            [f\"{m.title()}(s): {v}\" for m, v in zip(metrics, thresholds)]\n        )\n        plural = \"\" if len(thresholds) == 1 else \"s\"\n        comparison_desc += (\n            f\"Dates within the following threshold{plural} {thres_desc} vs. \"\n        )\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _datediff_level(self):\n        raise NotImplementedError(\"Datediff level not defined on base class\")\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_library.py</code> <pre><code>class DistanceInKMAtThresholdsBase(Comparison):\n    def __init__(\n        self,\n        lat_col: str,\n        long_col: str,\n        km_thresholds: int | list = [0.1, 1],\n        include_exact_match_level=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_km: float | list = None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A comparison of the coordinates defined in 'lat_col' and\n        'long col' giving the haversine distance between them in km.\n\n        An example of the output with default arguments and settings\n        `km_thresholds = [1]` would be\n\n        - The two coordinates within 1 km of one another\n        - Anything else (i.e.  the distance between all coordinate lie outside\n        this range)\n\n        Args:\n            col_name (str): The name of the date column to compare.\n            lat_col (str): The name of the column containing the lattitude of the\n                coordinates.\n            long_col (str): The name of the column containing the longitude of the\n                coordinates.\n            km_thresholds (Union[int, list], optional): The size(s) of given date\n                thresholds, to assess whether two coordinates fall within a given\n                distance.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_km (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the sizes specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ``` python\n                import splink.duckdb.comparison_library as cl\n                cl.distance_in_km_at_thresholds(\"lat_col\",\n                                           \"long_col\",\n                                           km_thresholds = [0.1, 1, 10]\n                                        )\n                ```\n            === \":simple-apachespark: Spark\"\n                ``` python\n                import splink.spark.comparison_library as cl\n                cl.distance_in_km_at_thresholds(\"lat_col\",\n                                           \"long_col\",\n                                           km_thresholds = [0.1, 1, 10]\n                                        )\n                ```\n            === \":simple-amazonaws: Athena\"\n                ``` python\n                import splink.athena.comparison_library as cl\n                cl.distance_in_km_at_thresholds(\"lat_col\",\n                                           \"long_col\",\n                                           km_thresholds = [0.1, 1, 10]\n                                        )\n                ```\n            === \":simple-postgresql: PostgreSql\"\n                ``` python\n                import splink.postgres.comparison_library as cl\n                cl.distance_in_km_at_thresholds(\"lat_col\",\n                                           \"long_col\",\n                                           km_thresholds = [0.1, 1, 10]\n                                        )\n                ```\n\n        Returns:\n            Comparison: A comparison for Distance in KM that can be included in the\n                Splink settings dictionary.\n        \"\"\"\n\n        thresholds = ensure_is_iterable(km_thresholds)\n\n        if m_probability_or_probabilities_km is None:\n            m_probability_or_probabilities_km = [None] * len(thresholds)\n        m_probabilities = ensure_is_iterable(m_probability_or_probabilities_km)\n\n        comparison_levels = []\n\n        null_level = {\n            \"sql_condition\": f\"({lat_col}_l IS NULL OR {lat_col}_r IS NULL) \\n\"\n            f\"OR ({long_col}_l IS NULL OR {long_col}_r IS NULL)\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        }\n        comparison_levels.append(null_level)\n\n        if include_exact_match_level:\n            label_suffix = f\" {lat_col}, {long_col}\"\n            level = {\n                \"sql_condition\": f\"({lat_col}_l = {lat_col}_r) \\n\"\n                f\"AND ({long_col}_l = {long_col}_r)\",\n                \"label_for_charts\": f\"Exact match{label_suffix}\",\n            }\n\n            if m_probability_exact_match:\n                level[\"m_probability\"] = m_probability_exact_match\n\n            comparison_levels.append(level)\n\n        for km_thres, m_prob in zip(km_thresholds, m_probabilities):\n            level = self._distance_in_km_level(\n                lat_col,\n                long_col,\n                km_threshold=km_thres,\n                m_probability=m_prob,\n            )\n            comparison_levels.append(level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        thres_desc = \", \".join([f\"Km threshold(s): {thres}\" for thres in thresholds])\n        plural = \"\" if len(thresholds) == 1 else \"s\"\n        comparison_desc += (\n            f\"Km distance within the following threshold{plural} {thres_desc} vs. \"\n        )\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ExactMatchBase.__init__","title":"<code>__init__(col_name, regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_else=None, include_colname_in_charts_label=False)</code>","text":"<p>A comparison of the data in <code>col_name</code> with two levels:</p> <ul> <li>Exact match</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, term frequency adjustments will be made on the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <code>include_colname_in_charts_label</code> <p>If true, append col name to label for charts.  Defaults to False.</p> <code>False</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Create comparison with exact match level </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Create comparison with exact match level </p><pre><code>import splink.spark.comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Create comparison with exact match level </p><pre><code>import splink.athena.comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression <pre><code>import splink.athena.comparison_library as cl\ncl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Create comparison with exact match level </p><pre><code>import splink.sqlite.comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> <p>Create comparison with exact match level </p><pre><code>import splink.postgres.comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for exact match that can be included in the Splink settings dictionary</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name,\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_else=None,\n    include_colname_in_charts_label=False,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with two levels:\n\n    - Exact match\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        term_frequency_adjustments (bool, optional): If True, term frequency\n            adjustments will be made on the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n        include_colname_in_charts_label: If true, append col name to label for\n            charts.  Defaults to False.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with exact match level\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.exact_match(\"first_name\")\n            ```\n            Create comparison with exact match level based on a\n            substring of first_name as determined by a regular expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with exact match level\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.exact_match(\"first_name\")\n            ```\n            Create comparison with exact match level based on a\n            substring of first_name as determined by a regular expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Create comparison with exact match level\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.exact_match(\"first_name\")\n            ```\n            Create comparison with exact match level based on a\n            substring of first_name as determined by a regular expression\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Create comparison with exact match level\n            ``` python\n            import splink.sqlite.comparison_library as cl\n            cl.exact_match(\"first_name\")\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Create comparison with exact match level\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.exact_match(\"first_name\")\n            ```\n\n    Returns:\n        Comparison: A comparison for exact match that can be included in the Splink\n            settings dictionary\n    \"\"\"\n\n    comparison_dict = {\n        \"comparison_description\": \"Exact match vs. anything else\",\n        \"comparison_levels\": [\n            self._null_level(col_name, valid_string_pattern),\n            self._exact_match_level(\n                col_name,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match,\n                include_colname_in_charts_label=include_colname_in_charts_label,\n            ),\n            self._else_level(m_probability=m_probability_else),\n        ],\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceFunctionAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_function_name, distance_threshold_or_thresholds, regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, higher_is_more_similar=True, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_thres=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with a user-provided distance function used to assess middle similarity levels.</p> <p>The user-provided distance function must exist in the SQL backend.</p> <p>An example of the output with default arguments and setting <code>distance_function_name</code> to <code>jaccard</code> and <code>distance_threshold_or_thresholds = [0.9,0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaccard distance &lt;= 0.9</li> <li>Jaccard distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Note: distance_function_at_thresholds() is primarily used in the backend to create the out-of-the-box cl.XXX_at_thresholds() functions</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_function_name</code> <code>str</code> <p>The name of the distance function.</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2].</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>higher_is_more_similar</code> <code>bool</code> <p>If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein).</p> <code>True</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_thres</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7]\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre> <p>Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 </p><pre><code>import splink.spark.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7]\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre> <p>Apply the <code>levenshtein_distance</code> function in a comparison with levels 1 and 2 </p><pre><code>import splink.athena.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'levenshtein_distance',\n                   distance_threshold_or_thresholds = [1, 2],\n                   higher_is_more_similar = False\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.athena.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre> <p>Apply the <code>levenshtein</code> function in a comparison with levels 1 and 2 </p><pre><code>import splink.sqlite.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'levenshtein',\n                   distance_threshold_or_thresholds = [1, 2],\n                   higher_is_more_similar = False\n                   )\n</code></pre> <p>Apply the <code>levenshtein</code> function in a comparison with levels 1 and 2 </p><pre><code>import splink.postgres.comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'levenshtein',\n                   distance_threshold_or_thresholds = [1, 2],\n                   higher_is_more_similar = False\n                   )\n</code></pre> ```  <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for a chosen distance function similarity that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_function_name: str,\n    distance_threshold_or_thresholds: float | list,\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    higher_is_more_similar: bool = True,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_thres: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with a user-provided distance\n    function used to assess middle similarity levels.\n\n    The user-provided distance function must exist in the SQL backend.\n\n    An example of the output with default arguments and setting\n    `distance_function_name` to `jaccard` and\n    `distance_threshold_or_thresholds = [0.9,0.7]` would be\n\n    - Exact match\n    - Jaccard distance &lt;= 0.9\n    - Jaccard distance &lt;= 0.7\n    - Anything else\n\n    Note: distance_function_at_thresholds() is primarily used in the\n    backend to create the out-of-the-box cl.XXX_at_thresholds() functions\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_function_name (str): The name of the distance function.\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to [1, 2].\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        set_to_lowercase (bool): If True, sets all entries to lowercase.\n        higher_is_more_similar (bool): If True, a higher value of the distance\n            function indicates a higher similarity (e.g. jaro_winkler).\n            If false, a higher value indicates a lower similarity\n            (e.g. levenshtein).\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_thres (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Apply the `jaccard` function in a comparison with levels 0.9 and 0.7\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'jaccard',\n                               distance_threshold_or_thresholds = [0.9, 0.7]\n                               )\n            ```\n            Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n            a substring of name column as determined by a regular expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'jaccard',\n                               distance_threshold_or_thresholds = [0.9, 0.7],\n                               regex_extract=\"^[A-Z]{1,4}\n                               )\n            ```\n        === \":simple-apachespark: Spark\"\n            Apply the `jaccard` function in a comparison with levels 0.9 and 0.7\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'jaccard',\n                               distance_threshold_or_thresholds = [0.9, 0.7]\n                               )\n            ```\n            Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n            a substring of name column as determined by a regular expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'jaccard',\n                               distance_threshold_or_thresholds = [0.9, 0.7],\n                               regex_extract=\"^[A-Z]{1,4}\n                               )\n            ```\n        === \":simple-amazonaws: Athena\"\n            Apply the `levenshtein_distance` function in a comparison with\n            levels 1 and 2\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'levenshtein_distance',\n                               distance_threshold_or_thresholds = [1, 2],\n                               higher_is_more_similar = False\n                               )\n            ```\n            Apply the `jaccard` function in a comparison with levels 0.9 and 0.7 on\n            a substring of name column as determined by a regular expression\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'jaccard',\n                               distance_threshold_or_thresholds = [0.9, 0.7],\n                               regex_extract=\"^[A-Z]{1,4}\n                               )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Apply the `levenshtein` function in a comparison with\n            levels 1 and 2\n            ``` python\n            import splink.sqlite.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'levenshtein',\n                               distance_threshold_or_thresholds = [1, 2],\n                               higher_is_more_similar = False\n                               )\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Apply the `levenshtein` function in a comparison with\n            levels 1 and 2\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.distance_function_at_thresholds(\"name\",\n                               distance_function_name = 'levenshtein',\n                               distance_threshold_or_thresholds = [1, 2],\n                               higher_is_more_similar = False\n                               )\n            ```\n            ```\n\n    Returns:\n        Comparison: A comparison for a chosen distance function similarity that\n            can be included in the Splink settings dictionary.\n    \"\"\"\n    # Validate user inputs\n\n    distance_threshold_or_thresholds = ensure_is_iterable(\n        distance_threshold_or_thresholds\n    )\n\n    comparison_at_thresholds_error_logger(\n        distance_function_name, distance_threshold_or_thresholds\n    )\n\n    comparison_levels = []\n    comparison_levels.append(self._null_level(col_name, valid_string_pattern))\n    if include_exact_match_level:\n        level = self._exact_match_level(\n            col_name,\n            term_frequency_adjustments=term_frequency_adjustments,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability=m_probability_exact_match,\n        )\n        comparison_levels.append(level)\n\n    threshold_comparison_levels = distance_threshold_comparison_levels(\n        self,\n        col_name,\n        distance_function_name,\n        distance_threshold_or_thresholds,\n        regex_extract,\n        set_to_lowercase,\n        m_probability_or_probabilities_thres,\n    )\n    comparison_levels = comparison_levels + threshold_comparison_levels\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct comparison description\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    threshold_desc = distance_threshold_description(\n        col_name, distance_function_name, distance_threshold_or_thresholds\n    )\n    comparison_desc += threshold_desc\n\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.LevenshteinAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[1, 2], regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_lev=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the levenshtein distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [1,2]</code> would be</p> <ul> <li>Exact match</li> <li>Levenshtein distance &lt;= 1</li> <li>Levenshtein distance &lt;= 2</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2].</p> <code>[1, 2]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena SQLite PostgreSql <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 </p><pre><code>import splink.spark.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 </p><pre><code>import splink.athena.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.athena.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 </p><pre><code>import splink.athena.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 </p><pre><code>import splink.postgres.comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> ```  <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Levenshtein similarity that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold_or_thresholds: int | list = [1, 2],\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_lev: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with the levenshtein distance used to\n    assess middle similarity levels.\n\n    An example of the output with default arguments and setting\n    `distance_threshold_or_thresholds = [1,2]` would be\n\n    - Exact match\n    - Levenshtein distance &lt;= 1\n    - Levenshtein distance &lt;= 2\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to [1, 2].\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_lev (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the thresholds specified for given function. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            on a substring of name column as determined by a regular expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            on a substring of name column as determined by a regular expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            on a substring of name column as determined by a regular expression\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Create comparison with levenshtein match levels with distance &lt;=1\n            and &lt;=2\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            ```\n\n    Returns:\n        Comparison: A comparison for Levenshtein similarity that can be included\n            in the Splink settings dictionary.\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        distance_function_name=self._levenshtein_name,\n        distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n        regex_extract=regex_extract,\n        valid_string_pattern=valid_string_pattern,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=False,\n        include_exact_match_level=include_exact_match_level,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability_exact_match=m_probability_exact_match,\n        m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n        m_probability_else=m_probability_else,\n    )\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DamerauLevenshteinAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=1, regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_dl=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the damerau-levenshtein distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [1]</code> would be</p> <ul> <li>Exact match</li> <li>Damerau-Levenshtein distance &lt;= 1</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to 1.</p> <code>1</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Damerau-Levenshtein similarity that can be</p> <code>Comparison</code> <p>included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold_or_thresholds: int | list = 1,\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_dl: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with the damerau-levenshtein distance\n    used to assess middle similarity levels.\n\n    An example of the output with default arguments and setting\n    `distance_threshold_or_thresholds = [1]` would be\n\n    - Exact match\n    - Damerau-Levenshtein distance &lt;= 1\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to 1.\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (_type_, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified for given function. Defaults to None.\n        m_probability_else (_type_, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with damerau-levenshtein match levels with\n            distance &lt;= 1, 2\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with damerau-levenshtein match levels with\n            distance &lt;= 1\n            on a substring of name column as determined by a regular expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                                 [1,2],\n                                                 regex_extract=\"^A|B\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with damerau-levenshtein match levels with\n            distance &lt;= 1, 2\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with damerau-evenshtein match levels with\n            distance &lt;= 1, 2\n            on a substring of name column as determined by a regular expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                                 [1,2],\n                                                 regex_extract=\"^A|B\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Create comparison with damerau-levenshtein match levels with\n            distance &lt;= 1, 2\n            ``` python\n            import splink.sqlite.comparison_library as cl\n            cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n            ```\n\n    Returns:\n        Comparison: A comparison for Damerau-Levenshtein similarity that can be\n        included in the Splink settings dictionary.\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        distance_function_name=self._levenshtein_name,\n        distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n        regex_extract=regex_extract,\n        valid_string_pattern=valid_string_pattern,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=False,\n        include_exact_match_level=include_exact_match_level,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability_exact_match=m_probability_exact_match,\n        m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n        m_probability_else=m_probability_else,\n    )\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaccardAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jac=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaccard distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9,0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaccard distance &lt;= 0.9</li> <li>Jaccard distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jac</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <p>Create comparison with jaccard match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.jaccard_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with jaccard match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre> <p>Create comparison with jaccard match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.spark.comparison_library as cl\ncl.jaccard_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with jaccard match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Jaccard similarity that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_jac: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with the jaccard distance used to\n    assess middle similarity levels.\n\n    An example of the output with default arguments and setting\n    `distance_threshold_or_thresholds = [0.9,0.7]` would be\n\n    - Exact match\n    - Jaccard distance &lt;= 0.9\n    - Jaccard distance &lt;= 0.7\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to [0.9, 0.7].\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_jac (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the thresholds specified for given function. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with jaccard match levels with similarity score &gt;=0.9\n            and &gt;=0.7\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaccard_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with jaccard match levels with similarity score &gt;=0.9\n            and &gt;=0.7 on a substring of name column as determined by a regular\n            expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with jaccard match levels with similarity score &gt;=0.9\n            and &gt;=0.7\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaccard_at_thresholds(\"first_name\", [1,2])\n            ```\n            Create comparison with jaccard match levels with similarity score &gt;=0.9\n            and &gt;=0.7 on a substring of name column as determined by a regular\n            expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaccard_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n            ```\n\n    Returns:\n        Comparison: A comparison for Jaccard similarity that can be included\n            in the Splink settings dictionary.\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        distance_function_name=self._jaccard_name,\n        distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n        regex_extract=regex_extract,\n        valid_string_pattern=valid_string_pattern,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_exact_match_level=include_exact_match_level,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability_exact_match=m_probability_exact_match,\n        m_probability_or_probabilities_thres=m_probability_or_probabilities_jac,\n        m_probability_else=m_probability_else,\n    )\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jar=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaro distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9, 0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaro distance &lt;= 0.9</li> <li>Jaro distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jar</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n</code></pre> <p>Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.spark.comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n</code></pre> <p>Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.sqlite.comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_jar: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with the jaro distance used to\n    assess middle similarity levels.\n\n    An example of the output with default arguments and setting\n    `distance_threshold_or_thresholds = [0.9, 0.7]` would be\n\n    - Exact match\n    - Jaro distance &lt;= 0.9\n    - Jaro distance &lt;= 0.7\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to [0.9, 0.7].\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_jar (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the thresholds specified for given function. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with jaro match levels with similarity score &gt;=0.9\n            and &gt;=0.7\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n            Create comparison with jaro match levels with similarity score &gt;=0.9\n            and &gt;=0.7 on a substring of name column as determined by a regular\n            expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with jaro match levels with similarity score &gt;=0.9\n            and &gt;=0.7\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n            Create comparison with jaro match levels with similarity score &gt;=0.9\n            and &gt;=0.7 on a substring of name column as determined by a regular\n            expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            Create comparison with jaro match levels with similarity score &gt;=0.9\n            and &gt;=0.7\n            ``` python\n            import splink.sqlite.comparison_library as cl\n            cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n\n    Returns:\n        Comparison:\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        distance_function_name=self._jaro_name,\n        distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n        regex_extract=regex_extract,\n        valid_string_pattern=valid_string_pattern,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_exact_match_level=include_exact_match_level,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability_exact_match=m_probability_exact_match,\n        m_probability_or_probabilities_thres=m_probability_or_probabilities_jar,\n        m_probability_else=m_probability_else,\n    )\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroWinklerAtThresholdsBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_pattern=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jw=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaro_winkler distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9, 0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaro-Winkler distance &lt;= 0.9</li> <li>Jaro-Winkler distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_pattern</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jw</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Create comparison with jaro_winkler match levels with similarity score &gt;= 0.9 and &gt;=0.7 </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro_winkler match levels with similarity score =&gt;0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\",\n                              [0.9, 0.7],\n                              regex_extract=\"^[A-Z]\"\n                              )\n</code></pre> <p>Create comparison with jaro_winkler match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.spark.comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro_winkler match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\",\n                              [0.9, 0.7],\n                              regex_extract=\"^[A-Z]\"\n                              )\n</code></pre> <p>Create comparison with jaro_winkler match levels with similarity score &gt;=0.9 and &gt;=0.7 </p><pre><code>import splink.sqlite.comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Jaro Winkler similarity that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    distance_threshold_or_thresholds: int | list = [0.9, 0.7],\n    regex_extract: str = None,\n    valid_string_pattern: str = None,\n    set_to_lowercase: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_jw: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in `col_name` with the jaro_winkler distance used to\n    assess middle similarity levels.\n\n    An example of the output with default arguments and setting\n    `distance_threshold_or_thresholds = [0.9, 0.7]` would be\n\n    - Exact match\n    - Jaro-Winkler distance &lt;= 0.9\n    - Jaro-Winkler distance &lt;= 0.7\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare\n        distance_threshold_or_thresholds (Union[int, list], optional): The\n            threshold(s) to use for the middle similarity level(s).\n            Defaults to [0.9, 0.7].\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        valid_string_pattern (str): regular expression pattern that if not\n            matched will result in column being treated as a null.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_jw (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the thresholds specified for given function. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Create comparison with jaro_winkler match levels with similarity\n            score &gt;= 0.9 and &gt;=0.7\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n            Create comparison with jaro_winkler match levels with similarity\n            score =&gt;0.9 and &gt;=0.7 on a substring of name column as determined by\n            a regular expression\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.jaro_winkler_at_thresholds(\"first_name\",\n                                          [0.9, 0.7],\n                                          regex_extract=\"^[A-Z]\"\n                                          )\n            ```\n        === \":simple-apachespark: Spark\"\n            Create comparison with jaro_winkler match levels with similarity\n            score &gt;=0.9 and &gt;=0.7\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n            Create comparison with jaro_winkler match levels with similarity\n            score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined\n            by a regular expression\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.jaro_winkler_at_thresholds(\"first_name\",\n                                          [0.9, 0.7],\n                                          regex_extract=\"^[A-Z]\"\n                                          )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Create comparison with jaro_winkler match levels with similarity\n            score &gt;=0.9 and &gt;=0.7\n            ``` python\n            import splink.sqlite.comparison_library as cl\n            cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n            ```\n\n    Returns:\n        Comparison: A comparison for Jaro Winkler similarity that can be included\n            in the Splink settings dictionary.\n    \"\"\"\n\n    super().__init__(\n        col_name,\n        distance_function_name=self._jaro_winkler_name,\n        distance_threshold_or_thresholds=distance_threshold_or_thresholds,\n        regex_extract=regex_extract,\n        valid_string_pattern=valid_string_pattern,\n        set_to_lowercase=set_to_lowercase,\n        higher_is_more_similar=True,\n        include_exact_match_level=include_exact_match_level,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability_exact_match=m_probability_exact_match,\n        m_probability_or_probabilities_thres=m_probability_or_probabilities_jw,\n        m_probability_else=m_probability_else,\n    )\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ArrayIntersectAtSizesBase.__init__","title":"<code>__init__(col_name, size_or_sizes=[1], m_probability_or_probabilities_sizes=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in array column <code>col_name</code> with various intersection sizes to assess similarity levels.</p> <p>An example of the output with default arguments and setting <code>size_or_sizes = [3, 1]</code> would be</p> <ul> <li>Intersection has at least 3 elements</li> <li>Intersection has at least 1 element (i.e. 1 or 2)</li> <li>Anything else (i.e. empty intersection)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>size_or_sizes</code> <code>Union[int, list]</code> <p>The size(s) of intersection to use for the non-'else' similarity level(s). Should be in descending order. Defaults to [1].</p> <code>[1]</code> <code>m_probability_or_probabilities_sizes</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena PostgreSql <pre><code>import splink.duckdb.comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <pre><code>import splink.spark.comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <pre><code>import splink.athena.comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <pre><code>import splink.postgres.comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for the intersection of arrays that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    size_or_sizes: int | list = [1],\n    m_probability_or_probabilities_sizes: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in array column `col_name` with various\n    intersection sizes to assess similarity levels.\n\n    An example of the output with default arguments and setting\n    `size_or_sizes = [3, 1]` would be\n\n    - Intersection has at least 3 elements\n    - Intersection has at least 1 element (i.e. 1 or 2)\n    - Anything else (i.e. empty intersection)\n\n    Args:\n        col_name (str): The name of the column to compare\n        size_or_sizes (Union[int, list], optional): The size(s) of intersection\n            to use for the non-'else' similarity level(s). Should be in\n            descending order. Defaults to [1].\n        m_probability_or_probabilities_sizes (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the sizes specified. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.array_intersect_at_sizes(\"first_name\", [3, 1])\n            ```\n\n    Returns:\n        Comparison: A comparison for the intersection of arrays that can be included\n            in the Splink settings dictionary.\n    \"\"\"\n\n    sizes = ensure_is_iterable(size_or_sizes)\n    if len(sizes) == 0:\n        raise ValueError(\n            \"`size_or_sizes` must have at least one element, so that Comparison \"\n            \"has more than just an 'else' level\"\n        )\n    if any(size &lt;= 0 for size in sizes):\n        raise ValueError(\"All entries of `size_or_sizes` must be postive\")\n\n    if m_probability_or_probabilities_sizes is None:\n        m_probability_or_probabilities_sizes = [None] * len(sizes)\n    m_probabilities = ensure_is_iterable(m_probability_or_probabilities_sizes)\n\n    comparison_levels = []\n    comparison_levels.append(self._null_level(col_name))\n\n    for size_intersect, m_prob in zip(sizes, m_probabilities):\n        level = self._array_intersect_level(\n            col_name, m_probability=m_prob, min_intersection=size_intersect\n        )\n        comparison_levels.append(level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    comparison_desc = \"\"\n\n    size_desc = \", \".join([str(s) for s in sizes])\n    plural = \"\" if len(sizes) == 1 else \"s\"\n    comparison_desc += (\n        f\"Array intersection at minimum size{plural} {size_desc} vs. \"\n    )\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DatediffAtThresholdsBase.__init__","title":"<code>__init__(col_name, date_thresholds=[1], date_metrics=['year'], cast_strings_to_date=False, date_format=None, invalid_dates_as_null=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_dat=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in the date column <code>col_name</code> with various date thresholds and metrics to assess similarity levels.</p> <p>An example of the output with default arguments and settings <code>date_thresholds = [1]</code> and <code>date_metrics = ['day']</code> would be - The two input dates are within 1 day of one another - Anything else (i.e. all other dates lie outside this range)</p> <p><code>date_thresholds</code> and <code>date_metrics</code> should be used in conjunction with one another. For example, <code>date_thresholds = [10, 12, 15]</code> with <code>date_metrics = ['day', 'month', 'year']</code> would result in the following checks:</p> <ul> <li>The two dates are within 10 days of one another</li> <li>The two dates are within 12 months of one another</li> <li>And the two dates are within 15 years of one another</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the date column to compare.</p> required <code>date_thresholds</code> <code>Union[int, list]</code> <p>The size(s) of given date thresholds, to assess whether two dates fall within a given time interval. These values can be any integer value and should be used in tandem with <code>date_metrics</code>.</p> <code>[1]</code> <code>date_metrics</code> <code>Union[str, list]</code> <p>The unit of time you wish your <code>date_thresholds</code> to be measured against. Metrics should be one of <code>day</code>, <code>month</code> or <code>year</code>.</p> <code>['year']</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format(str,</code> <code>optional</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd). Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB and Athena when invalid_dates_as_null=True</p> required <code>invalid_dates_as_null</code> <code>bool</code> <p>assign any dates that do not adhere to date_format to the null level. Defaults to False.</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dat</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark\" Athena PostgreSql <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre> <p>Datediff comparison with date-casting and unspecified date_format (default = %Y-%m-%d) </p><pre><code>import splink.duckdb.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True\n                            )\n</code></pre> Datediff comparison with date-casting and specified (non-default) date_format <pre><code>import splink.duckdb.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='%d/%m/%Y'\n                            )\n</code></pre> Datediff comparison with date-casting and invalid dates set to null <pre><code>import splink.duckdb.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True\n                            )\n</code></pre> <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years </p><pre><code>import splink.spark.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre> <p>Datediff comparison with date-casting and unspecified date_format (default = %Y-%m-%d) </p><pre><code>    import splink.spark.comparison_library as cl\n    cl.datediff_at_thresholds(\"date\",\n                                date_thresholds=[1,5],\n                                date_metrics = [\"day\", \"year\"],\n                                cast_strings_to_date=True\n                                )\n</code></pre> <p>Datediff comparison with date-casting and specified (non-default) date_format </p><pre><code>import splink.spark.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='%d/%m/%Y'\n                            )\n</code></pre>  Datediff comparison with date-casting and invalid dates set to null <pre><code>import splink.spark.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True\n                            )\n</code></pre> <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years </p><pre><code>import splink.athena.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre> <p>Datediff comparison with date-casting and unspecified date_format (default = %Y-%m-%d) </p><pre><code>    import splink.athena.comparison_library as cl\n    cl.datediff_at_thresholds(\"date\",\n                                date_thresholds=[1,5],\n                                date_metrics = [\"day\", \"year\"],\n                                cast_strings_to_date=True\n                                )\n</code></pre> <p>Datediff comparison with date-casting and specified (non-default) date_format </p><pre><code>import splink.athena.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='%d/%m/%Y'\n                            )\n</code></pre>  Datediff comparison with date-casting and invalid dates set to null <pre><code>import splink.athena.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True\n                            )\n</code></pre> <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years </p><pre><code>import splink.postgres.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre> <p>Datediff comparison with date-casting and unspecified date_format (default = yyyy-MM-dd) </p><pre><code>    import splink.postgres.comparison_library as cl\n    cl.datediff_at_thresholds(\"date\",\n                                date_thresholds=[1,5],\n                                date_metrics = [\"day\", \"year\"],\n                                cast_strings_to_date=True\n                                )\n</code></pre> <p>Datediff comparison with date-casting and specified (non-default) date_format </p><pre><code>import splink.postgres.comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='dd/MM/yyyy'\n                            )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Datediff that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    date_thresholds: int | list = [1],\n    date_metrics: str | list = [\"year\"],\n    cast_strings_to_date=False,\n    date_format: str = None,\n    invalid_dates_as_null: bool = False,\n    include_exact_match_level=True,\n    term_frequency_adjustments=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_dat: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the data in the date column `col_name` with various\n    date thresholds and metrics to assess similarity levels.\n\n    An example of the output with default arguments and settings\n    `date_thresholds = [1]` and `date_metrics = ['day']` would be\n    - The two input dates are within 1 day of one another\n    - Anything else (i.e. all other dates lie outside this range)\n\n    `date_thresholds` and `date_metrics` should be used in conjunction\n    with one another.\n    For example, `date_thresholds = [10, 12, 15]` with\n    `date_metrics = ['day', 'month', 'year']` would result in the following checks:\n\n    - The two dates are within 10 days of one another\n    - The two dates are within 12 months of one another\n    - And the two dates are within 15 years of one another\n\n    Args:\n        col_name (str): The name of the date column to compare.\n        date_thresholds (Union[int, list], optional): The size(s) of given date\n            thresholds, to assess whether two dates fall within a given time\n            interval.\n            These values can be any integer value and should be used in tandem with\n            `date_metrics`.\n        date_metrics (Union[str, list], optional): The unit of time you wish your\n            `date_thresholds` to be measured against.\n            Metrics should be one of `day`, `month` or `year`.\n        cast_strings_to_date (bool, optional): Set to True to\n            enable date-casting when input dates are strings. Also adjust\n            date_format if date-strings are not in (yyyy-mm-dd) format.\n            Defaults to False.\n        date_format(str, optional): Format of input dates if date-strings\n            are given. Must be consistent across record pairs. If None\n            (the default), downstream functions for each backend assign\n            date_format to ISO 8601 format (yyyy-mm-dd).\n            Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB and Athena\n            when invalid_dates_as_null=True\n        invalid_dates_as_null (bool, optional): assign any dates that do not adhere\n            to date_format to the null level. Defaults to False.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        m_probability_exact_match (_type_, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_dat (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the sizes specified. Defaults to None.\n        m_probability_else (_type_, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Date Difference comparison at thresholds 10 days, 12 months and 15 years\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds = [10, 12, 15],\n                                        date_metrics = ['day', 'month', 'year']\n                                        )\n            ```\n\n            Datediff comparison with date-casting and unspecified date_format\n            (default = %Y-%m-%d)\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True\n                                        )\n            ```\n            Datediff comparison with date-casting and specified (non-default)\n            date_format\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        date_format='%d/%m/%Y'\n                                        )\n            ```\n            Datediff comparison with date-casting and invalid dates set to null\n            ```py\n            import splink.duckdb.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        invalid_dates_as_null=True\n                                        )\n            ```\n        === \":simple-apachespark: Spark\"\n            Date Difference comparison at thresholds 10 days, 12 months and 15 years\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds = [10, 12, 15],\n                                        date_metrics = ['day', 'month', 'year']\n                                        )\n            ```\n\n            Datediff comparison with date-casting and unspecified date_format\n            (default = %Y-%m-%d)\n            ``` python\n                import splink.spark.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True\n                                            )\n            ```\n\n            Datediff comparison with date-casting and specified (non-default)\n            date_format\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        date_format='%d/%m/%Y'\n                                        )\n            ```\n             Datediff comparison with date-casting and invalid dates set to null\n            ```py\n            import splink.spark.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        invalid_dates_as_null=True\n                                        )\n            ```\n        === \"\":simple-amazonaws: Athena\"\n            Date Difference comparison at thresholds 10 days, 12 months and 15 years\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds = [10, 12, 15],\n                                        date_metrics = ['day', 'month', 'year']\n                                        )\n            ```\n\n            Datediff comparison with date-casting and unspecified date_format\n            (default = %Y-%m-%d)\n            ``` python\n                import splink.athena.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True\n                                            )\n            ```\n\n            Datediff comparison with date-casting and specified (non-default)\n            date_format\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        date_format='%d/%m/%Y'\n                                        )\n            ```\n             Datediff comparison with date-casting and invalid dates set to null\n            ```py\n            import splink.athena.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        invalid_dates_as_null=True\n                                        )\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            Date Difference comparison at thresholds 10 days, 12 months and 15 years\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds = [10, 12, 15],\n                                        date_metrics = ['day', 'month', 'year']\n                                        )\n            ```\n\n            Datediff comparison with date-casting and unspecified date_format\n            (default = yyyy-MM-dd)\n            ``` python\n                import splink.postgres.comparison_library as cl\n                cl.datediff_at_thresholds(\"date\",\n                                            date_thresholds=[1,5],\n                                            date_metrics = [\"day\", \"year\"],\n                                            cast_strings_to_date=True\n                                            )\n            ```\n\n            Datediff comparison with date-casting and specified (non-default)\n            date_format\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.datediff_at_thresholds(\"date\",\n                                        date_thresholds=[1,5],\n                                        date_metrics = [\"day\", \"year\"],\n                                        cast_strings_to_date=True,\n                                        date_format='dd/MM/yyyy'\n                                        )\n            ```\n\n    Returns:\n        Comparison: A comparison for Datediff that can be included in the Splink\n            settings dictionary.\n    \"\"\"\n\n    thresholds = ensure_is_iterable(date_thresholds)\n    metrics = ensure_is_iterable(date_metrics)\n\n    # Validate user inputs\n    comparison_at_thresholds_error_logger(\"datediff\", date_thresholds)\n    datediff_error_logger(thresholds, metrics)\n\n    if m_probability_or_probabilities_dat is None:\n        m_probability_or_probabilities_dat = [None] * len(thresholds)\n    m_probabilities = ensure_is_iterable(m_probability_or_probabilities_dat)\n\n    comparison_levels = []\n    comparison_levels.append(\n        self._null_level(\n            col_name,\n            invalid_dates_as_null=invalid_dates_as_null,\n            valid_string_pattern=date_format,\n        )\n    )\n\n    if include_exact_match_level:\n        level = self._exact_match_level(\n            col_name,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability=m_probability_exact_match,\n        )\n        comparison_levels.append(level)\n\n    for date_thres, date_metr, m_prob in zip(thresholds, metrics, m_probabilities):\n        level = self._datediff_level(\n            col_name,\n            date_threshold=date_thres,\n            date_metric=date_metr,\n            m_probability=m_prob,\n            cast_strings_to_date=cast_strings_to_date,\n            date_format=date_format,\n        )\n        comparison_levels.append(level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    thres_desc = \", \".join(\n        [f\"{m.title()}(s): {v}\" for m, v in zip(metrics, thresholds)]\n    )\n    plural = \"\" if len(thresholds) == 1 else \"s\"\n    comparison_desc += (\n        f\"Dates within the following threshold{plural} {thres_desc} vs. \"\n    )\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceInKMAtThresholdsBase.__init__","title":"<code>__init__(lat_col, long_col, km_thresholds=[0.1, 1], include_exact_match_level=False, m_probability_exact_match=None, m_probability_or_probabilities_km=None, m_probability_else=None)</code>","text":"<p>A comparison of the coordinates defined in 'lat_col' and 'long col' giving the haversine distance between them in km.</p> <p>An example of the output with default arguments and settings <code>km_thresholds = [1]</code> would be</p> <ul> <li>The two coordinates within 1 km of one another</li> <li>Anything else (i.e.  the distance between all coordinate lie outside this range)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the date column to compare.</p> required <code>lat_col</code> <code>str</code> <p>The name of the column containing the lattitude of the coordinates.</p> required <code>long_col</code> <code>str</code> <p>The name of the column containing the longitude of the coordinates.</p> required <code>km_thresholds</code> <code>Union[int, list]</code> <p>The size(s) of given date thresholds, to assess whether two coordinates fall within a given distance.</p> <code>[0.1, 1]</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_km</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena PostgreSql <pre><code>import splink.duckdb.comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>import splink.spark.comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>import splink.athena.comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>import splink.postgres.comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Distance in KM that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_library.py</code> <pre><code>def __init__(\n    self,\n    lat_col: str,\n    long_col: str,\n    km_thresholds: int | list = [0.1, 1],\n    include_exact_match_level=False,\n    m_probability_exact_match=None,\n    m_probability_or_probabilities_km: float | list = None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A comparison of the coordinates defined in 'lat_col' and\n    'long col' giving the haversine distance between them in km.\n\n    An example of the output with default arguments and settings\n    `km_thresholds = [1]` would be\n\n    - The two coordinates within 1 km of one another\n    - Anything else (i.e.  the distance between all coordinate lie outside\n    this range)\n\n    Args:\n        col_name (str): The name of the date column to compare.\n        lat_col (str): The name of the column containing the lattitude of the\n            coordinates.\n        long_col (str): The name of the column containing the longitude of the\n            coordinates.\n        km_thresholds (Union[int, list], optional): The size(s) of given date\n            thresholds, to assess whether two coordinates fall within a given\n            distance.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_km (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the sizes specified. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ``` python\n            import splink.duckdb.comparison_library as cl\n            cl.distance_in_km_at_thresholds(\"lat_col\",\n                                       \"long_col\",\n                                       km_thresholds = [0.1, 1, 10]\n                                    )\n            ```\n        === \":simple-apachespark: Spark\"\n            ``` python\n            import splink.spark.comparison_library as cl\n            cl.distance_in_km_at_thresholds(\"lat_col\",\n                                       \"long_col\",\n                                       km_thresholds = [0.1, 1, 10]\n                                    )\n            ```\n        === \":simple-amazonaws: Athena\"\n            ``` python\n            import splink.athena.comparison_library as cl\n            cl.distance_in_km_at_thresholds(\"lat_col\",\n                                       \"long_col\",\n                                       km_thresholds = [0.1, 1, 10]\n                                    )\n            ```\n        === \":simple-postgresql: PostgreSql\"\n            ``` python\n            import splink.postgres.comparison_library as cl\n            cl.distance_in_km_at_thresholds(\"lat_col\",\n                                       \"long_col\",\n                                       km_thresholds = [0.1, 1, 10]\n                                    )\n            ```\n\n    Returns:\n        Comparison: A comparison for Distance in KM that can be included in the\n            Splink settings dictionary.\n    \"\"\"\n\n    thresholds = ensure_is_iterable(km_thresholds)\n\n    if m_probability_or_probabilities_km is None:\n        m_probability_or_probabilities_km = [None] * len(thresholds)\n    m_probabilities = ensure_is_iterable(m_probability_or_probabilities_km)\n\n    comparison_levels = []\n\n    null_level = {\n        \"sql_condition\": f\"({lat_col}_l IS NULL OR {lat_col}_r IS NULL) \\n\"\n        f\"OR ({long_col}_l IS NULL OR {long_col}_r IS NULL)\",\n        \"label_for_charts\": \"Null\",\n        \"is_null_level\": True,\n    }\n    comparison_levels.append(null_level)\n\n    if include_exact_match_level:\n        label_suffix = f\" {lat_col}, {long_col}\"\n        level = {\n            \"sql_condition\": f\"({lat_col}_l = {lat_col}_r) \\n\"\n            f\"AND ({long_col}_l = {long_col}_r)\",\n            \"label_for_charts\": f\"Exact match{label_suffix}\",\n        }\n\n        if m_probability_exact_match:\n            level[\"m_probability\"] = m_probability_exact_match\n\n        comparison_levels.append(level)\n\n    for km_thres, m_prob in zip(km_thresholds, m_probabilities):\n        level = self._distance_in_km_level(\n            lat_col,\n            long_col,\n            km_threshold=km_thres,\n            m_probability=m_prob,\n        )\n        comparison_levels.append(level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    thres_desc = \", \".join([f\"Km threshold(s): {thres}\" for thres in thresholds])\n    plural = \"\" if len(thresholds) == 1 else \"s\"\n    comparison_desc += (\n        f\"Km distance within the following threshold{plural} {thres_desc} vs. \"\n    )\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_template_library.html","title":"Comparison Template Library","text":"","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#documentation-for-comparison_template_library","title":"Documentation for <code>comparison_template_library</code>","text":"<p>The <code>comparison_template_library</code> contains pre-made comparisons with pre-defined parameters available for use directly as described in this topic guide. However, not every comparison is available for every Splink-compatible SQL backend. More detail on creating comparisons for specific data types is also included in the topic guide.</p> <p>The pre-made Splink comparison templates available for each SQL dialect are as given in this table:</p>  DuckDB  Spark  Athena  SQLite  PostgreSql date_comparison \u2713 \u2713 email_comparison \u2713 \u2713 forename_surname_comparison \u2713 \u2713 \u2713 name_comparison \u2713 \u2713 \u2713 postcode_comparison \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#library-comparison-apis","title":"Library comparison APIs","text":"<p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>class DateComparisonBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        cast_strings_to_date: bool = False,\n        date_format: str = None,\n        invalid_dates_as_null: bool = False,\n        include_exact_match_level: bool = True,\n        term_frequency_adjustments: bool = False,\n        separate_1st_january: bool = False,\n        levenshtein_thresholds: int | list = [],\n        damerau_levenshtein_thresholds: int | list = [1],\n        datediff_thresholds: int | list = [1, 1, 10],\n        datediff_metrics: str | list = [\"month\", \"year\", \"year\"],\n        m_probability_exact_match: float = None,\n        m_probability_1st_january: float = None,\n        m_probability_or_probabilities_lev: float | list = None,\n        m_probability_or_probabilities_dl: float | list = None,\n        m_probability_or_probabilities_datediff: float | list = None,\n        m_probability_else: float = None,\n    ) -&gt; Comparison:\n        \"\"\"A wrapper to generate a comparison for a date column the data in\n        `col_name` with preselected defaults.\n\n        The default arguments will give a comparison with comparison levels:\\n\n        - Exact match (on 1st of January only)\\n\n        - Exact match (all other dates)\\n\n        - Damerau-Levenshtein distance &lt;= 1\\n\n        - Date difference &lt;= 1 year\\n\n        - Date difference &lt;= 10 years \\n\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare.\n            cast_strings_to_date (bool, optional): Set to True to\n                enable date-casting when input dates are strings. Also adjust\n                date_format if date-strings are not in (yyyy-mm-dd) format.\n                Defaults to False.\n            date_format (str, optional): Format of input dates if date-strings\n                are given. Must be consistent across record pairs. If None\n                (the default), downstream functions for each backend assign\n                date_format to ISO 8601 format (yyyy-mm-dd).\n                Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB\n                when invalid_dates_as_null=True\n            invalid_dates_as_null (bool, optional): assign any dates that do not adhere\n                to date_format to the null level. Defaults to False.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            separate_1st_january (bool, optional): If True, include a separate\n                exact match comparison level when date is 1st January.\n            levenshtein_thresholds (Union[int, list], optional): The thresholds to use\n                for levenshtein similarity level(s).\n                Defaults to []\n            damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for damerau-levenshtein similarity level(s).\n                Defaults to [1]\n            datediff_thresholds (Union[int, list], optional): The thresholds to use\n                for datediff similarity level(s).\n                Defaults to [1, 1].\n            datediff_metrics (Union[str, list], optional): The metrics to apply\n                thresholds to for datediff similarity level(s).\n                Defaults to [\"month\", \"year\"].\n            cast_strings_to_date (bool, optional): Set to True to\n                enable date-casting when input dates are strings. Also adjust\n                date_format if date-strings are not in (yyyy-mm-dd) format.\n                Defaults to False.\n            date_format (str, optional): Format of input dates if date-strings\n                are given. Must be consistent across record pairs. If None\n                (the default), downstream functions for each backend assign\n                date_format to ISO 8601 format (yyyy-mm-dd).\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_lev (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the levenshtein thresholds specified. Defaults to None.\n            m_probability_or_probabilities_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the damerau-levenshtein thresholds specified. Defaults to None.\n            m_probability_or_probabilities_datediff (Union[float, list], optional):\n                If provided, overrides the default m probabilities\n                for the datediff thresholds specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Basic Date Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\")\n                ```\n                Bespoke Date Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\",\n                                    damerau_levenshtein_thresholds=[],\n                                    levenshtein_thresholds=[2],\n                                    datediff_thresholds=[1, 1],\n                                    datediff_metrics=[\"month\", \"year\"])\n                ```\n                Date Comparison casting columns date and assigning values that do not\n                match the date_format to the null level\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\",\n                                    cast_strings_to_date=True,\n                                    date_format='%d/%m/%Y',\n                                    invalid_dates_as_null=True)\n                ```\n            === \":simple-apachespark: Spark\"\n                Basic Date Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\")\n                ```\n                Bespoke Date Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\",\n                                    damerau_levenshtein_thresholds=[],\n                                    levenshtein_thresholds=[2],\n                                    datediff_thresholds=[1, 1],\n                                    datediff_metrics=[\"month\", \"year\"])\n                ```\n                Date Comparison casting columns date and assigning values that do not\n                match the date_format to the null level\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.date_comparison(\"date_of_birth\",\n                                    cast_strings_to_date=True,\n                                    date_format='dd/mm/yyyy',\n                                    invalid_dates_as_null=True)\n                ```\n        Returns:\n            Comparison: A comparison that can be inclued in the Splink settings\n                dictionary.\n        \"\"\"\n        # Construct Comparison\n        comparison_levels = []\n        comparison_levels.append(\n            self._null_level(\n                col_name,\n                invalid_dates_as_null=invalid_dates_as_null,\n                valid_string_pattern=date_format,\n            )\n        )\n\n        # Validate user inputs\n        datediff_error_logger(thresholds=datediff_thresholds, metrics=datediff_metrics)\n\n        if separate_1st_january:\n            dob_first_jan = {\n                \"sql_condition\": f\"SUBSTR({col_name}_l, 6, 5) = '01-01'\",\n                \"label_for_charts\": \"Date is 1st Jan\",\n            }\n            comparison_level = and_(\n                self._exact_match_level(col_name),\n                dob_first_jan,\n                label_for_charts=\"Exact match and 1st Jan\",\n            )\n\n            if m_probability_1st_january:\n                comparison_level[\"m_probability\"] = m_probability_1st_january\n            if term_frequency_adjustments:\n                comparison_level[\"tf_adjustment_column\"] = col_name\n            comparison_levels.append(comparison_level)\n\n        if include_exact_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match,\n            )\n            comparison_levels.append(comparison_level)\n\n        levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        damerau_levenshtein_thresholds = ensure_is_iterable(\n            damerau_levenshtein_thresholds\n        )\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            damerau_levenshtein_thresholds = ensure_is_iterable(\n                damerau_levenshtein_thresholds\n            )\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        datediff_thresholds = ensure_is_iterable(datediff_thresholds)\n        datediff_metrics = ensure_is_iterable(datediff_metrics)\n        if len(datediff_thresholds) &gt; 0:\n            if m_probability_or_probabilities_datediff is None:\n                m_probability_or_probabilities_datediff = [None] * len(\n                    datediff_thresholds\n                )\n            m_probability_or_probabilities_datediff = ensure_is_iterable(\n                m_probability_or_probabilities_datediff\n            )\n\n            for thres, metric, m_prob in zip(\n                datediff_thresholds,\n                datediff_metrics,\n                m_probability_or_probabilities_datediff,\n            ):\n                comparison_level = self._datediff_level(\n                    col_name,\n                    date_threshold=thres,\n                    date_metric=metric,\n                    m_probability=m_prob,\n                    cast_strings_to_date=cast_strings_to_date,\n                    date_format=date_format,\n                )\n                comparison_levels.append(comparison_level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct Description\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        if len(levenshtein_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"levenshtein\", levenshtein_thresholds\n            )\n            comparison_desc += desc\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n            )\n            comparison_desc += desc\n\n        if len(datediff_thresholds) &gt; 0:\n            datediff_desc = \", \".join(\n                [\n                    f\"{m.title()}(s): {v}\"\n                    for v, m in zip(datediff_thresholds, datediff_metrics)\n                ]\n            )\n            plural = \"\" if len(datediff_thresholds) == 1 else \"s\"\n            comparison_desc += (\n                f\"Dates within the following threshold{plural} {datediff_desc} vs. \"\n            )\n\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _is_distance_subclass(self):\n        return False\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>class NameComparisonBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        regex_extract: str = None,\n        set_to_lowercase: str = False,\n        include_exact_match_level: bool = True,\n        phonetic_col_name: str = None,\n        term_frequency_adjustments: bool = False,\n        levenshtein_thresholds: int | list = [],\n        damerau_levenshtein_thresholds: int | list = [1],\n        jaro_thresholds: float | list = [],\n        jaro_winkler_thresholds: float | list = [0.9, 0.8],\n        jaccard_thresholds: float | list = [],\n        m_probability_exact_match_name: float = None,\n        m_probability_exact_match_phonetic_name: float = None,\n        m_probability_or_probabilities_lev: float | list = None,\n        m_probability_or_probabilities_dl: float | list = None,\n        m_probability_or_probabilities_jar: float | list = None,\n        m_probability_or_probabilities_jw: float | list = None,\n        m_probability_or_probabilities_jac: float | list = None,\n        m_probability_else: float = None,\n    ) -&gt; Comparison:\n        \"\"\"A wrapper to generate a comparison for a name column the data in\n        `col_name` with preselected defaults.\n\n        The default arguments will give a comparison with comparison levels:\\n\n        - Exact match \\n\n        - Damerau-Levenshtein Distance &lt;= 1\n        - Jaro Winkler similarity &gt;= 0.9\\n\n        - Jaro Winkler similarity &gt;= 0.8\\n\n        - Anything else\n\n        Args:\n            col_name (str): The name of the column to compare.\n            regex_extract (str): Regular expression pattern to evaluate a match on.\n            set_to_lowercase (bool): If True, all names are set to lowercase\n                during the pairwise comparisons.\n                Defaults to False\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level for col_name. Defaults to True.\n            phonetic_col_name (str): The name of the column with phonetic reduction\n                (such as dmetaphone) of col_name. Including parameter will create\n                an exact match level for  phonetic_col_name. The phonetic column must\n                be present in the dataset to use this parameter.\n                Defaults to None\n            term_frequency_adjustments (bool, optional): If True, apply term\n                frequency adjustments to the exact match level for \"col_name\".\n                Defaults to False.\n            term_frequency_adjustments_phonetic_name (bool, optional): If True, apply\n                term frequency adjustments to the exact match level for\n                \"phonetic_col_name\".\n                Defaults to False.\n            levenshtein_thresholds (Union[int, list], optional): The thresholds to use\n                for levenshtein similarity level(s).\n                Defaults to []\n            damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for damerau-levenshtein similarity level(s).\n                Defaults to [1]\n            jaro_thresholds (Union[int, list], optional): The thresholds to use\n                for jaro similarity level(s).\n                Defaults to []\n            jaro_winkler_thresholds (Union[int, list], optional): The thresholds to use\n                for jaro_winkler similarity level(s).\n                Defaults to [0.9, 0.8]\n            jaccard_thresholds (Union[int, list], optional): The thresholds to use\n                for jaccard similarity level(s).\n                Defaults to []\n            m_probability_exact_match_name (_type_, optional): Starting m probability\n                for exact match level. Defaults to None.\n            m_probability_exact_match_phonetic_name (_type_, optional): Starting m\n                probability for exact match level for phonetic_col_name.\n                Defaults to None.\n            m_probability_or_probabilities_lev (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_datediff (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_jar (Union[float, list], optional):\n                Starting m probabilities for the jaro thresholds specified.\n                Defaults to None.\n            m_probability_or_probabilities_jw (Union[float, list], optional):\n                Starting m probabilities for the jaro winkler thresholds specified.\n                Defaults to None.\n            m_probability_or_probabilities_jac (Union[float, list], optional):\n                Starting m probabilities for the jaccard thresholds specified.\n                Defaults to None.\n            m_probability_else (_type_, optional): Starting m probability for\n                the 'everything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Basic Name Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.name_comparison(\"name\")\n                ```\n                Bespoke Name Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.name_comparison(\"name\",\n                                    phonetic_col_name = \"name_dm\",\n                                    term_frequency_adjustments = True,\n                                    levenshtein_thresholds=[2],\n                                    damerau_levenshtein_thresholds=[],\n                                    jaro_winkler_thresholds=[],\n                                    jaccard_thresholds=[1]\n                                    )\n                ```\n            === \":simple-apachespark: Spark\"\n                Basic Name Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.name_comparison(\"name\")\n                ```\n                Bespoke Name Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.name_comparison(\"name\",\n                                    phonetic_col_name = \"name_dm\",\n                                    term_frequency_adjustments = True,\n                                    levenshtein_thresholds=[2],\n                                    damerau_levenshtein_thresholds=[],\n                                    jaro_winkler_thresholds=[],\n                                    jaccard_thresholds=[1]\n                                    )\n                ```\n            === \":simple-sqlite: SQLite\"\n                Basic Name Comparison\n                ``` python\n                import splink.sqlite.comparison_template_library as ctl\n                ctl.name_comparison(\"name\")\n                ```\n                Bespoke Name Comparison\n                ``` python\n                import splink.sqlite.comparison_template_library as ctl\n                ctl.name_comparison(\"name\",\n                                    phonetic_col_name = \"name_dm\",\n                                    term_frequency_adjustments = True,\n                                    levenshtein_thresholds=[2],\n                                    damerau_levenshtein_thresholds=[],\n                                    jaro_winkler_thresholds=[0.8],\n                                    )\n                ```\n\n        Returns:\n            Comparison: A comparison that can be included in the Splink settings\n                dictionary.\n        \"\"\"\n\n        # Construct Comparison\n        comparison_levels = []\n        comparison_levels.append(self._null_level(col_name))\n\n        if include_exact_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match_name,\n                include_colname_in_charts_label=True,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n            )\n            comparison_levels.append(comparison_level)\n\n            if phonetic_col_name is not None:\n                comparison_level = self._exact_match_level(\n                    phonetic_col_name,\n                    term_frequency_adjustments=term_frequency_adjustments,\n                    m_probability=m_probability_exact_match_phonetic_name,\n                    include_colname_in_charts_label=True,\n                    regex_extract=regex_extract,\n                    set_to_lowercase=set_to_lowercase,\n                )\n                comparison_levels.append(comparison_level)\n\n        levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        damerau_levenshtein_thresholds = ensure_is_iterable(\n            damerau_levenshtein_thresholds\n        )\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            levenshtein_thresholds = ensure_is_iterable(damerau_levenshtein_thresholds)\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n        if len(jaro_thresholds) &gt; 0:\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaro\",\n                distance_threshold_or_thresholds=jaro_thresholds,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_jar,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n        if len(jaro_winkler_thresholds) &gt; 0:\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaro-winkler\",\n                distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_jw,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        jaccard_thresholds = ensure_is_iterable(jaccard_thresholds)\n        if len(jaccard_thresholds) &gt; 0:\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaccard\",\n                distance_threshold_or_thresholds=jaccard_thresholds,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_jac,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct Description\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        if phonetic_col_name is not None:\n            comparison_desc += \"Names with phonetic exact match vs. \"\n\n        if len(levenshtein_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"levenshtein\", levenshtein_thresholds\n            )\n            comparison_desc += desc\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n            )\n            comparison_desc += desc\n\n        if len(jaro_thresholds) &gt; 0:\n            desc = distance_threshold_description(col_name, \"jaro\", jaro_thresholds)\n            comparison_desc += desc\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"jaro_winkler\", jaro_winkler_thresholds\n            )\n            comparison_desc += desc\n\n        if len(jaccard_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"jaccard\", jaccard_thresholds\n            )\n            comparison_desc += desc\n\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _is_distance_subclass(self):\n        return False\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>class ForenameSurnameComparisonBase(Comparison):\n    def __init__(\n        self,\n        forename_col_name,\n        surname_col_name,\n        set_to_lowercase=False,\n        include_exact_match_level: bool = True,\n        include_columns_reversed: bool = True,\n        term_frequency_adjustments: bool = False,\n        tf_adjustment_col_forename_and_surname: str = None,\n        phonetic_forename_col_name: str = None,\n        phonetic_surname_col_name: str = None,\n        levenshtein_thresholds: int | list = [],\n        damerau_levenshtein_thresholds: int | list = [],\n        jaro_winkler_thresholds: float | list = [0.88],\n        jaro_thresholds: float | list = [],\n        jaccard_thresholds: float | list = [],\n        m_probability_exact_match_forename_surname: float = None,\n        m_probability_exact_match_phonetic_forename_surname: float = None,\n        m_probability_columns_reversed_forename_surname: float = None,\n        m_probability_exact_match_surname: float = None,\n        m_probability_exact_match_forename: float = None,\n        m_probability_or_probabilities_surname_lev: float | list = None,\n        m_probability_or_probabilities_surname_dl: float | list = None,\n        m_probability_or_probabilities_surname_jw: float | list = None,\n        m_probability_or_probabilities_surname_jar: float | list = None,\n        m_probability_or_probabilities_surname_jac: float | list = None,\n        m_probability_or_probabilities_forename_lev: float | list = None,\n        m_probability_or_probabilities_forename_dl: float | list = None,\n        m_probability_or_probabilities_forename_jw: float | list = None,\n        m_probability_or_probabilities_forename_jar: float | list = None,\n        m_probability_or_probabilities_forename_jac: float | list = None,\n        m_probability_else: float = None,\n    ) -&gt; Comparison:\n        \"\"\"A wrapper to generate a comparison for a name column the data in\n        `col_name` with preselected defaults.\n\n        The default arguments will give a comparison with comparison levels:\\n\n        - Exact match forename and surname\\n\n        - Macth of forename and surname reversed\\n\n        - Exact match surname\\n\n        - Exact match forename\\n\n        - Fuzzy match surname jaro-winkler &gt;= 0.88\\n\n        - Fuzzy match forename jaro-winkler&gt;=  0.88\\n\n        - Anything else\n\n        Args:\n            forename_col_name (str): The name of the forename column to compare\n            surname_col_name (str): The name of the surname column to compare\n            set_to_lowercase (bool): If True, all names are set to lowercase\n                during the pairwise comparisons.\n                Defaults to False\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level for col_name. Defaults to True.\n            include_columns_reversed (bool, optional): If True, include a comparison\n                level for forename and surname being swapped. Defaults to True\n            term_frequency_adjustments (bool, optional): If True, apply term\n                frequency adjustments to the exact match level for forename_col_name\n                and surname_col_name.\n                Applies term frequency adjustments to full name exact match level\n                and columns reversed exact match level if\n                tf_adjustment_col_forename_and_surname is provided.\n                Applies term frequency adjustments to phonetic_forename_col_name\n                and phonetic_surname_col_name exact match levels, if they are provided.\n                Defaults to False.\n            tf_adjustment_col_forename_and_surname (str, optional): The name\n                of a combined forename surname column. This column is used to provide\n                term frequency adjustments for forename surname exact match and columns\n                reversed levels.\n                Defaults to None\n            set_to_lowercase (bool): If True, all postcodes are set to lowercase\n                during the pairwise comparisons.\n                Defaults to True\n            phonetic_forename_col_name (str, optional): The name of the column with\n                phonetic reduction (such as dmetaphone) of forename_col_name. Including\n                parameter along with phonetic_surname_col_name will create an exact\n                match level for \"Full name phonetic match\".\n                The phonetic column must be present in the dataset to use this\n                parameter.\n                Defaults to None\n            phonetic_surname_col_name (str, optional): The name of the column with\n                phonetic reduction (such as dmetaphone) of surname_col_name. Including\n                this parameter along with phonetic_forename_col_name will create an\n                exact match level for \"Full name phonetic match\".\n                The phonetic column must be present in\n                the dataset to use this parameter.\n                Defaults to None\n            levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for levenshtein similarity level(s) for surname_col_name\n                and forename_col_name.\n                Defaults to []\n            damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for damerau-levenshtein similarity level(s).\n                Defaults to []\n            jaro_winkler_thresholds (Union[int, list], optional): The thresholds\n                to use for jaro_winkler similarity level(s) for surname_col_name\n                and forename_col_name.\n                Defaults to [0.88]\n            jaro_thresholds (Union[int, list], optional): The thresholds\n                to use for jaro similarity level(s) for surname_col_name\n                and forename_col_name.\n                Defaults to []\n            jaccard_thresholds (Union[int, list], optional): The thresholds to\n                use for jaccard similarity level(s) for surname_col_name and\n                forename_col_name.\n                Defaults to []\n            m_probability_exact_match_forename_surname (_type_, optional): If provided,\n                overrides the default m probability for the exact match level for\n                forename and surname.\n                Defaults to None.\n            m_probability_exact_match_phonetic_forename_surname (_type_, optional): If\n                provided, overrides the default m probability for the phonetic match\n                level for forename and surname.\n                Defaults to None.\n            m_probability_columns_reversed_forename_surname (_type_, optional): If\n                provided, overrides the default m probability for the columns reversed\n                level for forename and surname.\n                Defaults to None.\n            m_probability_columns_reversed_forename_surname (_type_, optional): If\n                provided, overrides the default m probability for the columns reversed\n                level for forename and surname.\n                Defaults to None.\n            m_probability_exact_match_surname (_type_, optional): If provided,\n                overrides the default m probability for the surname exact match\n                level for forename and surname.\n                Defaults to None.\n            m_probability_exact_match_forename (_type_, optional): If provided,\n                overrides the default m probability for the forename exact match\n                level for forename and forename.\n                Defaults to None.\n            m_probability_phonetic_match_surname (_type_, optional): If provided,\n                overrides the default m probability for the surname phonetic match\n                level for forename and surname.\n                Defaults to None.\n            m_probability_phonetic_match_forename (_type_, optional): If provided,\n                overrides the default m probability for the forename phonetic match\n                level for forename and forename.\n                Defaults to None.\n            m_probability_or_probabilities_surname_lev (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_surname_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_surname_jw (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_surname_jar (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_surname_jac (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_forename_lev (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_forename_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_forename_jw (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_forename_jar (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_forename_jac (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_else (_type_, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Basic Forename Surname Comparison\n                ```py\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\"first_name\", \"surname)\n                ```\n\n                Bespoke Forename Surname Comparison\n                ```py\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\n                        \"forename\",\n                        \"surname\",\n                        term_frequency_adjustments=True,\n                        tf_adjustment_col_forename_and_surname=\"full_name\",\n                        phonetic_forename_col_name=\"forename_dm\",\n                        phonetic_surname_col_name=\"surname_dm\",\n                        levenshtein_thresholds=[2],\n                        jaro_winkler_thresholds=[],\n                        jaccard_thresholds=[1],\n                    )\n                ```\n            === \":simple-apachespark: Spark\"\n                Basic Forename Surname Comparison\n                ```py\n                import splink.spark.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\"first_name\", \"surname)\n                ```\n\n                Bespoke Forename Surname Comparison\n                ```py\n                import splink.spark.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\n                        \"forename\",\n                        \"surname\",\n                        term_frequency_adjustments=True,\n                        tf_adjustment_col_forename_and_surname=\"full_name\",\n                        phonetic_forename_col_name=\"forename_dm\",\n                        phonetic_surname_col_name=\"surname_dm\",\n                        levenshtein_thresholds=[2],\n                        jaro_winkler_thresholds=[],\n                        jaccard_thresholds=[1],\n                    )\n                ```\n            === \":simple-sqlite: SQLite\"\n                Basic Forename Surname Comparison\n                ```py\n                import splink.sqlite.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\"first_name\", \"surname)\n                ```\n\n                Bespoke Forename Surname Comparison\n                ```py\n                import splink.sqlite.comparison_template_library as ctl\n                ctl.forename_surname_comparison(\n                        \"forename\",\n                        \"surname\",\n                        term_frequency_adjustments=True,\n                        tf_adjustment_col_forename_and_surname=\"full_name\",\n                        phonetic_forename_col_name=\"forename_dm\",\n                        phonetic_surname_col_name=\"surname_dm\",\n                        levenshtein_thresholds=[2],\n                        jaro_winkler_thresholds=[0.8],\n                    )\n                ```\n\n\n        Returns:\n            Comparison: A comparison that can be included in the Splink settings\n                dictionary.\n        \"\"\"\n\n        # Construct Comparison\n        comparison_levels = []\n\n        comparison_level = and_(\n            self._null_level(forename_col_name),\n            self._null_level(surname_col_name),\n            label_for_charts=\"Null\",\n        )\n\n        comparison_levels.append(comparison_level)\n\n        ### Forename surname exact match\n\n        if include_exact_match_level:\n            if set_to_lowercase:\n                forename_col_name_l = f\"lower({forename_col_name}_l)\"\n                forename_col_name_r = f\"lower({forename_col_name}_r)\"\n                surname_col_name_l = f\"lower({surname_col_name}_l)\"\n                surname_col_name_r = f\"lower({surname_col_name}_r)\"\n            else:\n                forename_col_name_l = f\"{forename_col_name}_l\"\n                forename_col_name_r = f\"{forename_col_name}_r\"\n                surname_col_name_l = f\"{surname_col_name}_l\"\n                surname_col_name_r = f\"{surname_col_name}_r\"\n\n            comparison_level = {\n                \"sql_condition\": f\"{forename_col_name_l} = {forename_col_name_r} \"\n                f\"AND {surname_col_name_l} = {surname_col_name_r}\",\n                \"tf_adjustment_column\": tf_adjustment_col_forename_and_surname,\n                \"tf_adjustment_weight\": 1.0,\n                \"m_probability\": m_probability_exact_match_forename_surname,\n                \"label_for_charts\": \"Full name exact match\",\n            }\n\n            comparison_levels.append(comparison_level)\n\n        ### Phonetic forename surname match\n\n        if phonetic_forename_col_name and phonetic_surname_col_name is not None:\n            comparison_level = {\n                \"sql_condition\": f\"{phonetic_forename_col_name}_l = \"\n                f\"{phonetic_forename_col_name}_r\"\n                f\" AND {phonetic_surname_col_name}_l = {phonetic_surname_col_name}_r\",\n                \"tf_adjustment_column\": tf_adjustment_col_forename_and_surname,\n                \"tf_adjustment_weight\": 1.0,\n                \"m_probability\": m_probability_exact_match_phonetic_forename_surname,\n                \"label_for_charts\": \"Full name phonetic match\",\n            }\n            comparison_levels.append(comparison_level)\n\n        ### Columns reversed match\n\n        if include_columns_reversed:\n            comparison_level = self._columns_reversed_level(\n                forename_col_name,\n                surname_col_name,\n                set_to_lowercase=set_to_lowercase,\n                tf_adjustment_column=tf_adjustment_col_forename_and_surname,\n                m_probability=m_probability_columns_reversed_forename_surname,\n            )\n            comparison_levels.append(comparison_level)\n\n        ### Surname Exact match\n\n        comparison_level = self._exact_match_level(\n            surname_col_name,\n            set_to_lowercase=set_to_lowercase,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability=m_probability_exact_match_surname,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels.append(comparison_level)\n\n        ### Forename Exact match\n\n        comparison_level = self._exact_match_level(\n            forename_col_name,\n            set_to_lowercase=set_to_lowercase,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability=m_probability_exact_match_forename,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels.append(comparison_level)\n\n        ### Ensure fuzzy match thresholds are iterable\n        levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n        damerau_levenshtein_thresholds = ensure_is_iterable(\n            damerau_levenshtein_thresholds\n        )\n        jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n        jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n        jaccard_thresholds = ensure_is_iterable(jaccard_thresholds)\n\n        ### Surname Fuzzy match\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                surname_col_name,\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_lev,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            levenshtein_thresholds = ensure_is_iterable(damerau_levenshtein_thresholds)\n            threshold_comparison_levels = distance_threshold_comparison_levels(\n                self,\n                surname_col_name,\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_dl,\n            )\n            comparison_levels = comparison_levels + threshold_comparison_levels\n\n        if len(jaro_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                surname_col_name,\n                distance_function_name=\"jaro-\",\n                distance_threshold_or_thresholds=jaro_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jar,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                surname_col_name,\n                distance_function_name=\"jaro-winkler\",\n                distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jw,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaccard_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                surname_col_name,\n                distance_function_name=\"jaccard\",\n                distance_threshold_or_thresholds=jaccard_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jac,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        ### Forename Fuzzy match\n\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                forename_col_name,\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_lev,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                forename_col_name,\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_dl,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                forename_col_name,\n                distance_function_name=\"jaro-winkler\",\n                distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jw,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                forename_col_name,\n                distance_function_name=\"jaro\",\n                distance_threshold_or_thresholds=jaro_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jar,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaccard_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                forename_col_name,\n                distance_function_name=\"jaccard\",\n                distance_threshold_or_thresholds=jaccard_thresholds,\n                set_to_lowercase=set_to_lowercase,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jac,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct Description\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        if phonetic_forename_col_name and phonetic_surname_col_name is not None:\n            comparison_desc += \"Phonetic match forename and surname vs. \"\n\n        if include_columns_reversed:\n            comparison_desc += \"Forename and surname columns reversed vs. \"\n\n        comparison_desc += \"Surname exact match vs. \"\n\n        comparison_desc += \"Forename exact match vs. \"\n\n        if len(levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"levenshtein\", levenshtein_thresholds\n            )\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n            )\n\n        if len(jaro_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"jaro\", jaro_thresholds\n            )\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"jaro-winkler\", jaro_winkler_thresholds\n            )\n\n        if len(jaccard_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"jaccard\", jaccard_thresholds\n            )\n\n        if len(levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                forename_col_name, \"levenshtein\", levenshtein_thresholds\n            )\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                surname_col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n            )\n\n        if len(jaro_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                forename_col_name, \"jaro\", jaro_thresholds\n            )\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                forename_col_name, \"jaro-winkler\", jaro_winkler_thresholds\n            )\n\n        if len(jaccard_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                forename_col_name, \"jaccard\", jaccard_thresholds\n            )\n\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _is_distance_subclass(self):\n        return False\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>class PostcodeComparisonBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        invalid_postcodes_as_null=False,\n        set_to_lowercase=True,\n        valid_postcode_regex=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\",\n        term_frequency_adjustments_full=False,\n        include_full_match_level=True,\n        include_sector_match_level=True,\n        include_district_match_level=True,\n        include_area_match_level=True,\n        lat_col: str = None,\n        long_col: str = None,\n        km_thresholds: int | float | list = [],\n        m_probability_full_match=None,\n        m_probability_sector_match=None,\n        m_probability_district_match=None,\n        m_probability_area_match=None,\n        m_probability_or_probabilities_km_distance=None,\n        m_probability_else=None,\n    ) -&gt; Comparison:\n        \"\"\"A wrapper to generate a comparison for a poscode column 'col_name'\n            with preselected defaults.\n\n        The default arguments will give a comparison with levels:\\n\n        - Exact match on full postcode\\n\n        - Exact match on sector\\n\n        - Exact match on district\\n\n        - Exact match on area\\n\n        - All other comparisons\n\n        Args:\n            col_name (str): The name of the column to compare.\n            invalid_postcodes_as_null (bool): If True, postcodes that do not adhere\n                to valid_postcode_regex will be included in the null level.\n                Defaults to False\n            set_to_lowercase (bool): If True, all postcodes are set to lowercase\n                during the pairwise comparisons.\n                Defaults to True\n            valid_postcode_regex (str): regular expression pattern that is used\n                to validate postcodes. If invalid_postcodes_as_null is True,\n                postcodes that do not adhere to valid_postcode_regex will be included\n                 in the null level.\n                 Defaults to \"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\"\n            term_frequency_adjustments_full (bool, optional): If True, apply\n                term frequency adjustments to the full postcode exact match level.\n                Defaults to False.\n            include_full_match_level (bool, optional): If True, include an exact\n                match on full postcode level. Defaults to True.\n            include_sector_match_level (bool, optional): If True, include an exact\n                match on sector level. Defaults to True.\n            include_district_match_level (bool, optional): If True, include an exact\n                match on district level. Defaults to True.\n            include_area_match_level (bool, optional): If True, include an exact\n                match on area level. Defaults to True.\n            include_distance_in_km_level (bool, optional): If True, include a\n                comparison of distance between postcodes as measured in kilometers.\n                Defaults to False.\n            lat_col (str): The name of a latitude column or the respective array\n                or struct column column containing the information, plus an index.\n                For example: lat, long_lat['lat'] or long_lat[0].\n            long_col (str): The name of a longitudinal column or the respective array\n                or struct column column containing the information, plus an index.\n                For example: long, long_lat['long'] or long_lat[1].\n            km_thresholds (int, float, list): The total distance in kilometers to\n                evaluate the distance_in_km_level comparison against.\n            m_probability_full_match (float, optional): Starting m\n                probability for full match level. Defaults to None.\n            m_probability_sector_match (float, optional): Starting m\n                probability for sector match level. Defaults to None.\n            m_probability_district_match (float, optional): Starting m\n                probability for district match level. Defaults to None.\n            m_probability_area_match (float, optional): Starting m\n                probability for area match level. Defaults to None.\n            m_probability_or_probabilities_km_distance (float, optional): Starting m\n                probability for 'distance in km' level. Defaults to None.\n            m_probability_else (float, optional): Starting m probability for\n                the 'everything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Basic Postcode Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\")\n                ```\n                Bespoke Postcode Comparison\n                ``` python\n                import splink.duckdb.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\",\n                                    invalid_postcodes_as_null=True,\n                                    include_distance_in_km_level=True,\n                                    lat_col=\"lat\",\n                                    long_col=\"long\",\n                                    km_thresholds=[10, 100]\n                                    )\n                ```\n            === \":simple-apachespark: Spark\"\n                Basic Postcode Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\")\n                ```\n                Bespoke Postcode Comparison\n                ``` python\n                import splink.spark.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\",\n                                    invalid_postcodes_as_null=True,\n                                    include_distance_in_km_level=True,\n                                    lat_col=\"lat\",\n                                    long_col=\"long\",\n                                    km_thresholds=[10, 100]\n                                    )\n                ```\n            === \":simple-amazonaws: Athena\"\n                Basic Postcode Comparison\n                ``` python\n                import splink.athena.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\")\n                ```\n                Bespoke Postcode Comparison\n                ``` python\n                import splink.athena.comparison_template_library as ctl\n                ctl.postcode_comparison(\"postcode\",\n                                    invalid_postcodes_as_null=True,\n                                    include_distance_in_km_level=True,\n                                    lat_col=\"lat\",\n                                    long_col=\"long\",\n                                    km_thresholds=[10, 100]\n                                    )\n                ```\n\n        Returns:\n            Comparison: A comparison that can be inclued in the Splink settings\n                dictionary.\n        \"\"\"\n\n        comparison_levels = []\n\n        if invalid_postcodes_as_null:\n            comparison_levels.append(self._null_level(col_name, valid_postcode_regex))\n        else:\n            comparison_levels.append(self._null_level(col_name))\n\n        if include_full_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=None,\n                term_frequency_adjustments=term_frequency_adjustments_full,\n                set_to_lowercase=set_to_lowercase,\n                m_probability=m_probability_full_match,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels.append(comparison_level)\n\n        if include_sector_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]\",\n                set_to_lowercase=set_to_lowercase,\n                m_probability=m_probability_sector_match,\n                manual_col_name_for_charts_label=\"Postcode Sector\",\n            )\n            comparison_levels.append(comparison_level)\n\n        if include_district_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?\",\n                set_to_lowercase=set_to_lowercase,\n                m_probability=m_probability_district_match,\n                manual_col_name_for_charts_label=\"Postcode District\",\n            )\n            comparison_levels.append(comparison_level)\n\n        if include_area_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=\"^[A-Za-z]{1,2}\",\n                set_to_lowercase=set_to_lowercase,\n                m_probability=m_probability_area_match,\n                manual_col_name_for_charts_label=\"Postcode Area\",\n            )\n            comparison_levels.append(comparison_level)\n\n        km_thresholds = ensure_is_iterable(km_thresholds)\n        if len(km_thresholds) &gt; 0:\n            if m_probability_or_probabilities_km_distance is None:\n                m_probability_or_probabilities_km_distance = [None] * len(km_thresholds)\n            m_probability_or_probabilities_km_distance = ensure_is_iterable(\n                m_probability_or_probabilities_km_distance\n            )\n\n            for thres, m_prob in zip(\n                km_thresholds,\n                m_probability_or_probabilities_km_distance,\n            ):\n                comparison_level = self._distance_in_km_level(\n                    lat_col,\n                    long_col,\n                    km_threshold=thres,\n                    m_probability=m_prob,\n                )\n                comparison_levels.append(comparison_level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct Description\n        comparison_desc = \"\"\n        if include_full_match_level:\n            comparison_desc += \"Exact match on full postcode vs. \"\n\n        if include_sector_match_level:\n            comparison_desc += \"exact match on sector vs. \"\n\n        if include_district_match_level:\n            comparison_desc += \"exact match on district vs. \"\n\n        if include_area_match_level:\n            comparison_desc += \"exact match on area vs. \"\n\n        if len(km_thresholds) &gt; 0:\n            desc = distance_threshold_description(\n                col_name, \"km_distance\", km_thresholds\n            )\n            comparison_desc += desc\n\n        comparison_desc += \"all other comparisons\"\n\n        comparison_dict = {\n            \"output_column_name\": col_name,\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n</code></pre> <p>             Bases: <code>Comparison</code></p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>class EmailComparisonBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        invalid_emails_as_null: bool = False,\n        valid_email_regex: str = \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\",\n        term_frequency_adjustments_full: bool = False,\n        include_exact_match_level: bool = True,\n        include_username_match_level: bool = True,\n        include_username_fuzzy_level: bool = True,\n        include_domain_match_level: bool = False,\n        levenshtein_thresholds: int | list = [],\n        damerau_levenshtein_thresholds: int | list = [],\n        jaro_winkler_thresholds: float | list = [0.88],\n        jaro_thresholds: float | list = [],\n        m_probability_full_match: bool = None,\n        m_probability_username_match: bool = None,\n        m_probability_or_probabilities_username_lev: float | list = None,\n        m_probability_or_probabilities_username_dl: float | list = None,\n        m_probability_or_probabilities_username_jw: float | list = None,\n        m_probability_or_probabilities_username_jar: float | list = None,\n        m_probability_or_probabilities_email_lev: float | list = None,\n        m_probability_or_probabilities_email_dl: float | list = None,\n        m_probability_or_probabilities_email_jw: float | list = None,\n        m_probability_or_probabilities_email_jar: float | list = None,\n        m_probability_domain_match: float | list = None,\n        m_probability_else: float | list = None,\n    ) -&gt; Comparison:\n        \"\"\"A wrapped to generate a comparison for an email colummn\n        'col_name' with preselected defaults.\n\n        The default arguments will give a comparison with levels:\\n\n        - Exact match on email\\n\n        - Exact match on username with different domain\\n\n        - Fuzzy match on email user Jaro-Winkler\\n\n        - Fuzzy match on username using Jaro-Winkler \\n\n        - All other comparisons\n\n        Args:\n            col_name (str): The name of the column to compare.\n            invalid_email_as_null (bool): If True, emails that do not adhere\n                to valid_email_regex will be included in the null level.\n                Defaults to False\n            valid_email_regex (str): regular expression pattern that is used\n                to validate emails. If invalid_emails_as_null is True,\n                emails that do not adhere to valid_email_regex will be included\n                 in the null level.\n                 Defaults to \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n            term_frequency_adjustments_full (bool, optional): If True, apply\n                term frequency adjustments to the full email exact match level.\n                Defaults to False.\n            include_exact_match_level (bool, optional): If True, include an exact\n                match on full email level. Defaults to True.\n            include_username_match_level (bool, optional): If True, include an exact\n                match on username only level. Defaults to True.\n            include_username_fuzzy_level (bool, optional): If True, include a level\n                for fuzzy match on username. Defaults to True.\n            include_domain_match_level (bool, optional): If True, include an exact\n                match on domain only level. Defaults to True.\n            levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for levenshtein similarity level(s).\n                Defaults to []\n            damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n                to use for damerau-levenshtein similarity level(s).\n                Defaults to []\n            jaro_winkler_thresholds (Union[int, list], optional): The thresholds\n                to use for jaro_winkler similarity level(s).\n                Defaults to [0.88]\n            jaro_thresholds (Union[int, list], optional): The thresholds\n                to use for jaro similarity level(s).\n                Defaults to []\n            m_probability_full_match (float, optional): Starting m\n                probability for full match level. Defaults to None.\n            m_probability_username_match (float, optional): Starting m probability\n                for username only match level. Defaults to None.\n            m_probability_or_probabilities_username_lev (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_username_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_username_jw (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_username_jar (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_email_lev (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_email_dl (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_email_jw (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_or_probabilities_email_jar (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the thresholds specified. Defaults to None.\n            m_probability_domain_match (float, optional): Starting m probability\n                for domain only match level. Defaults to None.\n            m_probability_else (float, optional): Starting m probability for\n                the 'everything else' level. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Basic email Comparison\n                ``` python\n                import splink.duckdb.duckdb_comparison_template_library as ctl\n                ctl.email_comparison(\"email\")\n                ```\n                Bespoke email Comparison\n                ``` python\n                import splink.duckdb.duckdb_comparison_template_library as ctl\n                ctl.email_comparison(\"email\",\n                                    levenshtein_thresholds = [2],\n                                    damerau_levenshtein_thresholds = [2],\n                                    invalid_emails_as_null = True,\n                                    include_username_match_level = True,\n                                    include_domain_match_level = True,\n                                    )\n                ```\n            === \":simple-apachespark: Spark\"\n                Basic email Comparison\n                ``` python\n                import splink.spark.spark_comparison_template_library as ctl\n                ctl.email_comparison(col_name = \"email\")\n                ```\n                Bespoke email Comparison\n                ``` python\n                import splink.spark.spark_comparison_template_library as ctl\n                ctl.email_comparison(\"email\",\n                                    levenshtein_thresholds = [2],\n                                    damerau_levenshtein_thresholds = [2],\n                                    invalid_emails_as_null = True,\n                                    include_username_match_level = True,\n                                    include_domain_match_level = True,\n                                    )\n\n                ```\n\n        Returns:\n            Comparison: A comparison that can be inclued in the Splink settings\n                dictionary.\n        \"\"\"\n        # Contstruct comparrison\n\n        comparison_levels = []\n\n        # Decide whether invalid emails should be treated as null\n        if invalid_emails_as_null:\n            comparison_levels.append(\n                self._null_level(col_name, valid_string_pattern=valid_email_regex)\n            )\n        else:\n            comparison_levels.append(self._null_level(col_name))\n\n        # Exact match on full email\n\n        if include_exact_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=None,\n                term_frequency_adjustments=term_frequency_adjustments_full,\n                m_probability=m_probability_full_match,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels.append(comparison_level)\n\n        # Exact match on username with different domain\n\n        if include_username_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=\"^[^@]+\",\n                m_probability=m_probability_username_match,\n                include_colname_in_charts_label=True,\n                manual_col_name_for_charts_label=\"Username\",\n            )\n            comparison_levels.append(comparison_level)\n\n        # Ensure fuzzy match thresholds are iterable\n\n        damerau_levenshtein_thresholds = ensure_is_iterable(\n            damerau_levenshtein_thresholds\n        )\n        levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n        jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n        jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n\n        # Fuzzy match on full email\n\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_email_lev,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_email_dl,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaro-winkler\",\n                distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jw,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaro\",\n                distance_threshold_or_thresholds=jaro_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jar,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        # Fuzzy match on username only\n        if include_username_fuzzy_level:\n            if len(levenshtein_thresholds) &gt; 0:\n                threshold_levels = distance_threshold_comparison_levels(\n                    self,\n                    col_name,\n                    regex_extract=\"^[^@]+\",\n                    distance_function_name=\"levenshtein\",\n                    distance_threshold_or_thresholds=levenshtein_thresholds,\n                    m_probability_or_probabilities_thres=m_probability_or_probabilities_username_lev,\n                    include_colname_in_charts_label=True,\n                    manual_col_name_for_charts_label=\"Username\",\n                )\n                comparison_levels = comparison_levels + threshold_levels\n\n            if len(damerau_levenshtein_thresholds) &gt; 0:\n                threshold_levels = distance_threshold_comparison_levels(\n                    self,\n                    col_name,\n                    regex_extract=\"^[^@]+\",\n                    distance_function_name=\"damerau-levenshtein\",\n                    distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                    m_probability_or_probabilities_thres=m_probability_or_probabilities_username_dl,\n                    include_colname_in_charts_label=True,\n                    manual_col_name_for_charts_label=\"Username\",\n                )\n                comparison_levels = comparison_levels + threshold_levels\n\n            if len(jaro_winkler_thresholds) &gt; 0:\n                threshold_levels = distance_threshold_comparison_levels(\n                    self,\n                    col_name,\n                    regex_extract=\"^[^@]+\",\n                    distance_function_name=\"jaro-winkler\",\n                    distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                    m_probability_or_probabilities_thres=m_probability_or_probabilities_username_jw,\n                    include_colname_in_charts_label=True,\n                    manual_col_name_for_charts_label=\"Username\",\n                )\n                comparison_levels = comparison_levels + threshold_levels\n\n            if len(jaro_thresholds) &gt; 0:\n                threshold_levels = distance_threshold_comparison_levels(\n                    self,\n                    col_name,\n                    distance_function_name=\"jaro\",\n                    distance_threshold_or_thresholds=jaro_thresholds,\n                    m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jar,\n                    include_colname_in_charts_label=True,\n                )\n                comparison_levels = comparison_levels + threshold_levels\n\n        # Domain-only match\n\n        if include_domain_match_level:\n            comparison_level = self._exact_match_level(\n                col_name,\n                regex_extract=\"@([^@]+)$\",\n                m_probability=m_probability_domain_match,\n                manual_col_name_for_charts_label=\"Email Domain\",\n            )\n            comparison_levels.append(comparison_level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        # Construct Description\n\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        if include_username_match_level:\n            comparison_desc += \"Exact username match different domain vs. \"\n\n        if len(levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                \"fuzzy email\", \"levenshtein\", jaro_winkler_thresholds\n            )\n            comparison_desc += distance_threshold_description(\n                \"fuzzy username\", \"levenshtein\", jaro_winkler_thresholds\n            )\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                \"fuzzy email\", \"damerau_levenshtein\", jaro_winkler_thresholds\n            )\n            comparison_desc += distance_threshold_description(\n                \"fuzzy username\", \"levenshtein\", jaro_winkler_thresholds\n            )\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            comparison_desc += distance_threshold_description(\n                \"fuzzy email\", \"jaro_winkler\", jaro_winkler_thresholds\n            )\n            comparison_desc += distance_threshold_description(\n                \"fuzzy username\", \"jaro_winkler\", jaro_winkler_thresholds\n            )\n\n        if include_domain_match_level:\n            comparison_desc += \"Domain-only match vs.\"\n\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n\n    @property\n    def _is_distance_subclass(self):\n        return False\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.DateComparisonBase.__init__","title":"<code>__init__(col_name, cast_strings_to_date=False, date_format=None, invalid_dates_as_null=False, include_exact_match_level=True, term_frequency_adjustments=False, separate_1st_january=False, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[1], datediff_thresholds=[1, 1, 10], datediff_metrics=['month', 'year', 'year'], m_probability_exact_match=None, m_probability_1st_january=None, m_probability_or_probabilities_lev=None, m_probability_or_probabilities_dl=None, m_probability_or_probabilities_datediff=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a date column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match (on 1st of January only)</p> </li> <li> <p>Exact match (all other dates)</p> </li> <li> <p>Damerau-Levenshtein distance &lt;= 1</p> </li> <li> <p>Date difference &lt;= 1 year</p> </li> <li> <p>Date difference &lt;= 10 years </p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd). Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB when invalid_dates_as_null=True</p> <code>None</code> <code>invalid_dates_as_null</code> <code>bool</code> <p>assign any dates that do not adhere to date_format to the null level. Defaults to False.</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>separate_1st_january</code> <code>bool</code> <p>If True, include a separate exact match comparison level when date is 1st January.</p> <code>False</code> <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to [1]</p> <code>[1]</code> <code>datediff_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for datediff similarity level(s). Defaults to [1, 1].</p> <code>[1, 1, 10]</code> <code>datediff_metrics</code> <code>Union[str, list]</code> <p>The metrics to apply thresholds to for datediff similarity level(s). Defaults to [\"month\", \"year\"].</p> <code>['month', 'year', 'year']</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd).</p> <code>None</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the levenshtein thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the damerau-levenshtein thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_datediff</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the datediff thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <p>Basic Date Comparison </p><pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\")\n</code></pre> Bespoke Date Comparison <pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    damerau_levenshtein_thresholds=[],\n                    levenshtein_thresholds=[2],\n                    datediff_thresholds=[1, 1],\n                    datediff_metrics=[\"month\", \"year\"])\n</code></pre> Date Comparison casting columns date and assigning values that do not match the date_format to the null level <pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y',\n                    invalid_dates_as_null=True)\n</code></pre> <p>Basic Date Comparison </p><pre><code>import splink.spark.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\")\n</code></pre> Bespoke Date Comparison <pre><code>import splink.spark.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    damerau_levenshtein_thresholds=[],\n                    levenshtein_thresholds=[2],\n                    datediff_thresholds=[1, 1],\n                    datediff_metrics=[\"month\", \"year\"])\n</code></pre> Date Comparison casting columns date and assigning values that do not match the date_format to the null level <pre><code>import splink.spark.comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    cast_strings_to_date=True,\n                    date_format='dd/mm/yyyy',\n                    invalid_dates_as_null=True)\n</code></pre> Source code in <code>splink/comparison_template_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    cast_strings_to_date: bool = False,\n    date_format: str = None,\n    invalid_dates_as_null: bool = False,\n    include_exact_match_level: bool = True,\n    term_frequency_adjustments: bool = False,\n    separate_1st_january: bool = False,\n    levenshtein_thresholds: int | list = [],\n    damerau_levenshtein_thresholds: int | list = [1],\n    datediff_thresholds: int | list = [1, 1, 10],\n    datediff_metrics: str | list = [\"month\", \"year\", \"year\"],\n    m_probability_exact_match: float = None,\n    m_probability_1st_january: float = None,\n    m_probability_or_probabilities_lev: float | list = None,\n    m_probability_or_probabilities_dl: float | list = None,\n    m_probability_or_probabilities_datediff: float | list = None,\n    m_probability_else: float = None,\n) -&gt; Comparison:\n    \"\"\"A wrapper to generate a comparison for a date column the data in\n    `col_name` with preselected defaults.\n\n    The default arguments will give a comparison with comparison levels:\\n\n    - Exact match (on 1st of January only)\\n\n    - Exact match (all other dates)\\n\n    - Damerau-Levenshtein distance &lt;= 1\\n\n    - Date difference &lt;= 1 year\\n\n    - Date difference &lt;= 10 years \\n\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare.\n        cast_strings_to_date (bool, optional): Set to True to\n            enable date-casting when input dates are strings. Also adjust\n            date_format if date-strings are not in (yyyy-mm-dd) format.\n            Defaults to False.\n        date_format (str, optional): Format of input dates if date-strings\n            are given. Must be consistent across record pairs. If None\n            (the default), downstream functions for each backend assign\n            date_format to ISO 8601 format (yyyy-mm-dd).\n            Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB\n            when invalid_dates_as_null=True\n        invalid_dates_as_null (bool, optional): assign any dates that do not adhere\n            to date_format to the null level. Defaults to False.\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level. Defaults to True.\n        term_frequency_adjustments (bool, optional): If True, apply term frequency\n            adjustments to the exact match level. Defaults to False.\n        separate_1st_january (bool, optional): If True, include a separate\n            exact match comparison level when date is 1st January.\n        levenshtein_thresholds (Union[int, list], optional): The thresholds to use\n            for levenshtein similarity level(s).\n            Defaults to []\n        damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for damerau-levenshtein similarity level(s).\n            Defaults to [1]\n        datediff_thresholds (Union[int, list], optional): The thresholds to use\n            for datediff similarity level(s).\n            Defaults to [1, 1].\n        datediff_metrics (Union[str, list], optional): The metrics to apply\n            thresholds to for datediff similarity level(s).\n            Defaults to [\"month\", \"year\"].\n        cast_strings_to_date (bool, optional): Set to True to\n            enable date-casting when input dates are strings. Also adjust\n            date_format if date-strings are not in (yyyy-mm-dd) format.\n            Defaults to False.\n        date_format (str, optional): Format of input dates if date-strings\n            are given. Must be consistent across record pairs. If None\n            (the default), downstream functions for each backend assign\n            date_format to ISO 8601 format (yyyy-mm-dd).\n        m_probability_exact_match (float, optional): If provided, overrides the\n            default m probability for the exact match level. Defaults to None.\n        m_probability_or_probabilities_lev (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the levenshtein thresholds specified. Defaults to None.\n        m_probability_or_probabilities_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the damerau-levenshtein thresholds specified. Defaults to None.\n        m_probability_or_probabilities_datediff (Union[float, list], optional):\n            If provided, overrides the default m probabilities\n            for the datediff thresholds specified. Defaults to None.\n        m_probability_else (float, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Basic Date Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\")\n            ```\n            Bespoke Date Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\",\n                                damerau_levenshtein_thresholds=[],\n                                levenshtein_thresholds=[2],\n                                datediff_thresholds=[1, 1],\n                                datediff_metrics=[\"month\", \"year\"])\n            ```\n            Date Comparison casting columns date and assigning values that do not\n            match the date_format to the null level\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\",\n                                cast_strings_to_date=True,\n                                date_format='%d/%m/%Y',\n                                invalid_dates_as_null=True)\n            ```\n        === \":simple-apachespark: Spark\"\n            Basic Date Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\")\n            ```\n            Bespoke Date Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\",\n                                damerau_levenshtein_thresholds=[],\n                                levenshtein_thresholds=[2],\n                                datediff_thresholds=[1, 1],\n                                datediff_metrics=[\"month\", \"year\"])\n            ```\n            Date Comparison casting columns date and assigning values that do not\n            match the date_format to the null level\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.date_comparison(\"date_of_birth\",\n                                cast_strings_to_date=True,\n                                date_format='dd/mm/yyyy',\n                                invalid_dates_as_null=True)\n            ```\n    Returns:\n        Comparison: A comparison that can be inclued in the Splink settings\n            dictionary.\n    \"\"\"\n    # Construct Comparison\n    comparison_levels = []\n    comparison_levels.append(\n        self._null_level(\n            col_name,\n            invalid_dates_as_null=invalid_dates_as_null,\n            valid_string_pattern=date_format,\n        )\n    )\n\n    # Validate user inputs\n    datediff_error_logger(thresholds=datediff_thresholds, metrics=datediff_metrics)\n\n    if separate_1st_january:\n        dob_first_jan = {\n            \"sql_condition\": f\"SUBSTR({col_name}_l, 6, 5) = '01-01'\",\n            \"label_for_charts\": \"Date is 1st Jan\",\n        }\n        comparison_level = and_(\n            self._exact_match_level(col_name),\n            dob_first_jan,\n            label_for_charts=\"Exact match and 1st Jan\",\n        )\n\n        if m_probability_1st_january:\n            comparison_level[\"m_probability\"] = m_probability_1st_january\n        if term_frequency_adjustments:\n            comparison_level[\"tf_adjustment_column\"] = col_name\n        comparison_levels.append(comparison_level)\n\n    if include_exact_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability=m_probability_exact_match,\n        )\n        comparison_levels.append(comparison_level)\n\n    levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n    if len(levenshtein_thresholds) &gt; 0:\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"levenshtein\",\n            distance_threshold_or_thresholds=levenshtein_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    damerau_levenshtein_thresholds = ensure_is_iterable(\n        damerau_levenshtein_thresholds\n    )\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        damerau_levenshtein_thresholds = ensure_is_iterable(\n            damerau_levenshtein_thresholds\n        )\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"damerau-levenshtein\",\n            distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    datediff_thresholds = ensure_is_iterable(datediff_thresholds)\n    datediff_metrics = ensure_is_iterable(datediff_metrics)\n    if len(datediff_thresholds) &gt; 0:\n        if m_probability_or_probabilities_datediff is None:\n            m_probability_or_probabilities_datediff = [None] * len(\n                datediff_thresholds\n            )\n        m_probability_or_probabilities_datediff = ensure_is_iterable(\n            m_probability_or_probabilities_datediff\n        )\n\n        for thres, metric, m_prob in zip(\n            datediff_thresholds,\n            datediff_metrics,\n            m_probability_or_probabilities_datediff,\n        ):\n            comparison_level = self._datediff_level(\n                col_name,\n                date_threshold=thres,\n                date_metric=metric,\n                m_probability=m_prob,\n                cast_strings_to_date=cast_strings_to_date,\n                date_format=date_format,\n            )\n            comparison_levels.append(comparison_level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct Description\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    if len(levenshtein_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"levenshtein\", levenshtein_thresholds\n        )\n        comparison_desc += desc\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n        )\n        comparison_desc += desc\n\n    if len(datediff_thresholds) &gt; 0:\n        datediff_desc = \", \".join(\n            [\n                f\"{m.title()}(s): {v}\"\n                for v, m in zip(datediff_thresholds, datediff_metrics)\n            ]\n        )\n        plural = \"\" if len(datediff_thresholds) == 1 else \"s\"\n        comparison_desc += (\n            f\"Dates within the following threshold{plural} {datediff_desc} vs. \"\n        )\n\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.NameComparisonBase.__init__","title":"<code>__init__(col_name, regex_extract=None, set_to_lowercase=False, include_exact_match_level=True, phonetic_col_name=None, term_frequency_adjustments=False, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[1], jaro_thresholds=[], jaro_winkler_thresholds=[0.9, 0.8], jaccard_thresholds=[], m_probability_exact_match_name=None, m_probability_exact_match_phonetic_name=None, m_probability_or_probabilities_lev=None, m_probability_or_probabilities_dl=None, m_probability_or_probabilities_jar=None, m_probability_or_probabilities_jw=None, m_probability_or_probabilities_jac=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a name column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match </p> </li> <li> <p>Damerau-Levenshtein Distance &lt;= 1</p> </li> <li> <p>Jaro Winkler similarity &gt;= 0.9</p> </li> <li> <p>Jaro Winkler similarity &gt;= 0.8</p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all names are set to lowercase during the pairwise comparisons. Defaults to False</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level for col_name. Defaults to True.</p> <code>True</code> <code>phonetic_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of col_name. Including parameter will create an exact match level for  phonetic_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for \"col_name\". Defaults to False.</p> <code>False</code> <code>term_frequency_adjustments_phonetic_name</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for \"phonetic_col_name\". Defaults to False.</p> required <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to [1]</p> <code>[1]</code> <code>jaro_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro similarity level(s). Defaults to []</p> <code>[]</code> <code>jaro_winkler_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro_winkler similarity level(s). Defaults to [0.9, 0.8]</p> <code>[0.9, 0.8]</code> <code>jaccard_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaccard similarity level(s). Defaults to []</p> <code>[]</code> <code>m_probability_exact_match_name</code> <code>_type_</code> <p>Starting m probability for exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_phonetic_name</code> <code>_type_</code> <p>Starting m probability for exact match level for phonetic_col_name. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_datediff</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> required <code>m_probability_or_probabilities_jar</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaro thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jw</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaro winkler thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jac</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaccard thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>Starting m probability for the 'everything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Basic Name Comparison </p><pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.name_comparison(\"name\")\n</code></pre> Bespoke Name Comparison <pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.name_comparison(\"name\",\n                    phonetic_col_name = \"name_dm\",\n                    term_frequency_adjustments = True,\n                    levenshtein_thresholds=[2],\n                    damerau_levenshtein_thresholds=[],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1]\n                    )\n</code></pre> <p>Basic Name Comparison </p><pre><code>import splink.spark.comparison_template_library as ctl\nctl.name_comparison(\"name\")\n</code></pre> Bespoke Name Comparison <pre><code>import splink.spark.comparison_template_library as ctl\nctl.name_comparison(\"name\",\n                    phonetic_col_name = \"name_dm\",\n                    term_frequency_adjustments = True,\n                    levenshtein_thresholds=[2],\n                    damerau_levenshtein_thresholds=[],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1]\n                    )\n</code></pre> <p>Basic Name Comparison </p><pre><code>import splink.sqlite.comparison_template_library as ctl\nctl.name_comparison(\"name\")\n</code></pre> Bespoke Name Comparison <pre><code>import splink.sqlite.comparison_template_library as ctl\nctl.name_comparison(\"name\",\n                    phonetic_col_name = \"name_dm\",\n                    term_frequency_adjustments = True,\n                    levenshtein_thresholds=[2],\n                    damerau_levenshtein_thresholds=[],\n                    jaro_winkler_thresholds=[0.8],\n                    )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    regex_extract: str = None,\n    set_to_lowercase: str = False,\n    include_exact_match_level: bool = True,\n    phonetic_col_name: str = None,\n    term_frequency_adjustments: bool = False,\n    levenshtein_thresholds: int | list = [],\n    damerau_levenshtein_thresholds: int | list = [1],\n    jaro_thresholds: float | list = [],\n    jaro_winkler_thresholds: float | list = [0.9, 0.8],\n    jaccard_thresholds: float | list = [],\n    m_probability_exact_match_name: float = None,\n    m_probability_exact_match_phonetic_name: float = None,\n    m_probability_or_probabilities_lev: float | list = None,\n    m_probability_or_probabilities_dl: float | list = None,\n    m_probability_or_probabilities_jar: float | list = None,\n    m_probability_or_probabilities_jw: float | list = None,\n    m_probability_or_probabilities_jac: float | list = None,\n    m_probability_else: float = None,\n) -&gt; Comparison:\n    \"\"\"A wrapper to generate a comparison for a name column the data in\n    `col_name` with preselected defaults.\n\n    The default arguments will give a comparison with comparison levels:\\n\n    - Exact match \\n\n    - Damerau-Levenshtein Distance &lt;= 1\n    - Jaro Winkler similarity &gt;= 0.9\\n\n    - Jaro Winkler similarity &gt;= 0.8\\n\n    - Anything else\n\n    Args:\n        col_name (str): The name of the column to compare.\n        regex_extract (str): Regular expression pattern to evaluate a match on.\n        set_to_lowercase (bool): If True, all names are set to lowercase\n            during the pairwise comparisons.\n            Defaults to False\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level for col_name. Defaults to True.\n        phonetic_col_name (str): The name of the column with phonetic reduction\n            (such as dmetaphone) of col_name. Including parameter will create\n            an exact match level for  phonetic_col_name. The phonetic column must\n            be present in the dataset to use this parameter.\n            Defaults to None\n        term_frequency_adjustments (bool, optional): If True, apply term\n            frequency adjustments to the exact match level for \"col_name\".\n            Defaults to False.\n        term_frequency_adjustments_phonetic_name (bool, optional): If True, apply\n            term frequency adjustments to the exact match level for\n            \"phonetic_col_name\".\n            Defaults to False.\n        levenshtein_thresholds (Union[int, list], optional): The thresholds to use\n            for levenshtein similarity level(s).\n            Defaults to []\n        damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for damerau-levenshtein similarity level(s).\n            Defaults to [1]\n        jaro_thresholds (Union[int, list], optional): The thresholds to use\n            for jaro similarity level(s).\n            Defaults to []\n        jaro_winkler_thresholds (Union[int, list], optional): The thresholds to use\n            for jaro_winkler similarity level(s).\n            Defaults to [0.9, 0.8]\n        jaccard_thresholds (Union[int, list], optional): The thresholds to use\n            for jaccard similarity level(s).\n            Defaults to []\n        m_probability_exact_match_name (_type_, optional): Starting m probability\n            for exact match level. Defaults to None.\n        m_probability_exact_match_phonetic_name (_type_, optional): Starting m\n            probability for exact match level for phonetic_col_name.\n            Defaults to None.\n        m_probability_or_probabilities_lev (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_datediff (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_jar (Union[float, list], optional):\n            Starting m probabilities for the jaro thresholds specified.\n            Defaults to None.\n        m_probability_or_probabilities_jw (Union[float, list], optional):\n            Starting m probabilities for the jaro winkler thresholds specified.\n            Defaults to None.\n        m_probability_or_probabilities_jac (Union[float, list], optional):\n            Starting m probabilities for the jaccard thresholds specified.\n            Defaults to None.\n        m_probability_else (_type_, optional): Starting m probability for\n            the 'everything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Basic Name Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.name_comparison(\"name\")\n            ```\n            Bespoke Name Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.name_comparison(\"name\",\n                                phonetic_col_name = \"name_dm\",\n                                term_frequency_adjustments = True,\n                                levenshtein_thresholds=[2],\n                                damerau_levenshtein_thresholds=[],\n                                jaro_winkler_thresholds=[],\n                                jaccard_thresholds=[1]\n                                )\n            ```\n        === \":simple-apachespark: Spark\"\n            Basic Name Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.name_comparison(\"name\")\n            ```\n            Bespoke Name Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.name_comparison(\"name\",\n                                phonetic_col_name = \"name_dm\",\n                                term_frequency_adjustments = True,\n                                levenshtein_thresholds=[2],\n                                damerau_levenshtein_thresholds=[],\n                                jaro_winkler_thresholds=[],\n                                jaccard_thresholds=[1]\n                                )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Basic Name Comparison\n            ``` python\n            import splink.sqlite.comparison_template_library as ctl\n            ctl.name_comparison(\"name\")\n            ```\n            Bespoke Name Comparison\n            ``` python\n            import splink.sqlite.comparison_template_library as ctl\n            ctl.name_comparison(\"name\",\n                                phonetic_col_name = \"name_dm\",\n                                term_frequency_adjustments = True,\n                                levenshtein_thresholds=[2],\n                                damerau_levenshtein_thresholds=[],\n                                jaro_winkler_thresholds=[0.8],\n                                )\n            ```\n\n    Returns:\n        Comparison: A comparison that can be included in the Splink settings\n            dictionary.\n    \"\"\"\n\n    # Construct Comparison\n    comparison_levels = []\n    comparison_levels.append(self._null_level(col_name))\n\n    if include_exact_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            term_frequency_adjustments=term_frequency_adjustments,\n            m_probability=m_probability_exact_match_name,\n            include_colname_in_charts_label=True,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n        )\n        comparison_levels.append(comparison_level)\n\n        if phonetic_col_name is not None:\n            comparison_level = self._exact_match_level(\n                phonetic_col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match_phonetic_name,\n                include_colname_in_charts_label=True,\n                regex_extract=regex_extract,\n                set_to_lowercase=set_to_lowercase,\n            )\n            comparison_levels.append(comparison_level)\n\n    levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n    if len(levenshtein_thresholds) &gt; 0:\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"levenshtein\",\n            distance_threshold_or_thresholds=levenshtein_thresholds,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_lev,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    damerau_levenshtein_thresholds = ensure_is_iterable(\n        damerau_levenshtein_thresholds\n    )\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        levenshtein_thresholds = ensure_is_iterable(damerau_levenshtein_thresholds)\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"damerau-levenshtein\",\n            distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_dl,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n    if len(jaro_thresholds) &gt; 0:\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"jaro\",\n            distance_threshold_or_thresholds=jaro_thresholds,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jar,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n    if len(jaro_winkler_thresholds) &gt; 0:\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"jaro-winkler\",\n            distance_threshold_or_thresholds=jaro_winkler_thresholds,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jw,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    jaccard_thresholds = ensure_is_iterable(jaccard_thresholds)\n    if len(jaccard_thresholds) &gt; 0:\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"jaccard\",\n            distance_threshold_or_thresholds=jaccard_thresholds,\n            regex_extract=regex_extract,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_jac,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct Description\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    if phonetic_col_name is not None:\n        comparison_desc += \"Names with phonetic exact match vs. \"\n\n    if len(levenshtein_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"levenshtein\", levenshtein_thresholds\n        )\n        comparison_desc += desc\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n        )\n        comparison_desc += desc\n\n    if len(jaro_thresholds) &gt; 0:\n        desc = distance_threshold_description(col_name, \"jaro\", jaro_thresholds)\n        comparison_desc += desc\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"jaro_winkler\", jaro_winkler_thresholds\n        )\n        comparison_desc += desc\n\n    if len(jaccard_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"jaccard\", jaccard_thresholds\n        )\n        comparison_desc += desc\n\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.ForenameSurnameComparisonBase.__init__","title":"<code>__init__(forename_col_name, surname_col_name, set_to_lowercase=False, include_exact_match_level=True, include_columns_reversed=True, term_frequency_adjustments=False, tf_adjustment_col_forename_and_surname=None, phonetic_forename_col_name=None, phonetic_surname_col_name=None, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[], jaro_winkler_thresholds=[0.88], jaro_thresholds=[], jaccard_thresholds=[], m_probability_exact_match_forename_surname=None, m_probability_exact_match_phonetic_forename_surname=None, m_probability_columns_reversed_forename_surname=None, m_probability_exact_match_surname=None, m_probability_exact_match_forename=None, m_probability_or_probabilities_surname_lev=None, m_probability_or_probabilities_surname_dl=None, m_probability_or_probabilities_surname_jw=None, m_probability_or_probabilities_surname_jar=None, m_probability_or_probabilities_surname_jac=None, m_probability_or_probabilities_forename_lev=None, m_probability_or_probabilities_forename_dl=None, m_probability_or_probabilities_forename_jw=None, m_probability_or_probabilities_forename_jar=None, m_probability_or_probabilities_forename_jac=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a name column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match forename and surname</p> </li> <li> <p>Macth of forename and surname reversed</p> </li> <li> <p>Exact match surname</p> </li> <li> <p>Exact match forename</p> </li> <li> <p>Fuzzy match surname jaro-winkler &gt;= 0.88</p> </li> <li> <p>Fuzzy match forename jaro-winkler&gt;=  0.88</p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>forename_col_name</code> <code>str</code> <p>The name of the forename column to compare</p> required <code>surname_col_name</code> <code>str</code> <p>The name of the surname column to compare</p> required <code>set_to_lowercase</code> <code>bool</code> <p>If True, all names are set to lowercase during the pairwise comparisons. Defaults to False</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level for col_name. Defaults to True.</p> <code>True</code> <code>include_columns_reversed</code> <code>bool</code> <p>If True, include a comparison level for forename and surname being swapped. Defaults to True</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for forename_col_name and surname_col_name. Applies term frequency adjustments to full name exact match level and columns reversed exact match level if tf_adjustment_col_forename_and_surname is provided. Applies term frequency adjustments to phonetic_forename_col_name and phonetic_surname_col_name exact match levels, if they are provided. Defaults to False.</p> <code>False</code> <code>tf_adjustment_col_forename_and_surname</code> <code>str</code> <p>The name of a combined forename surname column. This column is used to provide term frequency adjustments for forename surname exact match and columns reversed levels. Defaults to None</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all postcodes are set to lowercase during the pairwise comparisons. Defaults to True</p> <code>False</code> <code>phonetic_forename_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of forename_col_name. Including parameter along with phonetic_surname_col_name will create an exact match level for \"Full name phonetic match\". The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>phonetic_surname_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of surname_col_name. Including this parameter along with phonetic_forename_col_name will create an exact match level for \"Full name phonetic match\". The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>jaro_winkler_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro_winkler similarity level(s) for surname_col_name and forename_col_name. Defaults to [0.88]</p> <code>[0.88]</code> <code>jaro_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>jaccard_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaccard similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>m_probability_exact_match_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_phonetic_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the phonetic match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_columns_reversed_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the columns reversed level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_columns_reversed_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the columns reversed level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the surname exact match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_forename</code> <code>_type_</code> <p>If provided, overrides the default m probability for the forename exact match level for forename and forename. Defaults to None.</p> <code>None</code> <code>m_probability_phonetic_match_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the surname phonetic match level for forename and surname. Defaults to None.</p> required <code>m_probability_phonetic_match_forename</code> <code>_type_</code> <p>If provided, overrides the default m probability for the forename phonetic match level for forename and forename. Defaults to None.</p> required <code>m_probability_or_probabilities_surname_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_jar</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_jac</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_jar</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_jac</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark SQLite <p>Basic Forename Surname Comparison </p><pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.forename_surname_comparison(\"first_name\", \"surname)\n</code></pre> <p>Bespoke Forename Surname Comparison </p><pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.forename_surname_comparison(\n        \"forename\",\n        \"surname\",\n        term_frequency_adjustments=True,\n        tf_adjustment_col_forename_and_surname=\"full_name\",\n        phonetic_forename_col_name=\"forename_dm\",\n        phonetic_surname_col_name=\"surname_dm\",\n        levenshtein_thresholds=[2],\n        jaro_winkler_thresholds=[],\n        jaccard_thresholds=[1],\n    )\n</code></pre> <p>Basic Forename Surname Comparison </p><pre><code>import splink.spark.comparison_template_library as ctl\nctl.forename_surname_comparison(\"first_name\", \"surname)\n</code></pre> <p>Bespoke Forename Surname Comparison </p><pre><code>import splink.spark.comparison_template_library as ctl\nctl.forename_surname_comparison(\n        \"forename\",\n        \"surname\",\n        term_frequency_adjustments=True,\n        tf_adjustment_col_forename_and_surname=\"full_name\",\n        phonetic_forename_col_name=\"forename_dm\",\n        phonetic_surname_col_name=\"surname_dm\",\n        levenshtein_thresholds=[2],\n        jaro_winkler_thresholds=[],\n        jaccard_thresholds=[1],\n    )\n</code></pre> <p>Basic Forename Surname Comparison </p><pre><code>import splink.sqlite.comparison_template_library as ctl\nctl.forename_surname_comparison(\"first_name\", \"surname)\n</code></pre> <p>Bespoke Forename Surname Comparison </p><pre><code>import splink.sqlite.comparison_template_library as ctl\nctl.forename_surname_comparison(\n        \"forename\",\n        \"surname\",\n        term_frequency_adjustments=True,\n        tf_adjustment_col_forename_and_surname=\"full_name\",\n        phonetic_forename_col_name=\"forename_dm\",\n        phonetic_surname_col_name=\"surname_dm\",\n        levenshtein_thresholds=[2],\n        jaro_winkler_thresholds=[0.8],\n    )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be included in the Splink settings dictionary.</p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>def __init__(\n    self,\n    forename_col_name,\n    surname_col_name,\n    set_to_lowercase=False,\n    include_exact_match_level: bool = True,\n    include_columns_reversed: bool = True,\n    term_frequency_adjustments: bool = False,\n    tf_adjustment_col_forename_and_surname: str = None,\n    phonetic_forename_col_name: str = None,\n    phonetic_surname_col_name: str = None,\n    levenshtein_thresholds: int | list = [],\n    damerau_levenshtein_thresholds: int | list = [],\n    jaro_winkler_thresholds: float | list = [0.88],\n    jaro_thresholds: float | list = [],\n    jaccard_thresholds: float | list = [],\n    m_probability_exact_match_forename_surname: float = None,\n    m_probability_exact_match_phonetic_forename_surname: float = None,\n    m_probability_columns_reversed_forename_surname: float = None,\n    m_probability_exact_match_surname: float = None,\n    m_probability_exact_match_forename: float = None,\n    m_probability_or_probabilities_surname_lev: float | list = None,\n    m_probability_or_probabilities_surname_dl: float | list = None,\n    m_probability_or_probabilities_surname_jw: float | list = None,\n    m_probability_or_probabilities_surname_jar: float | list = None,\n    m_probability_or_probabilities_surname_jac: float | list = None,\n    m_probability_or_probabilities_forename_lev: float | list = None,\n    m_probability_or_probabilities_forename_dl: float | list = None,\n    m_probability_or_probabilities_forename_jw: float | list = None,\n    m_probability_or_probabilities_forename_jar: float | list = None,\n    m_probability_or_probabilities_forename_jac: float | list = None,\n    m_probability_else: float = None,\n) -&gt; Comparison:\n    \"\"\"A wrapper to generate a comparison for a name column the data in\n    `col_name` with preselected defaults.\n\n    The default arguments will give a comparison with comparison levels:\\n\n    - Exact match forename and surname\\n\n    - Macth of forename and surname reversed\\n\n    - Exact match surname\\n\n    - Exact match forename\\n\n    - Fuzzy match surname jaro-winkler &gt;= 0.88\\n\n    - Fuzzy match forename jaro-winkler&gt;=  0.88\\n\n    - Anything else\n\n    Args:\n        forename_col_name (str): The name of the forename column to compare\n        surname_col_name (str): The name of the surname column to compare\n        set_to_lowercase (bool): If True, all names are set to lowercase\n            during the pairwise comparisons.\n            Defaults to False\n        include_exact_match_level (bool, optional): If True, include an exact match\n            level for col_name. Defaults to True.\n        include_columns_reversed (bool, optional): If True, include a comparison\n            level for forename and surname being swapped. Defaults to True\n        term_frequency_adjustments (bool, optional): If True, apply term\n            frequency adjustments to the exact match level for forename_col_name\n            and surname_col_name.\n            Applies term frequency adjustments to full name exact match level\n            and columns reversed exact match level if\n            tf_adjustment_col_forename_and_surname is provided.\n            Applies term frequency adjustments to phonetic_forename_col_name\n            and phonetic_surname_col_name exact match levels, if they are provided.\n            Defaults to False.\n        tf_adjustment_col_forename_and_surname (str, optional): The name\n            of a combined forename surname column. This column is used to provide\n            term frequency adjustments for forename surname exact match and columns\n            reversed levels.\n            Defaults to None\n        set_to_lowercase (bool): If True, all postcodes are set to lowercase\n            during the pairwise comparisons.\n            Defaults to True\n        phonetic_forename_col_name (str, optional): The name of the column with\n            phonetic reduction (such as dmetaphone) of forename_col_name. Including\n            parameter along with phonetic_surname_col_name will create an exact\n            match level for \"Full name phonetic match\".\n            The phonetic column must be present in the dataset to use this\n            parameter.\n            Defaults to None\n        phonetic_surname_col_name (str, optional): The name of the column with\n            phonetic reduction (such as dmetaphone) of surname_col_name. Including\n            this parameter along with phonetic_forename_col_name will create an\n            exact match level for \"Full name phonetic match\".\n            The phonetic column must be present in\n            the dataset to use this parameter.\n            Defaults to None\n        levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for levenshtein similarity level(s) for surname_col_name\n            and forename_col_name.\n            Defaults to []\n        damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for damerau-levenshtein similarity level(s).\n            Defaults to []\n        jaro_winkler_thresholds (Union[int, list], optional): The thresholds\n            to use for jaro_winkler similarity level(s) for surname_col_name\n            and forename_col_name.\n            Defaults to [0.88]\n        jaro_thresholds (Union[int, list], optional): The thresholds\n            to use for jaro similarity level(s) for surname_col_name\n            and forename_col_name.\n            Defaults to []\n        jaccard_thresholds (Union[int, list], optional): The thresholds to\n            use for jaccard similarity level(s) for surname_col_name and\n            forename_col_name.\n            Defaults to []\n        m_probability_exact_match_forename_surname (_type_, optional): If provided,\n            overrides the default m probability for the exact match level for\n            forename and surname.\n            Defaults to None.\n        m_probability_exact_match_phonetic_forename_surname (_type_, optional): If\n            provided, overrides the default m probability for the phonetic match\n            level for forename and surname.\n            Defaults to None.\n        m_probability_columns_reversed_forename_surname (_type_, optional): If\n            provided, overrides the default m probability for the columns reversed\n            level for forename and surname.\n            Defaults to None.\n        m_probability_columns_reversed_forename_surname (_type_, optional): If\n            provided, overrides the default m probability for the columns reversed\n            level for forename and surname.\n            Defaults to None.\n        m_probability_exact_match_surname (_type_, optional): If provided,\n            overrides the default m probability for the surname exact match\n            level for forename and surname.\n            Defaults to None.\n        m_probability_exact_match_forename (_type_, optional): If provided,\n            overrides the default m probability for the forename exact match\n            level for forename and forename.\n            Defaults to None.\n        m_probability_phonetic_match_surname (_type_, optional): If provided,\n            overrides the default m probability for the surname phonetic match\n            level for forename and surname.\n            Defaults to None.\n        m_probability_phonetic_match_forename (_type_, optional): If provided,\n            overrides the default m probability for the forename phonetic match\n            level for forename and forename.\n            Defaults to None.\n        m_probability_or_probabilities_surname_lev (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_surname_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_surname_jw (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_surname_jar (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_surname_jac (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_forename_lev (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_forename_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_forename_jw (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_forename_jar (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_forename_jac (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_else (_type_, optional): If provided, overrides the\n            default m probability for the 'anything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Basic Forename Surname Comparison\n            ```py\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\"first_name\", \"surname)\n            ```\n\n            Bespoke Forename Surname Comparison\n            ```py\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\n                    \"forename\",\n                    \"surname\",\n                    term_frequency_adjustments=True,\n                    tf_adjustment_col_forename_and_surname=\"full_name\",\n                    phonetic_forename_col_name=\"forename_dm\",\n                    phonetic_surname_col_name=\"surname_dm\",\n                    levenshtein_thresholds=[2],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1],\n                )\n            ```\n        === \":simple-apachespark: Spark\"\n            Basic Forename Surname Comparison\n            ```py\n            import splink.spark.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\"first_name\", \"surname)\n            ```\n\n            Bespoke Forename Surname Comparison\n            ```py\n            import splink.spark.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\n                    \"forename\",\n                    \"surname\",\n                    term_frequency_adjustments=True,\n                    tf_adjustment_col_forename_and_surname=\"full_name\",\n                    phonetic_forename_col_name=\"forename_dm\",\n                    phonetic_surname_col_name=\"surname_dm\",\n                    levenshtein_thresholds=[2],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1],\n                )\n            ```\n        === \":simple-sqlite: SQLite\"\n            Basic Forename Surname Comparison\n            ```py\n            import splink.sqlite.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\"first_name\", \"surname)\n            ```\n\n            Bespoke Forename Surname Comparison\n            ```py\n            import splink.sqlite.comparison_template_library as ctl\n            ctl.forename_surname_comparison(\n                    \"forename\",\n                    \"surname\",\n                    term_frequency_adjustments=True,\n                    tf_adjustment_col_forename_and_surname=\"full_name\",\n                    phonetic_forename_col_name=\"forename_dm\",\n                    phonetic_surname_col_name=\"surname_dm\",\n                    levenshtein_thresholds=[2],\n                    jaro_winkler_thresholds=[0.8],\n                )\n            ```\n\n\n    Returns:\n        Comparison: A comparison that can be included in the Splink settings\n            dictionary.\n    \"\"\"\n\n    # Construct Comparison\n    comparison_levels = []\n\n    comparison_level = and_(\n        self._null_level(forename_col_name),\n        self._null_level(surname_col_name),\n        label_for_charts=\"Null\",\n    )\n\n    comparison_levels.append(comparison_level)\n\n    ### Forename surname exact match\n\n    if include_exact_match_level:\n        if set_to_lowercase:\n            forename_col_name_l = f\"lower({forename_col_name}_l)\"\n            forename_col_name_r = f\"lower({forename_col_name}_r)\"\n            surname_col_name_l = f\"lower({surname_col_name}_l)\"\n            surname_col_name_r = f\"lower({surname_col_name}_r)\"\n        else:\n            forename_col_name_l = f\"{forename_col_name}_l\"\n            forename_col_name_r = f\"{forename_col_name}_r\"\n            surname_col_name_l = f\"{surname_col_name}_l\"\n            surname_col_name_r = f\"{surname_col_name}_r\"\n\n        comparison_level = {\n            \"sql_condition\": f\"{forename_col_name_l} = {forename_col_name_r} \"\n            f\"AND {surname_col_name_l} = {surname_col_name_r}\",\n            \"tf_adjustment_column\": tf_adjustment_col_forename_and_surname,\n            \"tf_adjustment_weight\": 1.0,\n            \"m_probability\": m_probability_exact_match_forename_surname,\n            \"label_for_charts\": \"Full name exact match\",\n        }\n\n        comparison_levels.append(comparison_level)\n\n    ### Phonetic forename surname match\n\n    if phonetic_forename_col_name and phonetic_surname_col_name is not None:\n        comparison_level = {\n            \"sql_condition\": f\"{phonetic_forename_col_name}_l = \"\n            f\"{phonetic_forename_col_name}_r\"\n            f\" AND {phonetic_surname_col_name}_l = {phonetic_surname_col_name}_r\",\n            \"tf_adjustment_column\": tf_adjustment_col_forename_and_surname,\n            \"tf_adjustment_weight\": 1.0,\n            \"m_probability\": m_probability_exact_match_phonetic_forename_surname,\n            \"label_for_charts\": \"Full name phonetic match\",\n        }\n        comparison_levels.append(comparison_level)\n\n    ### Columns reversed match\n\n    if include_columns_reversed:\n        comparison_level = self._columns_reversed_level(\n            forename_col_name,\n            surname_col_name,\n            set_to_lowercase=set_to_lowercase,\n            tf_adjustment_column=tf_adjustment_col_forename_and_surname,\n            m_probability=m_probability_columns_reversed_forename_surname,\n        )\n        comparison_levels.append(comparison_level)\n\n    ### Surname Exact match\n\n    comparison_level = self._exact_match_level(\n        surname_col_name,\n        set_to_lowercase=set_to_lowercase,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability=m_probability_exact_match_surname,\n        include_colname_in_charts_label=True,\n    )\n    comparison_levels.append(comparison_level)\n\n    ### Forename Exact match\n\n    comparison_level = self._exact_match_level(\n        forename_col_name,\n        set_to_lowercase=set_to_lowercase,\n        term_frequency_adjustments=term_frequency_adjustments,\n        m_probability=m_probability_exact_match_forename,\n        include_colname_in_charts_label=True,\n    )\n    comparison_levels.append(comparison_level)\n\n    ### Ensure fuzzy match thresholds are iterable\n    levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n    damerau_levenshtein_thresholds = ensure_is_iterable(\n        damerau_levenshtein_thresholds\n    )\n    jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n    jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n    jaccard_thresholds = ensure_is_iterable(jaccard_thresholds)\n\n    ### Surname Fuzzy match\n    if len(levenshtein_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            surname_col_name,\n            distance_function_name=\"levenshtein\",\n            distance_threshold_or_thresholds=levenshtein_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_lev,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        levenshtein_thresholds = ensure_is_iterable(damerau_levenshtein_thresholds)\n        threshold_comparison_levels = distance_threshold_comparison_levels(\n            self,\n            surname_col_name,\n            distance_function_name=\"damerau-levenshtein\",\n            distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_dl,\n        )\n        comparison_levels = comparison_levels + threshold_comparison_levels\n\n    if len(jaro_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            surname_col_name,\n            distance_function_name=\"jaro-\",\n            distance_threshold_or_thresholds=jaro_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jar,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            surname_col_name,\n            distance_function_name=\"jaro-winkler\",\n            distance_threshold_or_thresholds=jaro_winkler_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jw,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaccard_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            surname_col_name,\n            distance_function_name=\"jaccard\",\n            distance_threshold_or_thresholds=jaccard_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_surname_jac,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    ### Forename Fuzzy match\n\n    if len(levenshtein_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            forename_col_name,\n            distance_function_name=\"levenshtein\",\n            distance_threshold_or_thresholds=levenshtein_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_lev,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            forename_col_name,\n            distance_function_name=\"damerau-levenshtein\",\n            distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_dl,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            forename_col_name,\n            distance_function_name=\"jaro-winkler\",\n            distance_threshold_or_thresholds=jaro_winkler_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jw,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaro_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            forename_col_name,\n            distance_function_name=\"jaro\",\n            distance_threshold_or_thresholds=jaro_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jar,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaccard_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            forename_col_name,\n            distance_function_name=\"jaccard\",\n            distance_threshold_or_thresholds=jaccard_thresholds,\n            set_to_lowercase=set_to_lowercase,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_forename_jac,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct Description\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    if phonetic_forename_col_name and phonetic_surname_col_name is not None:\n        comparison_desc += \"Phonetic match forename and surname vs. \"\n\n    if include_columns_reversed:\n        comparison_desc += \"Forename and surname columns reversed vs. \"\n\n    comparison_desc += \"Surname exact match vs. \"\n\n    comparison_desc += \"Forename exact match vs. \"\n\n    if len(levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"levenshtein\", levenshtein_thresholds\n        )\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n        )\n\n    if len(jaro_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"jaro\", jaro_thresholds\n        )\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"jaro-winkler\", jaro_winkler_thresholds\n        )\n\n    if len(jaccard_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"jaccard\", jaccard_thresholds\n        )\n\n    if len(levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            forename_col_name, \"levenshtein\", levenshtein_thresholds\n        )\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            surname_col_name, \"damerau-levenshtein\", damerau_levenshtein_thresholds\n        )\n\n    if len(jaro_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            forename_col_name, \"jaro\", jaro_thresholds\n        )\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            forename_col_name, \"jaro-winkler\", jaro_winkler_thresholds\n        )\n\n    if len(jaccard_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            forename_col_name, \"jaccard\", jaccard_thresholds\n        )\n\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.PostcodeComparisonBase.__init__","title":"<code>__init__(col_name, invalid_postcodes_as_null=False, set_to_lowercase=True, valid_postcode_regex='^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$', term_frequency_adjustments_full=False, include_full_match_level=True, include_sector_match_level=True, include_district_match_level=True, include_area_match_level=True, lat_col=None, long_col=None, km_thresholds=[], m_probability_full_match=None, m_probability_sector_match=None, m_probability_district_match=None, m_probability_area_match=None, m_probability_or_probabilities_km_distance=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a poscode column 'col_name'     with preselected defaults.</p> <p>The default arguments will give a comparison with levels:</p> <ul> <li> <p>Exact match on full postcode</p> </li> <li> <p>Exact match on sector</p> </li> <li> <p>Exact match on district</p> </li> <li> <p>Exact match on area</p> </li> <li> <p>All other comparisons</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>invalid_postcodes_as_null</code> <code>bool</code> <p>If True, postcodes that do not adhere to valid_postcode_regex will be included in the null level. Defaults to False</p> <code>False</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all postcodes are set to lowercase during the pairwise comparisons. Defaults to True</p> <code>True</code> <code>valid_postcode_regex</code> <code>str</code> <p>regular expression pattern that is used to validate postcodes. If invalid_postcodes_as_null is True, postcodes that do not adhere to valid_postcode_regex will be included  in the null level.  Defaults to \"^[A-Za-z]{1,2}0-9? 0-9$\"</p> <code>'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$'</code> <code>term_frequency_adjustments_full</code> <code>bool</code> <p>If True, apply term frequency adjustments to the full postcode exact match level. Defaults to False.</p> <code>False</code> <code>include_full_match_level</code> <code>bool</code> <p>If True, include an exact match on full postcode level. Defaults to True.</p> <code>True</code> <code>include_sector_match_level</code> <code>bool</code> <p>If True, include an exact match on sector level. Defaults to True.</p> <code>True</code> <code>include_district_match_level</code> <code>bool</code> <p>If True, include an exact match on district level. Defaults to True.</p> <code>True</code> <code>include_area_match_level</code> <code>bool</code> <p>If True, include an exact match on area level. Defaults to True.</p> <code>True</code> <code>include_distance_in_km_level</code> <code>bool</code> <p>If True, include a comparison of distance between postcodes as measured in kilometers. Defaults to False.</p> required <code>lat_col</code> <code>str</code> <p>The name of a latitude column or the respective array or struct column column containing the information, plus an index. For example: lat, long_lat['lat'] or long_lat[0].</p> <code>None</code> <code>long_col</code> <code>str</code> <p>The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long, long_lat['long'] or long_lat[1].</p> <code>None</code> <code>km_thresholds</code> <code>(int, float, list)</code> <p>The total distance in kilometers to evaluate the distance_in_km_level comparison against.</p> <code>[]</code> <code>m_probability_full_match</code> <code>float</code> <p>Starting m probability for full match level. Defaults to None.</p> <code>None</code> <code>m_probability_sector_match</code> <code>float</code> <p>Starting m probability for sector match level. Defaults to None.</p> <code>None</code> <code>m_probability_district_match</code> <code>float</code> <p>Starting m probability for district match level. Defaults to None.</p> <code>None</code> <code>m_probability_area_match</code> <code>float</code> <p>Starting m probability for area match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_km_distance</code> <code>float</code> <p>Starting m probability for 'distance in km' level. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>Starting m probability for the 'everything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark Athena <p>Basic Postcode Comparison </p><pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.duckdb.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre> <p>Basic Postcode Comparison </p><pre><code>import splink.spark.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.spark.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre> <p>Basic Postcode Comparison </p><pre><code>import splink.athena.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.athena.comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be inclued in the Splink settings dictionary.</p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    invalid_postcodes_as_null=False,\n    set_to_lowercase=True,\n    valid_postcode_regex=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\",\n    term_frequency_adjustments_full=False,\n    include_full_match_level=True,\n    include_sector_match_level=True,\n    include_district_match_level=True,\n    include_area_match_level=True,\n    lat_col: str = None,\n    long_col: str = None,\n    km_thresholds: int | float | list = [],\n    m_probability_full_match=None,\n    m_probability_sector_match=None,\n    m_probability_district_match=None,\n    m_probability_area_match=None,\n    m_probability_or_probabilities_km_distance=None,\n    m_probability_else=None,\n) -&gt; Comparison:\n    \"\"\"A wrapper to generate a comparison for a poscode column 'col_name'\n        with preselected defaults.\n\n    The default arguments will give a comparison with levels:\\n\n    - Exact match on full postcode\\n\n    - Exact match on sector\\n\n    - Exact match on district\\n\n    - Exact match on area\\n\n    - All other comparisons\n\n    Args:\n        col_name (str): The name of the column to compare.\n        invalid_postcodes_as_null (bool): If True, postcodes that do not adhere\n            to valid_postcode_regex will be included in the null level.\n            Defaults to False\n        set_to_lowercase (bool): If True, all postcodes are set to lowercase\n            during the pairwise comparisons.\n            Defaults to True\n        valid_postcode_regex (str): regular expression pattern that is used\n            to validate postcodes. If invalid_postcodes_as_null is True,\n            postcodes that do not adhere to valid_postcode_regex will be included\n             in the null level.\n             Defaults to \"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\"\n        term_frequency_adjustments_full (bool, optional): If True, apply\n            term frequency adjustments to the full postcode exact match level.\n            Defaults to False.\n        include_full_match_level (bool, optional): If True, include an exact\n            match on full postcode level. Defaults to True.\n        include_sector_match_level (bool, optional): If True, include an exact\n            match on sector level. Defaults to True.\n        include_district_match_level (bool, optional): If True, include an exact\n            match on district level. Defaults to True.\n        include_area_match_level (bool, optional): If True, include an exact\n            match on area level. Defaults to True.\n        include_distance_in_km_level (bool, optional): If True, include a\n            comparison of distance between postcodes as measured in kilometers.\n            Defaults to False.\n        lat_col (str): The name of a latitude column or the respective array\n            or struct column column containing the information, plus an index.\n            For example: lat, long_lat['lat'] or long_lat[0].\n        long_col (str): The name of a longitudinal column or the respective array\n            or struct column column containing the information, plus an index.\n            For example: long, long_lat['long'] or long_lat[1].\n        km_thresholds (int, float, list): The total distance in kilometers to\n            evaluate the distance_in_km_level comparison against.\n        m_probability_full_match (float, optional): Starting m\n            probability for full match level. Defaults to None.\n        m_probability_sector_match (float, optional): Starting m\n            probability for sector match level. Defaults to None.\n        m_probability_district_match (float, optional): Starting m\n            probability for district match level. Defaults to None.\n        m_probability_area_match (float, optional): Starting m\n            probability for area match level. Defaults to None.\n        m_probability_or_probabilities_km_distance (float, optional): Starting m\n            probability for 'distance in km' level. Defaults to None.\n        m_probability_else (float, optional): Starting m probability for\n            the 'everything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Basic Postcode Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\")\n            ```\n            Bespoke Postcode Comparison\n            ``` python\n            import splink.duckdb.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\",\n                                invalid_postcodes_as_null=True,\n                                include_distance_in_km_level=True,\n                                lat_col=\"lat\",\n                                long_col=\"long\",\n                                km_thresholds=[10, 100]\n                                )\n            ```\n        === \":simple-apachespark: Spark\"\n            Basic Postcode Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\")\n            ```\n            Bespoke Postcode Comparison\n            ``` python\n            import splink.spark.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\",\n                                invalid_postcodes_as_null=True,\n                                include_distance_in_km_level=True,\n                                lat_col=\"lat\",\n                                long_col=\"long\",\n                                km_thresholds=[10, 100]\n                                )\n            ```\n        === \":simple-amazonaws: Athena\"\n            Basic Postcode Comparison\n            ``` python\n            import splink.athena.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\")\n            ```\n            Bespoke Postcode Comparison\n            ``` python\n            import splink.athena.comparison_template_library as ctl\n            ctl.postcode_comparison(\"postcode\",\n                                invalid_postcodes_as_null=True,\n                                include_distance_in_km_level=True,\n                                lat_col=\"lat\",\n                                long_col=\"long\",\n                                km_thresholds=[10, 100]\n                                )\n            ```\n\n    Returns:\n        Comparison: A comparison that can be inclued in the Splink settings\n            dictionary.\n    \"\"\"\n\n    comparison_levels = []\n\n    if invalid_postcodes_as_null:\n        comparison_levels.append(self._null_level(col_name, valid_postcode_regex))\n    else:\n        comparison_levels.append(self._null_level(col_name))\n\n    if include_full_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=None,\n            term_frequency_adjustments=term_frequency_adjustments_full,\n            set_to_lowercase=set_to_lowercase,\n            m_probability=m_probability_full_match,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels.append(comparison_level)\n\n    if include_sector_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]\",\n            set_to_lowercase=set_to_lowercase,\n            m_probability=m_probability_sector_match,\n            manual_col_name_for_charts_label=\"Postcode Sector\",\n        )\n        comparison_levels.append(comparison_level)\n\n    if include_district_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=\"^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?\",\n            set_to_lowercase=set_to_lowercase,\n            m_probability=m_probability_district_match,\n            manual_col_name_for_charts_label=\"Postcode District\",\n        )\n        comparison_levels.append(comparison_level)\n\n    if include_area_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=\"^[A-Za-z]{1,2}\",\n            set_to_lowercase=set_to_lowercase,\n            m_probability=m_probability_area_match,\n            manual_col_name_for_charts_label=\"Postcode Area\",\n        )\n        comparison_levels.append(comparison_level)\n\n    km_thresholds = ensure_is_iterable(km_thresholds)\n    if len(km_thresholds) &gt; 0:\n        if m_probability_or_probabilities_km_distance is None:\n            m_probability_or_probabilities_km_distance = [None] * len(km_thresholds)\n        m_probability_or_probabilities_km_distance = ensure_is_iterable(\n            m_probability_or_probabilities_km_distance\n        )\n\n        for thres, m_prob in zip(\n            km_thresholds,\n            m_probability_or_probabilities_km_distance,\n        ):\n            comparison_level = self._distance_in_km_level(\n                lat_col,\n                long_col,\n                km_threshold=thres,\n                m_probability=m_prob,\n            )\n            comparison_levels.append(comparison_level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct Description\n    comparison_desc = \"\"\n    if include_full_match_level:\n        comparison_desc += \"Exact match on full postcode vs. \"\n\n    if include_sector_match_level:\n        comparison_desc += \"exact match on sector vs. \"\n\n    if include_district_match_level:\n        comparison_desc += \"exact match on district vs. \"\n\n    if include_area_match_level:\n        comparison_desc += \"exact match on area vs. \"\n\n    if len(km_thresholds) &gt; 0:\n        desc = distance_threshold_description(\n            col_name, \"km_distance\", km_thresholds\n        )\n        comparison_desc += desc\n\n    comparison_desc += \"all other comparisons\"\n\n    comparison_dict = {\n        \"output_column_name\": col_name,\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.EmailComparisonBase.__init__","title":"<code>__init__(col_name, invalid_emails_as_null=False, valid_email_regex='^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$', term_frequency_adjustments_full=False, include_exact_match_level=True, include_username_match_level=True, include_username_fuzzy_level=True, include_domain_match_level=False, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[], jaro_winkler_thresholds=[0.88], jaro_thresholds=[], m_probability_full_match=None, m_probability_username_match=None, m_probability_or_probabilities_username_lev=None, m_probability_or_probabilities_username_dl=None, m_probability_or_probabilities_username_jw=None, m_probability_or_probabilities_username_jar=None, m_probability_or_probabilities_email_lev=None, m_probability_or_probabilities_email_dl=None, m_probability_or_probabilities_email_jw=None, m_probability_or_probabilities_email_jar=None, m_probability_domain_match=None, m_probability_else=None)</code>","text":"<p>A wrapped to generate a comparison for an email colummn 'col_name' with preselected defaults.</p> <p>The default arguments will give a comparison with levels:</p> <ul> <li> <p>Exact match on email</p> </li> <li> <p>Exact match on username with different domain</p> </li> <li> <p>Fuzzy match on email user Jaro-Winkler</p> </li> <li> <p>Fuzzy match on username using Jaro-Winkler </p> </li> <li> <p>All other comparisons</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>invalid_email_as_null</code> <code>bool</code> <p>If True, emails that do not adhere to valid_email_regex will be included in the null level. Defaults to False</p> required <code>valid_email_regex</code> <code>str</code> <p>regular expression pattern that is used to validate emails. If invalid_emails_as_null is True, emails that do not adhere to valid_email_regex will be included  in the null level.  Defaults to \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\"</p> <code>'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$'</code> <code>term_frequency_adjustments_full</code> <code>bool</code> <p>If True, apply term frequency adjustments to the full email exact match level. Defaults to False.</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match on full email level. Defaults to True.</p> <code>True</code> <code>include_username_match_level</code> <code>bool</code> <p>If True, include an exact match on username only level. Defaults to True.</p> <code>True</code> <code>include_username_fuzzy_level</code> <code>bool</code> <p>If True, include a level for fuzzy match on username. Defaults to True.</p> <code>True</code> <code>include_domain_match_level</code> <code>bool</code> <p>If True, include an exact match on domain only level. Defaults to True.</p> <code>False</code> <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>jaro_winkler_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro_winkler similarity level(s). Defaults to [0.88]</p> <code>[0.88]</code> <code>jaro_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro similarity level(s). Defaults to []</p> <code>[]</code> <code>m_probability_full_match</code> <code>float</code> <p>Starting m probability for full match level. Defaults to None.</p> <code>None</code> <code>m_probability_username_match</code> <code>float</code> <p>Starting m probability for username only match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_username_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_username_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_username_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_username_jar</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_email_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_email_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_email_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_email_jar</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_domain_match</code> <code>float</code> <p>Starting m probability for domain only match level. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>Starting m probability for the 'everything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <p>Basic email Comparison </p><pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.email_comparison(\"email\")\n</code></pre> Bespoke email Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.email_comparison(\"email\",\n                    levenshtein_thresholds = [2],\n                    damerau_levenshtein_thresholds = [2],\n                    invalid_emails_as_null = True,\n                    include_username_match_level = True,\n                    include_domain_match_level = True,\n                    )\n</code></pre> <p>Basic email Comparison </p><pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.email_comparison(col_name = \"email\")\n</code></pre> Bespoke email Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.email_comparison(\"email\",\n                    levenshtein_thresholds = [2],\n                    damerau_levenshtein_thresholds = [2],\n                    invalid_emails_as_null = True,\n                    include_username_match_level = True,\n                    include_domain_match_level = True,\n                    )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be inclued in the Splink settings dictionary.</p> Source code in <code>splink/comparison_template_library.py</code> <pre><code>def __init__(\n    self,\n    col_name: str,\n    invalid_emails_as_null: bool = False,\n    valid_email_regex: str = \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\",\n    term_frequency_adjustments_full: bool = False,\n    include_exact_match_level: bool = True,\n    include_username_match_level: bool = True,\n    include_username_fuzzy_level: bool = True,\n    include_domain_match_level: bool = False,\n    levenshtein_thresholds: int | list = [],\n    damerau_levenshtein_thresholds: int | list = [],\n    jaro_winkler_thresholds: float | list = [0.88],\n    jaro_thresholds: float | list = [],\n    m_probability_full_match: bool = None,\n    m_probability_username_match: bool = None,\n    m_probability_or_probabilities_username_lev: float | list = None,\n    m_probability_or_probabilities_username_dl: float | list = None,\n    m_probability_or_probabilities_username_jw: float | list = None,\n    m_probability_or_probabilities_username_jar: float | list = None,\n    m_probability_or_probabilities_email_lev: float | list = None,\n    m_probability_or_probabilities_email_dl: float | list = None,\n    m_probability_or_probabilities_email_jw: float | list = None,\n    m_probability_or_probabilities_email_jar: float | list = None,\n    m_probability_domain_match: float | list = None,\n    m_probability_else: float | list = None,\n) -&gt; Comparison:\n    \"\"\"A wrapped to generate a comparison for an email colummn\n    'col_name' with preselected defaults.\n\n    The default arguments will give a comparison with levels:\\n\n    - Exact match on email\\n\n    - Exact match on username with different domain\\n\n    - Fuzzy match on email user Jaro-Winkler\\n\n    - Fuzzy match on username using Jaro-Winkler \\n\n    - All other comparisons\n\n    Args:\n        col_name (str): The name of the column to compare.\n        invalid_email_as_null (bool): If True, emails that do not adhere\n            to valid_email_regex will be included in the null level.\n            Defaults to False\n        valid_email_regex (str): regular expression pattern that is used\n            to validate emails. If invalid_emails_as_null is True,\n            emails that do not adhere to valid_email_regex will be included\n             in the null level.\n             Defaults to \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n        term_frequency_adjustments_full (bool, optional): If True, apply\n            term frequency adjustments to the full email exact match level.\n            Defaults to False.\n        include_exact_match_level (bool, optional): If True, include an exact\n            match on full email level. Defaults to True.\n        include_username_match_level (bool, optional): If True, include an exact\n            match on username only level. Defaults to True.\n        include_username_fuzzy_level (bool, optional): If True, include a level\n            for fuzzy match on username. Defaults to True.\n        include_domain_match_level (bool, optional): If True, include an exact\n            match on domain only level. Defaults to True.\n        levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for levenshtein similarity level(s).\n            Defaults to []\n        damerau_levenshtein_thresholds (Union[int, list], optional): The thresholds\n            to use for damerau-levenshtein similarity level(s).\n            Defaults to []\n        jaro_winkler_thresholds (Union[int, list], optional): The thresholds\n            to use for jaro_winkler similarity level(s).\n            Defaults to [0.88]\n        jaro_thresholds (Union[int, list], optional): The thresholds\n            to use for jaro similarity level(s).\n            Defaults to []\n        m_probability_full_match (float, optional): Starting m\n            probability for full match level. Defaults to None.\n        m_probability_username_match (float, optional): Starting m probability\n            for username only match level. Defaults to None.\n        m_probability_or_probabilities_username_lev (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_username_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_username_jw (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_username_jar (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_email_lev (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_email_dl (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_email_jw (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_or_probabilities_email_jar (Union[float, list], optional):\n            _description_. If provided, overrides the default m probabilities\n            for the thresholds specified. Defaults to None.\n        m_probability_domain_match (float, optional): Starting m probability\n            for domain only match level. Defaults to None.\n        m_probability_else (float, optional): Starting m probability for\n            the 'everything else' level. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Basic email Comparison\n            ``` python\n            import splink.duckdb.duckdb_comparison_template_library as ctl\n            ctl.email_comparison(\"email\")\n            ```\n            Bespoke email Comparison\n            ``` python\n            import splink.duckdb.duckdb_comparison_template_library as ctl\n            ctl.email_comparison(\"email\",\n                                levenshtein_thresholds = [2],\n                                damerau_levenshtein_thresholds = [2],\n                                invalid_emails_as_null = True,\n                                include_username_match_level = True,\n                                include_domain_match_level = True,\n                                )\n            ```\n        === \":simple-apachespark: Spark\"\n            Basic email Comparison\n            ``` python\n            import splink.spark.spark_comparison_template_library as ctl\n            ctl.email_comparison(col_name = \"email\")\n            ```\n            Bespoke email Comparison\n            ``` python\n            import splink.spark.spark_comparison_template_library as ctl\n            ctl.email_comparison(\"email\",\n                                levenshtein_thresholds = [2],\n                                damerau_levenshtein_thresholds = [2],\n                                invalid_emails_as_null = True,\n                                include_username_match_level = True,\n                                include_domain_match_level = True,\n                                )\n\n            ```\n\n    Returns:\n        Comparison: A comparison that can be inclued in the Splink settings\n            dictionary.\n    \"\"\"\n    # Contstruct comparrison\n\n    comparison_levels = []\n\n    # Decide whether invalid emails should be treated as null\n    if invalid_emails_as_null:\n        comparison_levels.append(\n            self._null_level(col_name, valid_string_pattern=valid_email_regex)\n        )\n    else:\n        comparison_levels.append(self._null_level(col_name))\n\n    # Exact match on full email\n\n    if include_exact_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=None,\n            term_frequency_adjustments=term_frequency_adjustments_full,\n            m_probability=m_probability_full_match,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels.append(comparison_level)\n\n    # Exact match on username with different domain\n\n    if include_username_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=\"^[^@]+\",\n            m_probability=m_probability_username_match,\n            include_colname_in_charts_label=True,\n            manual_col_name_for_charts_label=\"Username\",\n        )\n        comparison_levels.append(comparison_level)\n\n    # Ensure fuzzy match thresholds are iterable\n\n    damerau_levenshtein_thresholds = ensure_is_iterable(\n        damerau_levenshtein_thresholds\n    )\n    levenshtein_thresholds = ensure_is_iterable(levenshtein_thresholds)\n    jaro_winkler_thresholds = ensure_is_iterable(jaro_winkler_thresholds)\n    jaro_thresholds = ensure_is_iterable(jaro_thresholds)\n\n    # Fuzzy match on full email\n\n    if len(levenshtein_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"levenshtein\",\n            distance_threshold_or_thresholds=levenshtein_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_email_lev,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"damerau-levenshtein\",\n            distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_email_dl,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"jaro-winkler\",\n            distance_threshold_or_thresholds=jaro_winkler_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jw,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    if len(jaro_thresholds) &gt; 0:\n        threshold_levels = distance_threshold_comparison_levels(\n            self,\n            col_name,\n            distance_function_name=\"jaro\",\n            distance_threshold_or_thresholds=jaro_thresholds,\n            m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jar,\n            include_colname_in_charts_label=True,\n        )\n        comparison_levels = comparison_levels + threshold_levels\n\n    # Fuzzy match on username only\n    if include_username_fuzzy_level:\n        if len(levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                regex_extract=\"^[^@]+\",\n                distance_function_name=\"levenshtein\",\n                distance_threshold_or_thresholds=levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_username_lev,\n                include_colname_in_charts_label=True,\n                manual_col_name_for_charts_label=\"Username\",\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(damerau_levenshtein_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                regex_extract=\"^[^@]+\",\n                distance_function_name=\"damerau-levenshtein\",\n                distance_threshold_or_thresholds=damerau_levenshtein_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_username_dl,\n                include_colname_in_charts_label=True,\n                manual_col_name_for_charts_label=\"Username\",\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_winkler_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                regex_extract=\"^[^@]+\",\n                distance_function_name=\"jaro-winkler\",\n                distance_threshold_or_thresholds=jaro_winkler_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_username_jw,\n                include_colname_in_charts_label=True,\n                manual_col_name_for_charts_label=\"Username\",\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n        if len(jaro_thresholds) &gt; 0:\n            threshold_levels = distance_threshold_comparison_levels(\n                self,\n                col_name,\n                distance_function_name=\"jaro\",\n                distance_threshold_or_thresholds=jaro_thresholds,\n                m_probability_or_probabilities_thres=m_probability_or_probabilities_email_jar,\n                include_colname_in_charts_label=True,\n            )\n            comparison_levels = comparison_levels + threshold_levels\n\n    # Domain-only match\n\n    if include_domain_match_level:\n        comparison_level = self._exact_match_level(\n            col_name,\n            regex_extract=\"@([^@]+)$\",\n            m_probability=m_probability_domain_match,\n            manual_col_name_for_charts_label=\"Email Domain\",\n        )\n        comparison_levels.append(comparison_level)\n\n    comparison_levels.append(\n        self._else_level(m_probability=m_probability_else),\n    )\n\n    # Construct Description\n\n    comparison_desc = \"\"\n    if include_exact_match_level:\n        comparison_desc += \"Exact match vs. \"\n\n    if include_username_match_level:\n        comparison_desc += \"Exact username match different domain vs. \"\n\n    if len(levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            \"fuzzy email\", \"levenshtein\", jaro_winkler_thresholds\n        )\n        comparison_desc += distance_threshold_description(\n            \"fuzzy username\", \"levenshtein\", jaro_winkler_thresholds\n        )\n\n    if len(damerau_levenshtein_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            \"fuzzy email\", \"damerau_levenshtein\", jaro_winkler_thresholds\n        )\n        comparison_desc += distance_threshold_description(\n            \"fuzzy username\", \"levenshtein\", jaro_winkler_thresholds\n        )\n\n    if len(jaro_winkler_thresholds) &gt; 0:\n        comparison_desc += distance_threshold_description(\n            \"fuzzy email\", \"jaro_winkler\", jaro_winkler_thresholds\n        )\n        comparison_desc += distance_threshold_description(\n            \"fuzzy username\", \"jaro_winkler\", jaro_winkler_thresholds\n        )\n\n    if include_domain_match_level:\n        comparison_desc += \"Domain-only match vs.\"\n\n    comparison_desc += \"anything else\"\n\n    comparison_dict = {\n        \"comparison_description\": comparison_desc,\n        \"comparison_levels\": comparison_levels,\n    }\n    super().__init__(comparison_dict)\n</code></pre>","tags":["API","comparisons","Date Comparison"]},{"location":"datasets.html","title":"In-built datasets","text":"","tags":["API","Datasets","Examples"]},{"location":"datasets.html#in-built-datasets","title":"In-built datasets","text":"<p>Splink has some datasets available for use to help you get up and running, test ideas, or explore Splink features. To use, simply import <code>splink_datasets</code>: </p><pre><code>from splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n</code></pre> which you can then use to set up a linker: <pre><code>from splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\n\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(\n    df,\n    {\n        \"link_type\": \"dedupe_only\",\n        \"comparisons\": [cl.exact_match(\"first_name\"), cl.exact_match(\"surname\")],\n    },\n)\n</code></pre> Troubleshooting <p>If you get a <code>SSLCertVerificationError</code> when trying to use the inbuilt datasets, this can be fixed with the <code>ssl</code> package by running:</p> <p><code>ssl._create_default_https_context = ssl._create_unverified_context</code>.</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink_datasets","title":"<code>splink_datasets</code>","text":"<p>Each attribute of <code>splink_datasets</code> is a dataset available for use, which exists as a pandas <code>DataFrame</code>. These datasets are not packaged directly with Splink, but instead are downloaded only when they are requested. Once requested they are cached for future use. The cache can be cleared using <code>splink_dataset_utils</code>,  which also contains information on available datasets, and which have already been cached.</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#available-datasets","title":"Available datasets","text":"<p>The datasets available are listed below:</p> dataset name description rows unique entities link to source <code>fake_1000</code> Fake 1000 from splink demos.  Records are 250 simulated people, with different numbers of duplicates, labelled. 1,000 250 source <code>historical_50k</code> The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors. 50,000 5,156 source <code>febrl3</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL3 data set contains 5000 records (2000 originals and 3000 duplicates), with a maximum of 5 duplicates based on one original record. 5,000 2,000 source <code>febrl4a</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4a contains 5000 original records. 5,000 5,000 source <code>febrl4b</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4b contains 5000 duplicate records, one for each record in FEBRL4a. 5,000 5,000 source <code>transactions_origin</code> This data has been generated to resemble bank transactions leaving an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart arriving in 'transactions_destination'. Memo is sometimes truncated or missing. 45,326 45,326 source <code>transactions_destination</code> This data has been generated to resemble bank transactions arriving in an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart sent from 'transactions_origin'. There may be a delay between the source and destination account, and the amount may vary due to hidden fees and foreign exchange rates. Memo is sometimes truncated or missing. 45,326 45,326 source","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink_dataset_labels","title":"<code>splink_dataset_labels</code>","text":"<p>Some of the <code>splink_datasets</code> have corresponding clerical labels to help assess model performance. These are requested through the <code>splink_dataset_labels</code> module.</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#available-datasets_1","title":"Available datasets","text":"<p>The datasets available are listed below:</p> dataset name description rows unique entities link to source <code>fake_1000_labels</code> Clerical labels for fake_1000 3,176 NA source","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink_dataset_utils-api","title":"<code>splink_dataset_utils</code> API","text":"<p>In addition to <code>splink_datasets</code>, you can also import <code>splink_dataset_utils</code>,  which has a few functions to help managing <code>splink_datasets</code>. This can be useful if you have limited internet connection and want to see what is already cached, or if you need to clear cache items (e.g. if datasets were to be updated, or if space is an issue).</p> <p>For example: </p><pre><code>from splink.datasets import splink_dataset_utils\n\nsplink_dataset_utils.show_downloaded_data()\nsplink_dataset_utils.clear_cache(['fake_1000'])\n</code></pre>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink.datasets._SplinkDataUtils.list_downloaded_datasets","title":"<code>list_downloaded_datasets()</code>","text":"<p>Return a list of datasets that have already been pre-downloaded</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink.datasets._SplinkDataUtils.list_all_datasets","title":"<code>list_all_datasets()</code>","text":"<p>Return a list of all available datasets, regardless of whether or not they have already been pre-downloaded</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink.datasets._SplinkDataUtils.show_downloaded_data","title":"<code>show_downloaded_data()</code>","text":"<p>Print a list of datasets that have already been pre-downloaded</p>","tags":["API","Datasets","Examples"]},{"location":"datasets.html#splink.datasets._SplinkDataUtils.clear_downloaded_data","title":"<code>clear_downloaded_data(datasets=None)</code>","text":"<p>Delete any pre-downloaded data stored locally.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>list</code> <p>A list of dataset names (without any file suffix) to delete. If <code>None</code>, all datasets will be deleted. Default <code>None</code></p> <code>None</code>","tags":["API","Datasets","Examples"]},{"location":"documentation_index.html","title":"Introduction","text":""},{"location":"documentation_index.html#documentation","title":"Documentation","text":"<p>This section contains reference material for the modules and functions within Splink.</p>"},{"location":"documentation_index.html#api","title":"API","text":"<p>The documentation for the Splink API is broken up into the following categories:</p> <ul> <li> <p>Linker API - for all of the methods that can be used with an instantiated <code>Linker</code> object.</p> </li> <li> <p>Comparisons Library API - for methods that can be used to define <code>Comparisons</code> within a Splink Settings dictionary.</p> </li> <li> <p>Blocking Rules Library API - for methods that can be used to define a <code>Blocking Rule</code> for use within a Splink Settings dictionary or in the expectation maximisation step.</p> </li> <li> <p>EM Training Session API - for methods that be used to inspect the results of individual iterations of the Expectation Maximisation model training algorithm.</p> </li> <li> <p>Splink Dataframe API - for methods that can be used to manipulate a <code>SplinkDataFrame</code> across all SQL backends.</p> </li> <li> <p>Comparisons API - for reference material giving more explanation on the <code>Comparison</code> and <code>ComparisonLevel</code> objects within Splink.</p> </li> </ul> <p>Note: When building a Splink model, the Linker API and Comparisons Library API will be the most useful parts of the API documentation. The other sections are primarily for reference.</p>"},{"location":"documentation_index.html#charts-gallery","title":"Charts Gallery","text":"<p>The Splink Charts Gallery contains examples for all of the interactive charts and dashboards that are provided in Splink to help the linking process.</p>"},{"location":"documentation_index.html#in-built-datasets","title":"In-built datasets","text":"<p>Information on pre-made data tables available within Splink suitable for linking, to get up-and-running or to try out ideas.</p> <ul> <li>In-build datasets - information on included datasets, as well as how to use them, and methods for managing them.</li> </ul>"},{"location":"documentation_index.html#splink-settings","title":"Splink Settings","text":"<p>Reference materials for the Splink Settings dictionary:</p> <ul> <li> <p>Settings Dictionary Reference - for reference material on the parameters available within a Splink Settings dictionary.</p> </li> <li> <p>Interactive Settings Editor - for an interactive Settings editor, giving a hands-on sandbox to test out the Splink Settings dictionary. Including validation, autocomplete and autoformatting.</p> </li> </ul>"},{"location":"em_training_session.html","title":"EM Training Session API","text":"","tags":["API","Expectation Maximisation","Model Training"]},{"location":"em_training_session.html#documentation-for-emtrainingsession-object","title":"Documentation for <code>EMTrainingSession</code> object","text":"<p>Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates.  Plots diagnostic charts</p> Source code in <code>splink/em_training_session.py</code> <pre><code>class EMTrainingSession:\n    \"\"\"Manages training models using the Expectation Maximisation algorithm, and\n    holds statistics on the evolution of parameter estimates.  Plots diagnostic charts\n    \"\"\"\n\n    def __init__(\n        self,\n        linker: Linker,\n        blocking_rule_for_training: BlockingRule,\n        fix_u_probabilities: bool = False,\n        fix_m_probabilities: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        comparisons_to_deactivate: list[Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n    ):\n        logger.info(\"\\n----- Starting EM training session -----\\n\")\n\n        self._original_settings_obj = linker._settings_obj\n        self._original_linker = linker\n        self._training_linker = deepcopy(linker)\n\n        self._settings_obj = self._training_linker._settings_obj\n        self._settings_obj._retain_matching_columns = False\n        self._settings_obj._retain_intermediate_calculation_columns = False\n        self._settings_obj._training_mode = True\n\n        if not isinstance(blocking_rule_for_training, BlockingRule):\n            blocking_rule_for_training = BlockingRule(blocking_rule_for_training)\n\n        self._settings_obj._blocking_rule_for_training = blocking_rule_for_training\n        self._blocking_rule_for_training = blocking_rule_for_training\n        self._settings_obj._estimate_without_term_frequencies = (\n            estimate_without_term_frequencies\n        )\n\n        if comparison_levels_to_reverse_blocking_rule:\n            self._comparison_levels_to_reverse_blocking_rule = (\n                comparison_levels_to_reverse_blocking_rule\n            )\n        else:\n            self._comparison_levels_to_reverse_blocking_rule = self._original_settings_obj._get_comparison_levels_corresponding_to_training_blocking_rule(  # noqa\n                blocking_rule_for_training.blocking_rule_sql\n            )\n\n        self._settings_obj._probability_two_random_records_match = (\n            self._blocking_adjusted_probability_two_random_records_match\n        )\n\n        self._training_fix_u_probabilities = fix_u_probabilities\n        self._training_fix_m_probabilities = fix_m_probabilities\n        self._training_fix_probability_two_random_records_match = (\n            fix_probability_two_random_records_match\n        )\n\n        # Remove comparison columns which are either 'used up' by the blocking rules\n        # or alternatively, if the user has manually provided a list to remove,\n        # use this instead\n        if not comparisons_to_deactivate:\n            comparisons_to_deactivate = []\n            br_cols = get_columns_used_from_sql(\n                blocking_rule_for_training.blocking_rule_sql,\n                self._settings_obj._sql_dialect,\n            )\n            for cc in self._settings_obj.comparisons:\n                cc_cols = cc._input_columns_used_by_case_statement\n                cc_cols = [c.input_name for c in cc_cols]\n                if set(br_cols).intersection(cc_cols):\n                    comparisons_to_deactivate.append(cc)\n        cc_names_to_deactivate = [\n            cc._output_column_name for cc in comparisons_to_deactivate\n        ]\n        self._comparisons_that_cannot_be_estimated: list[\n            Comparison\n        ] = comparisons_to_deactivate\n\n        filtered_ccs = [\n            cc\n            for cc in self._settings_obj.comparisons\n            if cc._output_column_name not in cc_names_to_deactivate\n        ]\n\n        self._settings_obj.comparisons = filtered_ccs\n        self._comparisons_that_can_be_estimated = filtered_ccs\n\n        self._settings_obj_history = []\n\n        # Add iteration 0 i.e. the starting parameters\n        self._add_iteration()\n\n    def _training_log_message(self):\n        not_estimated = [\n            cc._output_column_name for cc in self._comparisons_that_cannot_be_estimated\n        ]\n        not_estimated = \"\".join([f\"\\n    - {cc}\" for cc in not_estimated])\n\n        estimated = [\n            cc._output_column_name for cc in self._comparisons_that_can_be_estimated\n        ]\n        estimated = \"\".join([f\"\\n    - {cc}\" for cc in estimated])\n\n        if self._training_fix_m_probabilities and self._training_fix_u_probabilities:\n            raise ValueError(\"Can't train model if you fix both m and u probabilites\")\n        elif self._training_fix_u_probabilities:\n            mu = \"m probabilities\"\n        elif self._training_fix_m_probabilities:\n            mu = \"u probabilities\"\n        else:\n            mu = \"m and u probabilities\"\n\n        blocking_rule = self._blocking_rule_for_training.blocking_rule_sql\n\n        logger.info(\n            f\"Estimating the {mu} of the model by blocking on:\\n\"\n            f\"{blocking_rule}\\n\\n\"\n            \"Parameter estimates will be made for the following comparison(s):\"\n            f\"{estimated}\\n\"\n            \"\\nParameter estimates cannot be made for the following comparison(s)\"\n            f\" since they are used in the blocking rules: {not_estimated}\"\n        )\n\n    def _comparison_vectors(self):\n        self._training_log_message()\n\n        nodes_with_tf = self._original_linker._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self._training_linker)\n        for sql in sqls:\n            self._training_linker._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        # repartition after blocking only exists on the SparkLinker\n        repartition_after_blocking = getattr(\n            self._original_linker, \"repartition_after_blocking\", False\n        )\n\n        if repartition_after_blocking:\n            df_blocked = self._training_linker._execute_sql_pipeline([nodes_with_tf])\n            input_dataframes = [nodes_with_tf, df_blocked]\n        else:\n            input_dataframes = [nodes_with_tf]\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._training_linker._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n        return self._training_linker._execute_sql_pipeline(input_dataframes)\n\n    def _train(self):\n        cvv = self._comparison_vectors()\n\n        # check that the blocking rule actually generates _some_ record pairs,\n        # if not give the user a helpful message\n        if not cvv.as_record_dict(limit=1):\n            br_sql = f\"`{self._blocking_rule_for_training.blocking_rule_sql}`\"\n            raise EMTrainingException(\n                f\"Training rule {br_sql} resulted in no record pairs.  \"\n                \"This means that in the supplied data set \"\n                f\"there were no pairs of records for which {br_sql} was `true`.\\n\"\n                \"Expectation maximisation requires a substantial number of record \"\n                \"comparisons to produce accurate parameter estimates - usually \"\n                \"at least a few hundred, but preferably at least a few thousand.\\n\"\n                \"You must revise your training blocking rule so that the set of \"\n                \"generated comparisons is not empty.  You can use \"\n                \"`linker.count_num_comparisons_from_blocking_rule()` to compute \"\n                \"the number of comparisons that will be generated by a blocking rule.\"\n            )\n\n        # Compute the new params, populating the paramters in the copied settings object\n        # At this stage, we do not overwrite any of the parameters\n        # in the original (main) setting object\n        expectation_maximisation(self, cvv)\n\n        rule = self._blocking_rule_for_training.blocking_rule_sql\n        training_desc = f\"EM, blocked on: {rule}\"\n\n        # Add m and u values to original settings\n        for cc in self._settings_obj.comparisons:\n            orig_cc = self._original_settings_obj._get_comparison_by_output_column_name(\n                cc._output_column_name\n            )\n            for cl in cc._comparison_levels_excluding_null:\n                orig_cl = orig_cc._get_comparison_level_by_comparison_vector_value(\n                    cl._comparison_vector_value\n                )\n\n                if not self._training_fix_m_probabilities:\n                    not_observed = LEVEL_NOT_OBSERVED_TEXT\n                    if cl._m_probability == not_observed:\n                        orig_cl._add_trained_m_probability(not_observed, training_desc)\n                        logger.info(\n                            f\"m probability not trained for {cc._output_column_name} - \"\n                            f\"{cl.label_for_charts} (comparison vector value: \"\n                            f\"{cl._comparison_vector_value}). This usually means the \"\n                            \"comparison level was never observed in the training data.\"\n                        )\n                    else:\n                        orig_cl._add_trained_m_probability(\n                            cl.m_probability, training_desc\n                        )\n\n                if not self._training_fix_u_probabilities:\n                    not_observed = LEVEL_NOT_OBSERVED_TEXT\n                    if cl._u_probability == not_observed:\n                        orig_cl._add_trained_u_probability(not_observed, training_desc)\n                        logger.info(\n                            f\"u probability not trained for {cc._output_column_name} - \"\n                            f\"{cl.label_for_charts} (comparison vector value: \"\n                            f\"{cl._comparison_vector_value}). This usually means the \"\n                            \"comparison level was never observed in the training data.\"\n                        )\n                    else:\n                        orig_cl._add_trained_u_probability(\n                            cl.u_probability, training_desc\n                        )\n\n        self._original_linker._em_training_sessions.append(self)\n\n    def _add_iteration(self):\n        self._settings_obj_history.append(deepcopy(self._settings_obj))\n\n    @property\n    def _blocking_adjusted_probability_two_random_records_match(self):\n        orig_prop_m = self._original_settings_obj._probability_two_random_records_match\n\n        adj_bayes_factor = prob_to_bayes_factor(orig_prop_m)\n\n        logger.log(15, f\"Original prob two random records match: {orig_prop_m:.3f}\")\n\n        comp_levels = self._comparison_levels_to_reverse_blocking_rule\n        if not comp_levels:\n            comp_levels = self._original_settings_obj._get_comparison_levels_corresponding_to_training_blocking_rule(  # noqa\n                self._blocking_rule_for_training.blocking_rule_sql\n            )\n\n        for cl in comp_levels:\n            adj_bayes_factor = cl._bayes_factor * adj_bayes_factor\n\n            logger.log(\n                15,\n                f\"Increasing prob two random records match using \"\n                f\"{cl.comparison._output_column_name} - {cl.label_for_charts}\"\n                f\" using bayes factor {cl._bayes_factor:,.3f}\",\n            )\n\n        adjusted_prop_m = bayes_factor_to_prob(adj_bayes_factor)\n        logger.log(\n            15,\n            f\"\\nProb two random records match adjusted for blocking on \"\n            f\"{self._blocking_rule_for_training.blocking_rule_sql}: \"\n            f\"{adjusted_prop_m:.3f}\",\n        )\n        return adjusted_prop_m\n\n    @property\n    def _iteration_history_records(self):\n        output_records = []\n\n        for iteration, settings_obj in enumerate(self._settings_obj_history):\n            records = settings_obj._parameters_as_detailed_records\n\n            for r in records:\n                r[\"iteration\"] = iteration\n                r[\n                    \"probability_two_random_records_match\"\n                ] = self._settings_obj._probability_two_random_records_match\n\n            output_records.extend(records)\n        return output_records\n\n    @property\n    def _lambda_history_records(self):\n        output_records = []\n        for i, s in enumerate(self._settings_obj_history):\n            lam = s._probability_two_random_records_match\n            r = {\n                \"probability_two_random_records_match\": lam,\n                \"probability_two_random_records_match_reciprocal\": 1 / lam,\n                \"iteration\": i,\n            }\n\n            output_records.append(r)\n        return output_records\n\n    def probability_two_random_records_match_iteration_chart(self):\n        records = self._lambda_history_records\n        return probability_two_random_records_match_iteration_chart(records)\n\n    def match_weights_interactive_history_chart(self):\n        records = self._iteration_history_records\n        return match_weights_interactive_history_chart(\n            records, blocking_rule=self._blocking_rule_for_training\n        )\n\n    def m_u_values_interactive_history_chart(self):\n        records = self._iteration_history_records\n        return m_u_parameters_interactive_history_chart(records)\n\n    def _max_change_message(self, max_change_dict):\n        message = \"Largest change in params was\"\n\n        if max_change_dict[\"max_change_type\"] == \"probability_two_random_records_match\":\n            message = (\n                f\"{message} {max_change_dict['max_change_value']:,.3g} in \"\n                \"probability_two_random_records_match\"\n            )\n        else:\n            cl = max_change_dict[\"current_comparison_level\"]\n            m_u = max_change_dict[\"max_change_type\"]\n            cc_name = cl.comparison._output_column_name\n\n            cl_label = cl.label_for_charts\n            level_text = f\"{cc_name}, level `{cl_label}`\"\n\n            message = (\n                f\"{message} {max_change_dict['max_change_value']:,.3g} in \"\n                f\"the {m_u} of {level_text}\"\n            )\n\n        return message\n\n    def _max_change_in_parameters_comparison_levels(self):\n        previous_iteration = self._settings_obj_history[-2]\n        this_iteration = self._settings_obj_history[-1]\n        max_change = -0.1\n\n        max_change_levels = {\n            \"previous_iteration\": None,\n            \"this_iteration\": None,\n            \"max_change_type\": None,\n            \"max_change_value\": None,\n        }\n        comparisons = zip(previous_iteration.comparisons, this_iteration.comparisons)\n        for comparison in comparisons:\n            prev_cc = comparison[0]\n            this_cc = comparison[1]\n            z_cls = zip(prev_cc.comparison_levels, this_cc.comparison_levels)\n            for z_cl in z_cls:\n                if z_cl[0].is_null_level:\n                    continue\n                prev_cl = z_cl[0]\n                this_cl = z_cl[1]\n                change_m = this_cl.m_probability - prev_cl.m_probability\n                change_u = this_cl.u_probability - prev_cl.u_probability\n                change = max(abs(change_m), abs(change_u))\n                change_type = (\n                    \"m_probability\"\n                    if abs(change_m) &gt; abs(change_u)\n                    else \"u_probability\"\n                )\n                change_value = change_m if abs(change_m) &gt; abs(change_u) else change_u\n                if change &gt; max_change:\n                    max_change = change\n                    max_change_levels[\"prev_comparison_level\"] = prev_cl\n                    max_change_levels[\"current_comparison_level\"] = this_cl\n                    max_change_levels[\"max_change_type\"] = change_type\n                    max_change_levels[\"max_change_value\"] = change_value\n                    max_change_levels[\"max_abs_change_value\"] = abs(change_value)\n\n        change_probability_two_random_records_match = (\n            this_iteration._probability_two_random_records_match\n            - previous_iteration._probability_two_random_records_match\n        )\n\n        if abs(change_probability_two_random_records_match) &gt; max_change:\n            max_change = abs(change_probability_two_random_records_match)\n            max_change_levels[\"prev_comparison_level\"] = None\n            max_change_levels[\"current_comparison_level\"] = None\n            max_change_levels[\n                \"max_change_type\"\n            ] = \"probability_two_random_records_match\"\n            max_change_levels[\n                \"max_change_value\"\n            ] = change_probability_two_random_records_match\n            max_change_levels[\"max_abs_change_value\"] = abs(\n                change_probability_two_random_records_match\n            )\n\n        max_change_levels[\"message\"] = self._max_change_message(max_change_levels)\n\n        return max_change_levels\n\n    def __repr__(self):\n        deactivated_cols = \", \".join(\n            [\n                cc._output_column_name\n                for cc in self._comparisons_that_cannot_be_estimated\n            ]\n        )\n        blocking_rule = self._blocking_rule_for_training.blocking_rule_sql\n        return (\n            f\"&lt;EMTrainingSession, blocking on {blocking_rule}, \"\n            f\"deactivating comparisons {deactivated_cols}&gt;\"\n        )\n</code></pre>","tags":["API","Expectation Maximisation","Model Training"]},{"location":"getting_started.html","title":"Getting Started","text":""},{"location":"getting_started.html#getting-started","title":"Getting Started","text":""},{"location":"getting_started.html#install","title":"Install","text":"<p>Splink supports python 3.8+.</p> <p>To obtain the latest released version of splink you can install from PyPI using pip: </p><pre><code>pip install splink\n</code></pre> <p>or if you prefer, you can instead install splink using conda: </p><pre><code>conda install -c conda-forge splink\n</code></pre> Backend Specific Installs DuckDB-less Installation"},{"location":"getting_started.html#backend-specific-installs","title":"Backend Specific Installs","text":"<p>From Splink v3.9.7, packages required by specific splink backends can be optionally installed by adding the <code>[&lt;backend&gt;]</code> suffix to the end of your pip install.</p> <p>Note that SQLite and DuckDB come packaged with Splink and do not need to be optionally installed.</p> <p>The following backends are supported:</p>  Spark Athena PostgreSql <pre><code>pip install 'splink[spark]'\n</code></pre> <pre><code>pip install 'splink[athena]'\n</code></pre> <pre><code>pip install 'splink[postgres]'\n</code></pre>"},{"location":"getting_started.html#duckdb-less-installation","title":"DuckDB-less Installation","text":"<p>Should you be unable to install <code>DuckDB</code> to your local machine, you can still run <code>Splink</code> without the <code>DuckDB</code> dependency using a small workaround.</p> <p>To start, install the latest released version of splink from PyPI without any dependencies using: </p><pre><code>pip install splink --no-deps\n</code></pre> <p>Then, to install the remaining requirements, download the following <code>requirements.txt</code> from our github repository using: </p><pre><code>github_url=\"https://raw.githubusercontent.com/moj-analytical-services/splink/master/scripts/duckdbless_requirements.txt\"\noutput_file=\"splink_requirements.txt\"\n\n# Download the file from GitHub using curl\ncurl -o \"$output_file\" \"$github_url\"\n</code></pre> <p>Or, if you're either unable to download it directly from github or you'd rather create the file manually, simply:</p> <ol> <li>Create a file called <code>splink_requirements.txt</code></li> <li>Copy and paste the contents from our duckdbless requirements file into your file.</li> </ol> <p>Finally, run the following command within your virtual environment to install the remaining splink dependencies: </p><pre><code>pip install -r splink_requirements.txt\n</code></pre>"},{"location":"getting_started.html#quickstart","title":"Quickstart","text":"<p>To get a basic Splink model up and running, use the following code. It demonstrates how to:</p> <ol> <li>Estimate the parameters of a deduplication model</li> <li>Use the parameter estimates to identify duplicate records</li> <li>Use clustering to generate an estimated unique person ID.</li> </ol> <p>For more detailed tutorial, please see section below.</p> Simple Splink Model Example <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training, estimate_without_term_frequencies=True)\n\nblocking_rule_for_training = block_on(\"substr(dob, 1, 4)\")  # block on year\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training, estimate_without_term_frequencies=True)\n\n\npairwise_predictions = linker.predict()\n\nclusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.95)\nclusters.as_pandas_dataframe(limit=5)\n</code></pre>"},{"location":"getting_started.html#tutorials","title":"Tutorials","text":"<p>You can learn more about Splink in the step-by-step tutorial.</p>"},{"location":"getting_started.html#videos","title":"Videos","text":""},{"location":"getting_started.html#example-notebooks","title":"Example Notebooks","text":"<p>You can see end-to-end example of several use cases in the example notebooks, or by clicking the following Binder link:</p> <p></p>"},{"location":"getting_started.html#charts-gallery","title":"Charts Gallery","text":"<p>You can see all of the interactive charts provided in Splink by checking out the Charts Gallery.</p>"},{"location":"linker.html","title":"Full API","text":"","tags":["API"]},{"location":"linker.html#documentation-for-linker-object","title":"Documentation for <code>Linker</code> object","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API"]},{"location":"linkerbloc.html","title":"Blocking","text":"","tags":["API","Blocking"]},{"location":"linkerbloc.html#documentation-for-linker-object-methods-related-to-blocking","title":"Documentation for <code>Linker</code> object methods related to blocking","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerbloc.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Blocking"]},{"location":"linkerest.html","title":"Estimating model parameters","text":"","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#documentation-for-linker-object-methods-related-to-parameter-estimation","title":"Documentation for <code>Linker</code> object methods related to parameter estimation","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkereval.html","title":"Evaluation","text":"","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#documentation-for-linker-object-methods-related-to-evaluation","title":"Documentation for <code>Linker</code> object methods related to Evaluation","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkereval.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Evaluation","Labels"]},{"location":"linkerexp.html","title":"Exploratory analysis","text":"","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#documentation-for-linker-object-methods-related-to-exploratory-analysis","title":"Documentation for <code>Linker</code> object methods related to exploratory analysis","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkermodelviz.html","title":"Visualisation","text":"","tags":["API","Clusters"]},{"location":"linkermodelviz.html#documentation-for-linker-object-methods-related-to-modelprediction-visualisation","title":"Documentation for <code>Linker</code> object methods related to Model/Prediction Visualisation","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Clusters"]},{"location":"linkermodelviz.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Clusters"]},{"location":"linkerpred.html","title":"Predicting results","text":"","tags":["API","Prediction"]},{"location":"linkerpred.html#documentation-for-linker-object-methods-related-to-link-prediction","title":"Documentation for <code>Linker</code> object methods related to link prediction","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n    \"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n        validate_settings: bool = True,\n    ):\n        \"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n        self._db_schema = \"splink\"\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    def _input_columns(\n        self,\n        include_unique_id_col_names=True,\n        include_additional_columns_to_retain=True,\n    ) -&gt; list[InputColumn]:\n        \"\"\"Retrieve the column names from the input dataset(s) as InputColumns\n\n        Args:\n            include_unique_id_col_names (bool, optional): Whether to include unique ID\n                column names. Defaults to True.\n            include_additional_columns_to_retain (bool, optional): Whether to include\n                additional columns to retain. Defaults to True.\n\n        Raises:\n            SplinkException: If the input frames have different sets of columns.\n\n        Returns:\n            list[InputColumn]\n        \"\"\"\n\n        input_dfs = self._input_tables_dict.values()\n\n        # get a list of the column names for each input frame\n        # sort it for consistent ordering, and give each frame's\n        # columns as a tuple so we can hash it\n        column_names_by_input_df = [\n            tuple(sorted([col.name for col in input_df.columns]))\n            for input_df in input_dfs\n        ]\n        # check that the set of input columns is the same for each frame,\n        # fail if the sets are different\n        if len(set(column_names_by_input_df)) &gt; 1:\n            common_cols = set.intersection(\n                *(set(col_names) for col_names in column_names_by_input_df)\n            )\n            problem_names = {\n                col\n                for frame_col_names in column_names_by_input_df\n                for col in frame_col_names\n                if col not in common_cols\n            }\n            raise SplinkException(\n                \"All linker input frames must have the same set of columns.  \"\n                \"The following columns were not found in all input frames: \"\n                + \", \".join(problem_names)\n            )\n\n        columns = next(iter(input_dfs)).columns\n\n        remove_columns = []\n        if not include_unique_id_col_names:\n            remove_columns.extend(self._settings_obj._unique_id_input_columns)\n        if not include_additional_columns_to_retain:\n            remove_columns.extend(self._settings_obj._additional_columns_to_retain)\n\n        remove_id_cols = [c.unquote().name for c in remove_columns]\n        columns = [col for col in columns if col.unquote().name not in remove_id_cols]\n\n        return columns\n\n    @property\n    def _source_dataset_column_already_exists(self):\n        if self._settings_obj_ is None:\n            return False\n        input_cols = [c.unquote().name for c in self._input_columns()]\n        return self._settings_obj._source_dataset_column_name in input_cols\n\n    @property\n    def _cache_uid(self):\n        if getattr(self, \"_settings_dict\", None):\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if getattr(self, \"_settings_dict\", None):\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_left\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            if self._two_dataset_link_only:\n                return \"__splink__df_concat_with_tf_sample_right\"\n            else:\n                return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    def _random_sample_sql(\n        self, proportion, sample_size, seed=None, table=None, unique_id=None\n    ):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict, validate_settings: bool = True):\n        # Always sets a default cache uid -&gt; _cache_uid_no_settings\n        self._cache_uid = ascii_uid(8)\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n            return\n\n        if not isinstance(settings_dict, (str, dict)):\n            raise ValueError(\n                \"Invalid settings object supplied. Ensure this is either \"\n                \"None, a dictionary or a filepath to a settings object saved \"\n                \"as a json file.\"\n            )\n\n        self.load_settings(settings_dict, validate_settings)\n\n    def _check_for_valid_settings(self):\n        if (\n            # no settings to check\n            self._settings_obj_ is None\n            or\n            # raw tables don't yet exist in db\n            not hasattr(self, \"_input_tables_dict\")\n        ):\n            return False\n        else:\n            return True\n\n    def _validate_settings(self, validate_settings):\n        # Vaidate our settings after plugging them through\n        # `Settings(&lt;settings&gt;)`\n        if not self._check_for_valid_settings():\n            return\n\n        self._validate_input_dfs()\n\n        # Run miscellaneous checks on our settings dictionary.\n        _validate_dialect(\n            settings_dialect=self._settings_obj._sql_dialect,\n            linker_dialect=self._sql_dialect,\n            linker_type=self.__class__.__name__,\n        )\n\n        # Constructs output logs for our various settings inputs\n        cleaned_settings = SettingsColumnCleaner(\n            settings_object=self._settings_obj,\n            input_columns=self._input_tables_dict,\n        )\n        InvalidColumnsLogger(cleaned_settings).construct_output_logs(validate_settings)\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat\")\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache.get_with_logging(\"__splink__df_concat_with_tf\")\n\n        else:\n            # In duckdb, calls to random() in a CTE pipeline cause problems:\n            # https://gist.github.com/RobinL/d329e7004998503ce91b68479aa41139\n            if self._settings_obj.salting_required:\n                materialise = True\n\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n        \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                start_time = time.time()\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")  # noqa: T201\n                print(  # noqa: T201\n                    f\"--------Creating table: {output_tablename}--------\"\n                )\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    use_cache=False,\n                )\n                run_time = parse_duration(time.time() - start_time)\n                print(f\"Step ran in: {run_time}\")  # noqa: T201\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}`({physical_name}):\\n{final_sql}\"\n                f\"\\n\\nError was: {e}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n        \"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n        \"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database_and_remove_from_cache()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            # Certain tables are put in the cache using their templated_name\n            # An example is __splink__df_concat_with_tf\n            # These tables are put in the cache when they are first calculated\n            # e.g. with _initialise_df_concat_with_tf()\n            # But they can also be put in the cache manually using\n            # e.g. register_table_input_nodes_concat_with_tf()\n\n            # Look for these 'named' tables in the cache prior\n            # to looking for the hashed version\n\n            if output_tablename_templated in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(\n                    output_tablename_templated\n                )\n\n            if table_name_hash in self._intermediate_table_cache:\n                return self._intermediate_table_cache.get_with_logging(table_name_hash)\n\n            # If not in cache, fall back on checking the database\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Found cache for {output_tablename_templated} \"\n                    f\"in database using table name with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)  # noqa: T201\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)  # noqa: T201\n\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n            self._intermediate_table_cache.executed_queries.append(splink_dataframe)\n\n        splink_dataframe.created_by_splink = True\n        splink_dataframe.sql_used_to_create = sql\n\n        physical_name = splink_dataframe.physical_name\n\n        self._intermediate_table_cache[physical_name] = splink_dataframe\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n        \"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule_sql}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def delete_tables_created_by_splink_from_db(self):\n        for splink_df in list(self._intermediate_table_cache.values()):\n            if splink_df.created_by_splink:\n                splink_df.drop_table_from_database_and_remove_from_cache()\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(\n        self,\n        settings_dict: dict | str | Path,\n        validate_settings: str = True,\n    ):\n        \"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict, validate_settings=True)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n            validate_settings (bool, optional): When True, check your settings\n                dictionary for any potential errors that may cause splink to fail.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_uid = self._cache_uid\n\n        # Invalidate the cache if anything currently exists. If the settings are\n        # changing, our charts, tf tables, etc may need changing.\n        self.invalidate_cache()\n\n        self._settings_dict = settings_dict  # overwrite or add\n\n        # Get the SQL dialect from settings_dict or use the default\n        sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n        settings_dict[\"sql_dialect\"] = sql_dialect\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n        # Check the user's comparisons (if they exist)\n        log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n        self._settings_obj_ = Settings(settings_dict)\n        # Check the final settings object\n        self._validate_settings(validate_settings)\n\n    def load_model(self, model_path: Path):\n        \"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n        \"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n        \"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \":simple-apachespark: Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            tf_col.unquote().name\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache.get_with_logging(tf_tablename)\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                from splink.duckdb.linker import DuckDBLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = DuckDBLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                from splink.spark.linker import SparkLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SparkLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                from splink.athena.linker import AthenaLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = AthenaLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                from splink.sqlite.linker import SQLiteLinker\n\n                settings = {\n                    \"link_type\": \"dedupe_only\",\n                    \"blocking_rules_to_generate_predictions\": [\n                        \"l.first_name = r.first_name\",\n                        \"l.surname = r.surname\",\n                    ],\n                    \"comparisons\": []\n                }\n                &gt;&gt;&gt;\n                linker = SQLiteLinker(df, settings)\n                df = linker.deterministic_link()\n                ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n        return deterministic_link_df\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n        \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                SplinkDeprecated,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n        \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        estimate_without_term_frequencies: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n        \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (BlockingRule | str): The blocking rule used to generate\n                pairwise record comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            estimate_without_term_frequencies (bool, optional): If True, the iterations\n                of the EM algorithm ignore any term frequency adjustments and only\n                depend on the comparison vectors. This allows the EM algorithm to run\n                much faster, but the estimation of the parameters will change slightly.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n            or using pre-built rules\n            ```py\n            from splink.duckdb.blocking_rule_library import block_on\n            blocking_rule = block_on([\"first_name\", \"surname\"])\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        # Extract the blocking rule\n        # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n        # and raise error if not specfically a BlockingRule\n        blocking_rule = blocking_rule_to_obj(blocking_rule)\n        if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n            raise TypeError(\n                \"EM blocking rules must be plain blocking rules, not \"\n                \"salted or exploding blocking rules\"\n            )\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                (\n                    self._settings_obj._get_comparison_by_output_column_name(n)\n                    if isinstance(n, str)\n                    else n\n                )\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n            estimate_without_term_frequencies=estimate_without_term_frequencies,\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        # If exploded blocking rules exist, we need to materialise\n        # the tables of ID pairs\n        exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n\n        [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the Splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        blocking_rules = ensure_is_list(blocking_rules)\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n            self.register_table(\n                records_or_tablename, new_records_tablename, overwrite=True\n            )\n\n        else:\n            new_records_tablename = records_or_tablename\n\n        new_records_df = self._table_to_splink_dataframe(\n            \"__splink__df_new_records\", new_records_tablename\n        )\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, derive the term frequency\n        # tables from df_concat_with_tf rather than computing them\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n        for n, br in enumerate(blocking_rules):\n            br.add_preceding_rules(blocking_rules[:n])\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n        add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n        \"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n        \"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\", sqlglot_dialect=self._sql_dialect)\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sqls = block_using_rules_sqls(self)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` at or above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability at or\n                above this threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n        cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n        return cc\n\n    def _compute_metrics_nodes(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing node-level metrics.\n\n        Accepts outputs of `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of node metrics.\n\n        Node metrics produced:\n        * node_degree (absolute number of neighbouring nodes)\n\n        Output table has a single row per input node, along with the cluster id (as\n        assigned in `linker.cluster_pairwise_at_threshold()`) and the metric\n        node_degree:\n        |-------------------------------------------------|\n        | composite_unique_id | cluster_id  | node_degree |\n        |---------------------|-------------|-------------|\n        | s1-__-10001         | s1-__-10001 | 6           |\n        | s1-__-10002         | s1-__-10001 | 4           |\n        | s1-__-10003         | s1-__-10003 | 2           |\n        ...\n        \"\"\"\n        uid_cols = self._settings_obj._unique_id_input_columns\n        # need composite unique ids\n        composite_uid_edges_l = _composite_unique_id_from_edges_sql(uid_cols, \"l\")\n        composite_uid_edges_r = _composite_unique_id_from_edges_sql(uid_cols, \"r\")\n        composite_uid_clusters = _composite_unique_id_from_nodes_sql(uid_cols)\n\n        sqls = _node_degree_sql(\n            df_predict,\n            df_clustered,\n            composite_uid_edges_l,\n            composite_uid_edges_r,\n            composite_uid_clusters,\n            threshold_match_probability,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_node_metrics = self._execute_sql_pipeline()\n\n        df_node_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_node_metrics\n\n    def _compute_metrics_edges(\n        self,\n        df_node_metrics: SplinkDataFrame,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        threshold_match_probability: float,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing edge-level metrics.\n\n        Accepts outputs of `linker._compute_node_metrics()`, `linker.predict()` and\n        `linker.cluster_pairwise_at_threshold()`, along with the clustering threshold\n        and produces a table of edge metrics.\n\n        Uses `igraph` under-the-hood for calculations\n\n        Edge metrics produced:\n        * is_bridge (is the edge a bridge?)\n\n        Output table has a single row per edge, and the metric is_bridge:\n        |-------------------------------------------------------------|\n        | composite_unique_id_l | composite_unique_id_r   | is_bridge |\n        |-----------------------|-------------------------|-----------|\n        | s1-__-10001           | s1-__-10003             | True      |\n        | s1-__-10001           | s1-__-10005             | False     |\n        | s1-__-10005           | s1-__-10009             | False     |\n        | s1-__-10021           | s1-__-10024             | True      |\n        ...\n        \"\"\"\n        df_edge_metrics = compute_edge_metrics(\n            self, df_node_metrics, df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = threshold_match_probability\n        return df_edge_metrics\n\n    def _compute_metrics_clusters(\n        self,\n        df_node_metrics: SplinkDataFrame,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"\n        Internal function for computing cluster-level metrics.\n\n        Accepts output of `linker._compute_node_metrics()` (which has the relevant\n        information from `linker.predict() and\n        `linker.cluster_pairwise_at_threshold()`), produces a table of cluster metrics.\n\n        Cluster metrics produced:\n        * n_nodes (aka cluster size, number of nodes in cluster)\n        * n_edges (number of edges in cluster)\n        * density (number of edges normalised wrt maximum possible number)\n        * cluster_centralisation (average absolute deviation from maximum node_degree\n            normalised wrt maximum possible value)\n\n        Output table has a single row per cluster, along with the cluster metrics\n        listed above\n        |--------------------------------------------------------------------|\n        | cluster_id  | n_nodes | n_edges | density | cluster_centralisation |\n        |-------------|---------|---------|---------|------------------------|\n        | s1-__-10006 | 4       | 4       | 0.66667 | 0.6666                 |\n        | s1-__-10008 | 6       | 5       | 0.33333 | 0.4                    |\n        | s1-__-10013 | 11      | 19      | 0.34545 | 0.3111                 |\n        ...\n        \"\"\"\n\n        sqls = _size_density_centralisation_sql(\n            df_node_metrics,\n        )\n\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df_cluster_metrics = self._execute_sql_pipeline()\n        df_cluster_metrics.metadata[\n            \"threshold_match_probability\"\n        ] = df_node_metrics.metadata[\"threshold_match_probability\"]\n        return df_cluster_metrics\n\n    def compute_graph_metrics(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        *,\n        threshold_match_probability: float = None,\n    ) -&gt; GraphMetricsResults:\n        \"\"\"\n        Generates tables containing graph metrics (for nodes, edges and clusters),\n        and returns a data class of Splink dataframes\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            threshold_match_probability (float, optional): Filter the pairwise match\n                predictions to include only pairwise comparisons with a\n                match_probability at or above this threshold. If not provided, the value\n                will be taken from metadata on `df_clustered`. If no such metadata is\n                available, this value _must_ be provided.\n\n        Returns:\n            GraphMetricsResult: A data class containing SplinkDataFrames\n            of cluster IDs and selected node, edge or cluster metrics.\n                attribute \"nodes\" for nodes metrics table\n                attribute \"edges\" for edge metrics table\n                attribute \"clusters\" for cluster metrics table\n\n        \"\"\"\n        if threshold_match_probability is None:\n            threshold_match_probability = df_clustered.metadata.get(\n                \"threshold_match_probability\", None\n            )\n            # we may not have metadata if clusters have been manually registered, or\n            # read in from a format that does not include it\n            if threshold_match_probability is None:\n                raise TypeError(\n                    \"As `df_clustered` has no threshold metadata associated to it, \"\n                    \"to compute graph metrics you must provide \"\n                    \"`threshold_match_probability` manually\"\n                )\n        df_node_metrics = self._compute_metrics_nodes(\n            df_predict, df_clustered, threshold_match_probability\n        )\n        df_edge_metrics = self._compute_metrics_edges(\n            df_node_metrics,\n            df_predict,\n            df_clustered,\n            threshold_match_probability,\n        )\n        # don't need edges as information is baked into node metrics\n        df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n        return GraphMetricsResults(\n            nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n        )\n\n    def profile_columns(\n        self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n    ):\n        \"\"\"\n        Profiles the specified columns of the dataframe initiated with the linker.\n\n        This can be computationally expensive if the dataframe is large.\n\n        For the provided columns with column_expressions (or for all columns if\n         left empty) calculate:\n        - A distribution plot that shows the count of values at each percentile.\n        - A top n chart, that produces a chart showing the count of the top n values\n        within the column\n        - A bottom n chart, that produces a chart showing the count of the bottom\n        n values within the column\n\n        This should be used to explore the dataframe, determine if columns have\n        sufficient completeness for linking, analyse the cardinality of columns, and\n        identify the need for standardisation within a given column.\n\n        Args:\n            linker (object): The initiated linker.\n            column_expressions (list, optional): A list of strings containing the\n                specified column names.\n                If left empty this will default to all columns.\n            top_n (int, optional): The number of top n values to plot.\n            bottom_n (int, optional): The number of bottom n values to plot.\n\n        Returns:\n            altair.Chart or dict: A visualization or JSON specification describing the\n            profiling charts.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                linker = DuckDBLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                linker = SparkLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-amazonaws: Athena\"\n                ```py\n                linker = AthenaLinker(df)\n                linker.profile_columns()\n                ```\n            === \":simple-sqlite: SQLite\"\n                ```py\n                linker = SQLiteLinker(df)\n                linker.profile_columns()\n                ```\n\n        Note:\n            - The `linker` object should be an instance of the initiated linker.\n            - The provided `column_expressions` can be a list of column names to\n                profile. If left empty, all columns will be profiled.\n            - The `top_n` and `bottom_n` parameters determine the number of top and\n                 bottom values to display in the respective charts.\n        \"\"\"\n\n        return profile_columns(\n            self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n        )\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n        \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            === \":simple-duckdb: DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n            === \":simple-apachespark: Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.confusion_matrix_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n        \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def accuracy_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        add_metrics: list = [],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            add_metrics (list(str), optional): Precision and recall metrics are always\n                included. Where provided, `add_metrics` specifies additional metrics\n                to show, with the following options:\n\n                - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n                - `\"npv\"`: negative predictive value (NPV)\n                - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n                - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n                (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n                - `\"p4\"` -  an extended F1 score with specificity and NPV included\n                - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n        Examples:\n            ```py\n            linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n        if not isinstance(add_metrics, list):\n            raise Exception(\n                \"add_metrics must be a list containing one or more of the following:\",\n                allowed,\n            )\n\n        # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n        add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return accuracy_chart(recs, add_metrics=add_metrics)\n\n    def confusion_matrix_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n        match_weight_range=[-15, 15],\n    ):\n        \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the chart. Defaults to None.\n            match_weight_range (list(float), optional): minimum and maximum thresholds\n                to include in chart output. Defaults to [-15,15].\n        Examples:\n            ```py\n            linker.confusion_matrix_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n        recs = df_truth_space.as_record_dict()\n        a, b = match_weight_range\n        recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n        return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n        \"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n        \"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(\n        self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n    ):\n        \"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n            remove_sensitive_data (bool, optional): When True, The waterfall chart will\n                contain match weights only, and all of the (potentially sensitive) data\n                from the input tables will be removed prior to the chart being created.\n\n\n        Returns:\n            altair.Chart: An altair chart\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(\n            records, self._settings_obj, filter_nulls, remove_sensitive_data\n        )\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n        \"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            from splink.datasets import splink_datasets\n            df = splink_datasets.fake_1000\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n        \"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n        \"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to False.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n        \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, missingness will be computed for\n                this table alone.\n                Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n        \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str | BlockingRule,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str | BlockingRule): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.surname = r.surname\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n            Alternatively, you can use the blocking rule library functions\n            ```py\n            import splink.duckdb.blocking_rule_library as brl\n            br = brl.exact_match_rule(\"surname\")\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 3167\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n            self, blocking_rule\n        )\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def _count_num_comparisons_from_blocking_rule_pre_filter_conditions(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n        \"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule, prior to any filters (non equi-join conditions) being applied\n        by the SQL engine.\n\n        For more information on what this means, see\n        https://github.com/moj-analytical-services/splink/discussions/1391\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        input_dataframes = []\n        df_concat = self._initialise_df_concat()\n\n        if df_concat:\n            input_dataframes.append(df_concat)\n\n        sqls = count_comparisons_from_blocking_rule_pre_filter_conditions_sqls(\n            self, blocking_rule\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        res = self._execute_sql_pipeline(input_dataframes).as_record_dict()[0]\n        return int(res[\"count_of_pairwise_comparisons_generated\"])\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            Generate total comparisons from Blocking Rules defined in settings\n            dictionary\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            ```\n\n            Generate total comparisons with custom blocking rules.\n            ```py\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str | BlockingRule | list = None,\n    ):\n        \"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n        \"\"\"Counts the marginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_model(\"settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n        \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n        \"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n        \"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.to_dict(), \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            altair.Chart: An altair chart\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n        _df_cluster_metrics: SplinkDataFrame = None,\n    ):\n        \"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random`, `by_cluster_size` or\n                `lowest_density_clusters`. Defaults to `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n            _df_cluster_metrics=_df_cluster_metrics,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n        \"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n        \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n        \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n\n        # Nothing to delete\n        if len(self._intermediate_table_cache) == 0:\n            return\n\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # Drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self.delete_tables_created_by_splink_from_db()\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n        \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        splink_dataframe.templated_name = \"__splink__df_predict\"\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        splink_dataframe.templated_name = table_name_templated\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        splink_dataframe.templated_name = \"__splink__df_labels\"\n        return splink_dataframe\n\n    def labelling_tool_for_specific_record(\n        self,\n        unique_id,\n        source_dataset=None,\n        out_path=\"labelling_tool.html\",\n        overwrite=False,\n        match_weight_threshold=-4,\n        view_in_jupyter=False,\n        show_splink_predictions_in_interface=True,\n    ):\n        \"\"\"Create a standalone, offline labelling dashboard for a specific record\n        as identified by its unique id\n\n        Args:\n            unique_id (str): The unique id of the record for which to create the\n                labelling tool\n            source_dataset (str, optional): If there are multiple datasets, to\n                identify the record you must also specify the source_dataset. Defaults\n                to None.\n            out_path (str, optional): The output path for the labelling tool. Defaults\n                to \"labelling_tool.html\".\n            overwrite (bool, optional): If true, overwrite files at the output\n                path if they exist. Defaults to False.\n            match_weight_threshold (int, optional): Include possible matches in the\n                output which score above this threshold. Defaults to -4.\n            view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n                html viewer, set this to True to extract your labels. Defaults to False.\n            show_splink_predictions_in_interface (bool, optional): Whether to\n                show information about the Splink model's predictions that could\n                potentially bias the decision of the clerical labeller. Defaults to\n                True.\n        \"\"\"\n\n        df_comparisons = generate_labelling_tool_comparisons(\n            self,\n            unique_id,\n            source_dataset,\n            match_weight_threshold=match_weight_threshold,\n        )\n\n        render_labelling_tool_html(\n            self,\n            df_comparisons,\n            show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n            out_path=out_path,\n            view_in_jupyter=view_in_jupyter,\n            overwrite=overwrite,\n        )\n\n    def _remove_splinkdataframe_from_cache(self, splink_dataframe: SplinkDataFrame):\n        keys_to_delete = set()\n        for key, df in self._intermediate_table_cache.items():\n            if df.physical_name == splink_dataframe.physical_name:\n                keys_to_delete.add(key)\n\n        for k in keys_to_delete:\n            del self._intermediate_table_cache[k]\n\n    def _find_blocking_rules_below_threshold(\n        self, max_comparisons_per_rule, blocking_expressions=None\n    ):\n        return find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n    def _detect_blocking_rules_for_prediction(\n        self,\n        max_comparisons_per_rule,\n        blocking_expressions=None,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=10,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for prediction below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            blocking_expressions: By default, blocking rules will be equi-joins\n                on the columns used by the Splink model.  This allows you to manually\n                specify sql expressions from which combinations will be created. For\n                example, if you specify [\"substr(dob, 1,4)\", \"surname\", \"dob\"]\n                blocking rules will be chosen by blocking on combinations\n                of those expressions.\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them. The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, assign recommendation to settings.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule, blocking_expressions\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n            else:\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                self._settings_obj._blocking_rules_to_generate_predictions = suggestion\n\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_for_prediction\"\n                ].iloc[0]\n                msg = (\n                    \"The following blocking_rules_to_generate_predictions were \"\n                    \"automatically detected and assigned to your settings:\\n\"\n                )\n                logger.info(f\"{msg}{suggestion_str}\")\n\n    def _detect_blocking_rules_for_em_training(\n        self,\n        max_comparisons_per_rule,\n        min_freedom=1,\n        num_runs=200,\n        num_equi_join_weight=0,\n        field_freedom_weight=1,\n        num_brs_weight=20,\n        num_comparison_weight=10,\n        return_as_df=False,\n    ):\n        \"\"\"Find blocking rules for EM training below some given threshold of the\n        maximum number of comparisons that can be generated per blocking rule\n        (max_comparisons_per_rule).\n        Uses a heuristic cost algorithm to identify the 'best' set of blocking rules\n        Args:\n            max_comparisons_per_rule (int): The maximum number of comparisons that\n                each blocking rule is allowed to generate\n            min_freedom (int, optional): The minimum amount of freedom any column should\n                be allowed.\n            num_runs (int, optional): Each run selects rows using a heuristic and costs\n                them.  The more runs, the more likely you are to find the best rule.\n                Defaults to 5.\n            num_equi_join_weight (int, optional): Weight allocated to number of equi\n                joins in the blocking rules.\n                Defaults to 0 since this is cost better captured by other criteria.\n                Defaults to 0 since this is cost better captured by other criteria.\n            field_freedom_weight (int, optional): Weight given to the cost of\n                having individual fields which don't havem much flexibility.  Assigning\n                a high weight here makes it more likely you'll generate combinations of\n                blocking rules for which most fields are allowed to vary more than\n                the minimum. Defaults to 1.\n            num_brs_weight (int, optional): Weight assigned to the cost of\n                additional blocking rules.  Higher weight here will result in a\n                 preference for fewer blocking rules. Defaults to 10.\n            num_comparison_weight (int, optional): Weight assigned to the cost of\n                larger numbers of comparisons, which happens when more of the blocking\n                rules are close to the max_comparisons_per_rule.  A higher\n                 weight here prefers sets of rules which generate lower total\n                comparisons. Defaults to 10.\n            return_as_df (bool, optional): If false, return just the recommendation.\n                If true, return a dataframe containing details of the weights.\n                Defaults to False.\n        \"\"\"\n\n        df_br_below_thres = find_blocking_rules_below_threshold_comparison_count(\n            self, max_comparisons_per_rule\n        )\n\n        blocking_rule_suggestions = suggest_blocking_rules(\n            df_br_below_thres,\n            min_freedom=min_freedom,\n            num_runs=num_runs,\n            num_equi_join_weight=num_equi_join_weight,\n            field_freedom_weight=field_freedom_weight,\n            num_brs_weight=num_brs_weight,\n            num_comparison_weight=num_comparison_weight,\n        )\n\n        if return_as_df:\n            return blocking_rule_suggestions\n        else:\n            if blocking_rule_suggestions is None or len(blocking_rule_suggestions) == 0:\n                logger.warning(\"No set of blocking rules found within constraints\")\n                return None\n            else:\n                suggestion_str = blocking_rule_suggestions[\n                    \"suggested_EM_training_statements\"\n                ].iloc[0]\n                msg = \"The following EM training strategy was detected:\\n\"\n                msg = f\"{msg}{suggestion_str}\"\n                logger.info(msg)\n                suggestion = blocking_rule_suggestions[\n                    \"suggested_blocking_rules_as_splink_brs\"\n                ].iloc[0]\n                return suggestion\n\n    def _explode_arrays_sql(\n        self, tbl_name, columns_to_explode, other_columns_to_retain\n    ):\n        raise NotImplementedError(\n            f\"Unnesting blocking rules are not supported for {type(self)}\"\n        )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker.  This method implements ensures linker can be deepcopied.</p> Source code in <code>splink/linker.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"When we do EM training, we need a copy of the linker which is independent\n    of the main linker e.g. setting parameters on the copy will not affect the\n    main linker.  This method implements ensures linker can be deepcopied.\n    \"\"\"\n    new_linker = copy(self)\n    new_linker._em_training_sessions = []\n    new_settings = deepcopy(self._settings_obj_)\n    new_linker._settings_obj_ = new_settings\n    return new_linker\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None, validate_settings=True)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p>  DuckDB Spark <p>Dedupe </p><pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre> <p>Dedupe </p><pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n    validate_settings: bool = True,\n):\n    \"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n    self._db_schema = \"splink\"\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._setup_settings_objs(deepcopy(settings_dict), validate_settings)\n\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.accuracy_chart_from_labels_column","title":"<code>accuracy_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        ```py\n        linker.accuracy_chart_from_labels_column(\"ground_truth\", add_metrics=[\"f1\"])\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, add_metrics=[])</code>","text":"<p>Generate an accuracy measure chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>add_metrics</code> <code>list(str)</code> <p>Precision and recall metrics are always included. Where provided, <code>add_metrics</code> specifies additional metrics to show, with the following options:</p> <ul> <li><code>\"specificity\"</code>: specificity, selectivity, true negative rate (TNR)</li> <li><code>\"npv\"</code>: negative predictive value (NPV)</li> <li><code>\"accuracy\"</code>: overall accuracy (TP+TN)/(P+N)</li> <li><code>\"f1\"</code>/<code>\"f2\"</code>/<code>\"f0_5\"</code>: F-scores for \u03b2=1 (balanced), \u03b2=2 (emphasis on recall) and \u03b2=0.5 (emphasis on precision)</li> <li><code>\"p4\"</code> -  an extended F1 score with specificity and NPV included</li> <li><code>\"phi\"</code> - \u03c6 coefficient or Matthews correlation coefficient (MCC)</li> </ul> <code>[]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def accuracy_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    add_metrics: list = [],\n):\n    \"\"\"Generate an accuracy measure chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        add_metrics (list(str), optional): Precision and recall metrics are always\n            included. Where provided, `add_metrics` specifies additional metrics\n            to show, with the following options:\n\n            - `\"specificity\"`: specificity, selectivity, true negative rate (TNR)\n            - `\"npv\"`: negative predictive value (NPV)\n            - `\"accuracy\"`: overall accuracy (TP+TN)/(P+N)\n            - `\"f1\"`/`\"f2\"`/`\"f0_5\"`: F-scores for \\u03B2=1 (balanced), \\u03B2=2\n            (emphasis on recall) and \\u03B2=0.5 (emphasis on precision)\n            - `\"p4\"` -  an extended F1 score with specificity and NPV included\n            - `\"phi\"` - \\u03C6 coefficient or Matthews correlation coefficient (MCC)\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=[\"f1\"])\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.accuracy_chart_from_labels_table(\"labels\", add_metrics=['f1'])\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    allowed = [\"specificity\", \"npv\", \"accuracy\", \"f1\", \"f2\", \"f0_5\", \"p4\", \"phi\"]\n\n    if not isinstance(add_metrics, list):\n        raise Exception(\n            \"add_metrics must be a list containing one or more of the following:\",\n            allowed,\n        )\n\n    # Silently filter out invalid entries (except case errors - e.g. [\"NPV\", \"F1\"])\n    add_metrics = list(set(map(str.lower, add_metrics)).intersection(allowed))\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return accuracy_chart(recs, add_metrics=add_metrics)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> at or above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` at or above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability at or\n            above this threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n    cc.metadata[\"threshold_match_probability\"] = threshold_match_probability\n\n    return cc\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False, _df_cluster_metrics=None)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code>, <code>by_cluster_size</code> or <code>lowest_density_clusters</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p></p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n    _df_cluster_metrics: SplinkDataFrame = None,\n):\n    \"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random`, `by_cluster_size` or\n            `lowest_density_clusters`. Defaults to `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n        _df_cluster_metrics=_df_cluster_metrics,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n    \"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser </p><pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n    \"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.completeness_chart","title":"<code>completeness_chart(input_dataset=None, cols=None)</code>","text":"<p>Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, completeness will be computed for this table alone. Defaults to None.</p> <code>None</code> <code>cols</code> <code>List[str]</code> <p>List of column names to calculate completeness. Default to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.completeness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.completeness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n    \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n    columns in each of the input datasets. By default, completeness is assessed for\n    all column in the input data.\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, completeness will be computed for this table\n            alone. Defaults to None.\n        cols (List[str], optional): List of column names to calculate completeness.\n            Default to None.\n\n    Examples:\n        ```py\n        linker.completeness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.completeness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = completeness_data(self, input_dataset, cols)\n    return completeness_chart(records)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.compute_graph_metrics","title":"<code>compute_graph_metrics(df_predict, df_clustered, *, threshold_match_probability=None)</code>","text":"<p>Generates tables containing graph metrics (for nodes, edges and clusters), and returns a data class of Splink dataframes</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability at or above this threshold. If not provided, the value will be taken from metadata on <code>df_clustered</code>. If no such metadata is available, this value must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GraphMetricsResult</code> <code>GraphMetricsResults</code> <p>A data class containing SplinkDataFrames</p> <code>GraphMetricsResults</code> <p>of cluster IDs and selected node, edge or cluster metrics. attribute \"nodes\" for nodes metrics table attribute \"edges\" for edge metrics table attribute \"clusters\" for cluster metrics table</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_graph_metrics(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    *,\n    threshold_match_probability: float = None,\n) -&gt; GraphMetricsResults:\n    \"\"\"\n    Generates tables containing graph metrics (for nodes, edges and clusters),\n    and returns a data class of Splink dataframes\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        threshold_match_probability (float, optional): Filter the pairwise match\n            predictions to include only pairwise comparisons with a\n            match_probability at or above this threshold. If not provided, the value\n            will be taken from metadata on `df_clustered`. If no such metadata is\n            available, this value _must_ be provided.\n\n    Returns:\n        GraphMetricsResult: A data class containing SplinkDataFrames\n        of cluster IDs and selected node, edge or cluster metrics.\n            attribute \"nodes\" for nodes metrics table\n            attribute \"edges\" for edge metrics table\n            attribute \"clusters\" for cluster metrics table\n\n    \"\"\"\n    if threshold_match_probability is None:\n        threshold_match_probability = df_clustered.metadata.get(\n            \"threshold_match_probability\", None\n        )\n        # we may not have metadata if clusters have been manually registered, or\n        # read in from a format that does not include it\n        if threshold_match_probability is None:\n            raise TypeError(\n                \"As `df_clustered` has no threshold metadata associated to it, \"\n                \"to compute graph metrics you must provide \"\n                \"`threshold_match_probability` manually\"\n            )\n    df_node_metrics = self._compute_metrics_nodes(\n        df_predict, df_clustered, threshold_match_probability\n    )\n    df_edge_metrics = self._compute_metrics_edges(\n        df_node_metrics,\n        df_predict,\n        df_clustered,\n        threshold_match_probability,\n    )\n    # don't need edges as information is baked into node metrics\n    df_cluster_metrics = self._compute_metrics_clusters(df_node_metrics)\n\n    return GraphMetricsResults(\n        nodes=df_node_metrics, edges=df_edge_metrics, clusters=df_cluster_metrics\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p>  DuckDB Spark <p>Real time linkage </p><pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Real time linkage </p><pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n    \"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \":simple-apachespark: Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        tf_col.unquote().name\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache.get_with_logging(tf_tablename)\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline([cache[\"__splink__df_concat_with_tf\"]])\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.confusion_matrix_from_labels_column","title":"<code>confusion_matrix_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an accuracy chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an accuracy chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        ```py\n        linker.confusion_matrix_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None, match_weight_range=[-15, 15])</code>","text":"<p>Generate an interactive confusion matrix from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the chart. Defaults to None.</p> <code>None</code> <code>match_weight_range</code> <code>list(float)</code> <p>minimum and maximum thresholds to include in chart output. Defaults to [-15,15].</p> <code>[-15, 15]</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def confusion_matrix_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n    match_weight_range=[-15, 15],\n):\n    \"\"\"Generate an interactive confusion matrix from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the chart. Defaults to None.\n        match_weight_range (list(float), optional): minimum and maximum thresholds\n            to include in chart output. Defaults to [-15,15].\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.confusion_matrix_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n\n    recs = df_truth_space.as_record_dict()\n    a, b = match_weight_range\n    recs = [r for r in recs if a &lt; r[\"truth_threshold\"] &lt; b]\n    return confusion_matrix_chart(recs, match_weight_range=match_weight_range)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str | BlockingRule</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.surname = r.surname\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387</p> <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>394 Alternatively, you can use the blocking rule library functions </p><pre><code>import splink.duckdb.blocking_rule_library as brl\nbr = brl.exact_match_rule(\"surname\")\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 3167  <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str | BlockingRule,\n) -&gt; int:\n    \"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str | BlockingRule): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.surname = r.surname\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n        Alternatively, you can use the blocking rule library functions\n        ```py\n        import splink.duckdb.blocking_rule_library as brl\n        br = brl.exact_match_rule(\"surname\")\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 3167\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    blocking_rule = blocking_rule_to_obj(blocking_rule).blocking_rule_sql\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_post_filters_sql(\n        self, blocking_rule\n    )\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the marginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n    \"\"\"Counts the marginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_model(\"settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <p>Generate total comparisons from Blocking Rules defined in settings dictionary </p><pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>Generate total comparisons with custom blocking rules. </p><pre><code>blocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        Generate total comparisons from Blocking Rules defined in settings\n        dictionary\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        ```\n\n        Generate total comparisons with custom blocking rules.\n        ```py\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str | BlockingRule | list = None,\n):\n    \"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n    \"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            from splink.duckdb.linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            from splink.spark.linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            from splink.athena.linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            from splink.sqlite.linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    deterministic_link_df = self._execute_sql_pipeline([concat_with_tf])\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n    return deterministic_link_df\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n    \"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n    \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n    labels.\n\n    The table of labels should be in the following format, and should\n    be registered with your database:\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n    |----------------|-----------|----------------|-----------|\n    |df_1            |1          |df_2            |2          |\n    |df_1            |1          |df_2            |3          |\n\n    Note that `source_dataset` and `unique_id` should correspond to the\n    values specified in the settings dict, and the `input_table_aliases`\n    passed to the `linker` object. Note that at the moment, this method does\n    not respect values in a `clerical_match_score` column.  If provided, these\n    are ignored and it is assumed that every row in the table of labels is a score\n    of 1, i.e. a perfect match.\n\n    Args:\n      labels_splinkdataframe_or_table_name (str): Name of table containing labels\n        in the database or SplinkDataframe\n\n    Examples:\n        ```py\n        pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n        linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n        linker.estimate_m_from_pairwise_labels(\"labels\")\n        ```\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    estimate_m_from_pairwise_labels(self, labels_tablename)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, estimate_without_term_frequencies=False, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour </p><pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>BlockingRule | str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>estimate_without_term_frequencies</code> <code>bool</code> <p>If True, the iterations of the EM algorithm ignore any term frequency adjustments and only depend on the comparison vectors. This allows the EM algorithm to run much faster, but the estimation of the parameters will change slightly.</p> <code>False</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p></p><pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> or using pre-built rules <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    estimate_without_term_frequencies: bool = False,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n    \"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (BlockingRule | str): The blocking rule used to generate\n            pairwise record comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        estimate_without_term_frequencies (bool, optional): If True, the iterations\n            of the EM algorithm ignore any term frequency adjustments and only\n            depend on the comparison vectors. This allows the EM algorithm to run\n            much faster, but the estimation of the parameters will change slightly.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n        or using pre-built rules\n        ```py\n        from splink.duckdb.blocking_rule_library import block_on\n        blocking_rule = block_on([\"first_name\", \"surname\"])\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    # Extract the blocking rule\n    # Check it's a BlockingRule (not a SaltedBlockingRule, ExlpodingBlockingRule)\n    # and raise error if not specfically a BlockingRule\n    blocking_rule = blocking_rule_to_obj(blocking_rule)\n    if type(blocking_rule) not in (BlockingRule, SaltedBlockingRule):\n        raise TypeError(\n            \"EM blocking rules must be plain blocking rules, not \"\n            \"salted or exploding blocking rules\"\n        )\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            (\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n            )\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        estimate_without_term_frequencies=estimate_without_term_frequencies,\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n    \"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n    \"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            SplinkDeprecated,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the Splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n    \"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the Splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    blocking_rules = ensure_is_list(blocking_rules)\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        self.register_table(\n            records_or_tablename, new_records_tablename, overwrite=True\n        )\n\n    else:\n        new_records_tablename = records_or_tablename\n\n    new_records_df = self._table_to_splink_dataframe(\n        \"__splink__df_new_records\", new_records_tablename\n    )\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, derive the term frequency\n    # tables from df_concat_with_tf rather than computing them\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    blocking_rules = [blocking_rule_to_obj(br) for br in blocking_rules]\n    for n, br in enumerate(blocking_rules):\n        br.add_preceding_rules(blocking_rules[:n])\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf_before_uid_fix\")\n\n    add_unique_id_and_source_dataset_cols_if_needed(self, new_records_df)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. Examples:     === \" DuckDB\"         </p><pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Spark\"         <pre><code>linker = SparkLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" Athena\"         <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre>     === \" SQLite\"         <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> Args:     settings_dict (dict): A Splink settings dictionary             Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.invalidate_cache","title":"<code>invalidate_cache()</code>","text":"<p>Invalidate the Splink cache.  Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed.</p> Source code in <code>splink/linker.py</code> <pre><code>def invalidate_cache(self):\n    \"\"\"Invalidate the Splink cache.  Any previously-computed tables\n    will be recomputed.\n    This is useful, for example, if the input data tables have changed.\n    \"\"\"\n\n    # Nothing to delete\n    if len(self._intermediate_table_cache) == 0:\n        return\n\n    # Before Splink executes a SQL command, it checks the cache to see\n    # whether a table already exists with the name of the output table\n\n    # This function has the effect of changing the names of the output tables\n    # to include a different unique id\n\n    # As a result, any previously cached tables will not be found\n    self._cache_uid = ascii_uid(8)\n\n    # Drop any existing splink tables from the database\n    # Note, this is not actually necessary, it's just good housekeeping\n    self.delete_tables_created_by_splink_from_db()\n\n    # As a result, any previously cached tables will not be found\n    self._intermediate_table_cache.invalidate_cache()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.labelling_tool_for_specific_record","title":"<code>labelling_tool_for_specific_record(unique_id, source_dataset=None, out_path='labelling_tool.html', overwrite=False, match_weight_threshold=-4, view_in_jupyter=False, show_splink_predictions_in_interface=True)</code>","text":"<p>Create a standalone, offline labelling dashboard for a specific record as identified by its unique id</p> <p>Parameters:</p> Name Type Description Default <code>unique_id</code> <code>str</code> <p>The unique id of the record for which to create the labelling tool</p> required <code>source_dataset</code> <code>str</code> <p>If there are multiple datasets, to identify the record you must also specify the source_dataset. Defaults to None.</p> <code>None</code> <code>out_path</code> <code>str</code> <p>The output path for the labelling tool. Defaults to \"labelling_tool.html\".</p> <code>'labelling_tool.html'</code> <code>overwrite</code> <code>bool</code> <p>If true, overwrite files at the output path if they exist. Defaults to False.</p> <code>False</code> <code>match_weight_threshold</code> <code>int</code> <p>Include possible matches in the output which score above this threshold. Defaults to -4.</p> <code>-4</code> <code>view_in_jupyter</code> <code>bool</code> <p>If you're viewing in the Jupyter html viewer, set this to True to extract your labels. Defaults to False.</p> <code>False</code> <code>show_splink_predictions_in_interface</code> <code>bool</code> <p>Whether to show information about the Splink model's predictions that could potentially bias the decision of the clerical labeller. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def labelling_tool_for_specific_record(\n    self,\n    unique_id,\n    source_dataset=None,\n    out_path=\"labelling_tool.html\",\n    overwrite=False,\n    match_weight_threshold=-4,\n    view_in_jupyter=False,\n    show_splink_predictions_in_interface=True,\n):\n    \"\"\"Create a standalone, offline labelling dashboard for a specific record\n    as identified by its unique id\n\n    Args:\n        unique_id (str): The unique id of the record for which to create the\n            labelling tool\n        source_dataset (str, optional): If there are multiple datasets, to\n            identify the record you must also specify the source_dataset. Defaults\n            to None.\n        out_path (str, optional): The output path for the labelling tool. Defaults\n            to \"labelling_tool.html\".\n        overwrite (bool, optional): If true, overwrite files at the output\n            path if they exist. Defaults to False.\n        match_weight_threshold (int, optional): Include possible matches in the\n            output which score above this threshold. Defaults to -4.\n        view_in_jupyter (bool, optional): If you're viewing in the Jupyter\n            html viewer, set this to True to extract your labels. Defaults to False.\n        show_splink_predictions_in_interface (bool, optional): Whether to\n            show information about the Splink model's predictions that could\n            potentially bias the decision of the clerical labeller. Defaults to\n            True.\n    \"\"\"\n\n    df_comparisons = generate_labelling_tool_comparisons(\n        self,\n        unique_id,\n        source_dataset,\n        match_weight_threshold=match_weight_threshold,\n    )\n\n    render_labelling_tool_html(\n        self,\n        df_comparisons,\n        show_splink_predictions_in_interface=show_splink_predictions_in_interface,\n        out_path=out_path,\n        view_in_jupyter=view_in_jupyter,\n        overwrite=overwrite,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n    \"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict, validate_settings=True)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict, validate_settings=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required <code>validate_settings</code> <code>bool</code> <p>When True, check your settings dictionary for any potential errors that may cause splink to fail.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def load_settings(\n    self,\n    settings_dict: dict | str | Path,\n    validate_settings: str = True,\n):\n    \"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict, validate_settings=True)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n        validate_settings (bool, optional): When True, check your settings\n            dictionary for any potential errors that may cause splink to fail.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_uid = self._cache_uid\n\n    # Invalidate the cache if anything currently exists. If the settings are\n    # changing, our charts, tf tables, etc may need changing.\n    self.invalidate_cache()\n\n    self._settings_dict = settings_dict  # overwrite or add\n\n    # Get the SQL dialect from settings_dict or use the default\n    sql_dialect = settings_dict.get(\"sql_dialect\", self._sql_dialect)\n    settings_dict[\"sql_dialect\"] = sql_dialect\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_uid)\n\n    # Check the user's comparisons (if they exist)\n    log_comparison_errors(settings_dict.get(\"comparisons\"), sql_dialect)\n    self._settings_obj_ = Settings(settings_dict)\n    # Check the final settings object\n    self._validate_settings(validate_settings)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code> Examples:     </p><pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> Args:     in_path (str): Path to settings json file             Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n    \"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n    \"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p></p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n    \"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.match_weights_histogram","title":"<code>match_weights_histogram(df_predict, target_bins=30, width=600, height=250)</code>","text":"<p>Generate a histogram that shows the distribution of match weights in <code>df_predict</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>Output of <code>linker.predict()</code></p> required <code>target_bins</code> <code>int</code> <p>Target number of bins in histogram. Defaults to 30.</p> <code>30</code> <code>width</code> <code>int</code> <p>Width of output. Defaults to 600.</p> <code>600</code> <code>height</code> <code>int</code> <p>Height of output chart. Defaults to 250.</p> <code>250</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_histogram(\n    self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n):\n    \"\"\"Generate a histogram that shows the distribution of match weights in\n    `df_predict`\n\n    Args:\n        df_predict (SplinkDataFrame): Output of `linker.predict()`\n        target_bins (int, optional): Target number of bins in histogram. Defaults to\n            30.\n        width (int, optional): Width of output. Defaults to 600.\n        height (int, optional): Height of output chart. Defaults to 250.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    df = histogram_data(self, df_predict, target_bins)\n    recs = df.as_record_dict()\n    return match_weights_histogram(recs, width=width, height=height)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the database.  If provided, missingness will be computed for this table alone. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p></p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.to_dict(), \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n    \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for\n            this table alone.\n            Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.to_dict(), \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=False)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to False.</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=False):\n    \"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to False.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n    \"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    # If exploded blocking rules exist, we need to materialise\n    # the tables of ID pairs\n    exploding_br_with_id_tables = materialise_exploded_id_tables(self)\n\n    sqls = block_using_rules_sqls(self)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n\n    [b.drop_materialised_id_pairs_dataframe() for b in exploding_br_with_id_tables]\n\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.prediction_errors_from_labels_column","title":"<code>prediction_errors_from_labels_column(label_colname, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column.  A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>Name of labels column in input data</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_column(\n    self,\n    label_colname,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the splink match probability and the\n    labels column.  A label column is a column in the input dataset that contains\n    the 'ground truth' cluster to which the record belongs\n\n    Args:\n        label_colname (str): Name of labels column in input data\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    return prediction_errors_from_label_column(\n        self,\n        label_colname,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n    \"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions=None, top_n=10, bottom_n=10)</code>","text":"<p>Profiles the specified columns of the dataframe initiated with the linker.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>For the provided columns with column_expressions (or for all columns if  left empty) calculate: - A distribution plot that shows the count of values at each percentile. - A top n chart, that produces a chart showing the count of the top n values within the column - A bottom n chart, that produces a chart showing the count of the bottom n values within the column</p> <p>This should be used to explore the dataframe, determine if columns have sufficient completeness for linking, analyse the cardinality of columns, and identify the need for standardisation within a given column.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>object</code> <p>The initiated linker.</p> required <code>column_expressions</code> <code>list</code> <p>A list of strings containing the specified column names. If left empty this will default to all columns.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>The number of top n values to plot.</p> <code>10</code> <code>bottom_n</code> <code>int</code> <p>The number of bottom n values to plot.</p> <code>10</code> <p>Returns:</p> Type Description <p>altair.Chart or dict: A visualization or JSON specification describing the</p> <p>profiling charts.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SparkLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = AthenaLinker(df)\nlinker.profile_columns()\n</code></pre> <pre><code>linker = SQLiteLinker(df)\nlinker.profile_columns()\n</code></pre> Note <ul> <li>The <code>linker</code> object should be an instance of the initiated linker.</li> <li>The provided <code>column_expressions</code> can be a list of column names to     profile. If left empty, all columns will be profiled.</li> <li>The <code>top_n</code> and <code>bottom_n</code> parameters determine the number of top and      bottom values to display in the respective charts.</li> </ul> Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str] = None, top_n=10, bottom_n=10\n):\n    \"\"\"\n    Profiles the specified columns of the dataframe initiated with the linker.\n\n    This can be computationally expensive if the dataframe is large.\n\n    For the provided columns with column_expressions (or for all columns if\n     left empty) calculate:\n    - A distribution plot that shows the count of values at each percentile.\n    - A top n chart, that produces a chart showing the count of the top n values\n    within the column\n    - A bottom n chart, that produces a chart showing the count of the bottom\n    n values within the column\n\n    This should be used to explore the dataframe, determine if columns have\n    sufficient completeness for linking, analyse the cardinality of columns, and\n    identify the need for standardisation within a given column.\n\n    Args:\n        linker (object): The initiated linker.\n        column_expressions (list, optional): A list of strings containing the\n            specified column names.\n            If left empty this will default to all columns.\n        top_n (int, optional): The number of top n values to plot.\n        bottom_n (int, optional): The number of bottom n values to plot.\n\n    Returns:\n        altair.Chart or dict: A visualization or JSON specification describing the\n        profiling charts.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df)\n            linker.profile_columns()\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df)\n            linker.profile_columns()\n            ```\n\n    Note:\n        - The `linker` object should be an instance of the initiated linker.\n        - The provided `column_expressions` can be a list of column names to\n            profile. If left empty, all columns will be profiled.\n        - The `top_n` and `bottom_n` parameters determine the number of top and\n             bottom values to display in the respective charts.\n    \"\"\"\n\n    return profile_columns(\n        self, column_expressions=column_expressions, top_n=top_n, bottom_n=bottom_n\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p>  DuckDB Spark Athena SQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n    \"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-amazonaws: Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \":simple-sqlite: SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database_and_remove_from_cache()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n    \"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.register_table_input_nodes_concat_with_tf","title":"<code>register_table_input_nodes_concat_with_tf(input_data, overwrite=False)</code>","text":"<p>Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run</p> <p>This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> Source code in <code>splink/linker.py</code> <pre><code>def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n    \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n    you want to re-use e.g. that you created in a previous run\n\n    This method allowed you to register this table in the Splink cache\n    so it will be used rather than Splink computing this table anew.\n\n    Args:\n        input_data: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n    \"\"\"\n\n    table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n    splink_dataframe = self.register_table(\n        input_data, table_name_physical, overwrite=overwrite\n    )\n    splink_dataframe.templated_name = \"__splink__df_concat_with_tf\"\n\n    self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n    return splink_dataframe\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n    \"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        SplinkDeprecated,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.tf_adjustment_chart","title":"<code>tf_adjustment_chart(output_column_name, n_most_freq=10, n_least_freq=10, vals_to_include=None, as_dict=False)</code>","text":"<p>Display a chart showing the impact of term frequency adjustments on a specific comparison level. Each value</p> <p>Parameters:</p> Name Type Description Default <code>output_column_name</code> <code>str</code> <p>Name of an output column for which term frequency  adjustment has been applied.</p> required <code>n_most_freq</code> <code>int</code> <p>Number of most frequent values to show. If this  or <code>n_least_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>n_least_freq</code> <code>int</code> <p>Number of least frequent values to show. If this or <code>n_most_freq</code> set to None, all values will be shown. Default to 10.</p> <code>10</code> <code>vals_to_include</code> <code>list</code> <p>Specific values for which to show term sfrequency adjustments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def tf_adjustment_chart(\n    self,\n    output_column_name: str,\n    n_most_freq: int = 10,\n    n_least_freq: int = 10,\n    vals_to_include: str | list = None,\n    as_dict: bool = False,\n):\n    \"\"\"Display a chart showing the impact of term frequency adjustments on a\n    specific comparison level.\n    Each value\n\n    Args:\n        output_column_name (str): Name of an output column for which term frequency\n             adjustment has been applied.\n        n_most_freq (int, optional): Number of most frequent values to show. If this\n             or `n_least_freq` set to None, all values will be shown.\n            Default to 10.\n        n_least_freq (int, optional): Number of least frequent values to show. If\n            this or `n_most_freq` set to None, all values will be shown.\n            Default to 10.\n        vals_to_include (list, optional): Specific values for which to show term\n            sfrequency adjustments.\n            Defaults to None.\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Comparisons with TF adjustments\n    tf_comparisons = [\n        c._output_column_name\n        for c in self._settings_obj.comparisons\n        if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n    ]\n    if output_column_name not in tf_comparisons:\n        raise ValueError(\n            f\"{output_column_name} is not a valid comparison column, or does not\"\n            f\" have term frequency adjustment activated\"\n        )\n\n    vals_to_include = ensure_is_list(vals_to_include)\n\n    return tf_adjustment_chart(\n        self,\n        output_column_name,\n        n_most_freq,\n        n_least_freq,\n        vals_to_include,\n        as_dict,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p>  DuckDB Spark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n    \"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \":simple-duckdb: DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \":simple-apachespark: Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. </p><pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().      <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n    \"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        from splink.datasets import splink_datasets\n        df = splink_datasets.fake_1000\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        altair.Chart: An altair chart\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True, remove_sensitive_data=False)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <code>remove_sensitive_data</code> <code>bool</code> <p>When True, The waterfall chart will contain match weights only, and all of the (potentially sensitive) data from the input tables will be removed prior to the chart being created.</p> <code>False</code> <p>Returns:</p> Type Description <p>altair.Chart: An altair chart</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(\n    self, records: list[dict], filter_nulls=True, remove_sensitive_data=False\n):\n    \"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n        remove_sensitive_data (bool, optional): When True, The waterfall chart will\n            contain match weights only, and all of the (potentially sensitive) data\n            from the input tables will be removed prior to the chart being created.\n\n\n    Returns:\n        altair.Chart: An altair chart\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(\n        records, self._settings_obj, filter_nulls, remove_sensitive_data\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"settings_dict_guide.html","title":"Settings Dictionary Reference","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#guide-to-splink-settings","title":"Guide to Splink settings","text":"<p>This document enumerates all the settings and configuration options available when developing your data linkage model.</p> <p>You can find an interactive settings editor here.</p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#link_type","title":"link_type","text":"<p>The type of data linking task.  Required.</p> <ul> <li> <p>When <code>dedupe_only</code>, <code>splink</code> find duplicates.  User expected to provide a single input dataset.</p> </li> <li> <p>When <code>link_and_dedupe</code>, <code>splink</code> finds links within and between input datasets.  User is expected to provide two or more input datasets.</p> </li> <li> <p>When <code>link_only</code>,  <code>splink</code> finds links between datasets, but does not attempt to deduplicate the datasets (it does not try and find links within each input dataset.) User is expected to provide two or more input datasets.</p> </li> </ul> <p>Examples: <code>['dedupe_only', 'link_only', 'link_and_dedupe']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#probability_two_random_records_match","title":"probability_two_random_records_match","text":"<p>The probability that two records chosen at random (with no blocking) are a match.  For example, if there are a million input records and each has on average one match, then this value should be 1/1,000,000.</p> <p>If you estimate parameters using expectation maximisation (EM), this provides an initial value (prior) from which the EM algorithm will start iterating.  EM will then estimate the true value of this parameter.</p> <p>Default value: <code>0.0001</code></p> <p>Examples: <code>[1e-05, 0.006]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#em_convergence","title":"em_convergence","text":"<p>Convergence tolerance for the Expectation Maximisation algorithm</p> <p>The algorithm will stop converging when the maximum of the change in model parameters between iterations is below this value</p> <p>Default value: <code>0.0001</code></p> <p>Examples: <code>[0.0001, 1e-05, 1e-06]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#max_iterations","title":"max_iterations","text":"<p>The maximum number of Expectation Maximisation iterations to run (even if convergence has not been reached)</p> <p>Default value: <code>25</code></p> <p>Examples: <code>[20, 150]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#unique_id_column_name","title":"unique_id_column_name","text":"<p>Splink requires that the input dataset has a column that uniquely identifies each record.  <code>unique_id_column_name</code> is the name of the column in the input dataset representing this unique id</p> <p>For linking tasks, ids must be unique within each dataset being linked, and do not need to be globally unique across input datasets</p> <p>Default value: <code>unique_id</code></p> <p>Examples: <code>['unique_id', 'id', 'pk']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#source_dataset_column_name","title":"source_dataset_column_name","text":"<p>The name of the column in the input dataset representing the source dataset</p> <p>Where we are linking datasets, we can't guarantee that the unique id column is globally unique across datasets, so we combine it with a source_dataset column.  Usually, this is created by Splink for the user</p> <p>Default value: <code>source_dataset</code></p> <p>Examples: <code>['source_dataset', 'dataset_name']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#retain_matching_columns","title":"retain_matching_columns","text":"<p>If set to true, each column used by the <code>comparisons</code> sql expressions will be retained in output datasets</p> <p>This is helpful so that the user can inspect matches, but once the comparison vector (gamma) columns are computed, this information is not actually needed by the algorithm.  The algorithm will run faster and use less resources if this is set to false.</p> <p>Default value: <code>True</code></p> <p>Examples: <code>[False, True]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#retain_intermediate_calculation_columns","title":"retain_intermediate_calculation_columns","text":"<p>Retain intermediate calculation columns, such as the Bayes factors associated with each column in <code>comparisons</code></p> <p>The algorithm will run faster and use less resources if this is set to false.</p> <p>Default value: <code>False</code></p> <p>Examples: <code>[False, True]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparisons","title":"comparisons","text":"<p>A list specifying how records should be compared for probabilistic matching.  Each element is a dictionary</p> Settings keys nested within each member of <code>comparisons</code>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#output_column_name","title":"output_column_name","text":"<p>The name used to refer to this comparison in the output dataset.  By default, Splink will set this to the name(s) of any input columns used in the comparison.  This key is most useful to give a clearer description to comparisons that use multiple input columns.  e.g. a location column that uses postcode and town may be named location</p> <p>For a comparison column that uses a single input column, e.g. first_name, this will be set first_name. For comparison columns that use multiple columns, if left blank, this will be set to the concatenation of columns used.</p> <p>Examples: <code>['first_name', 'surname']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_description","title":"comparison_description","text":"<p>An optional label to describe this comparison, to be used in charting outputs.</p> <p>Examples: <code>['First name exact match', 'Surname with middle levenshtein level']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_levels","title":"comparison_levels","text":"<p>Comparison levels specify how input values should be compared.  Each level corresponds to an assessment of similarity, such as exact match, Jaro-Winkler match, one side of the match being null, etc</p> <p>Each comparison level represents a branch of a SQL case expression. They are specified in order of evaluation, each with a sql_condition that represents the branch of a case expression</p> <p>Example:  </p><pre><code>[{\n    \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\", \n    \"label_for_charts\": \"null\", \n    \"null_level\": True\n}, \n{\n    \"sql_condition\": \"first_name_l = first_name_r\", \n    \"label_for_charts\": \"exact_match\", \n    \"tf_adjustment_column\": \"first_name\"\n}, \n{\n    \"sql_condition\": \"ELSE\", \n    \"label_for_charts\": \"else\"\n}]\n</code></pre> Settings keys nested within each member of <code>comparison_levels</code>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#sql_condition","title":"sql_condition","text":"<p>A branch of a SQL case expression without WHEN and THEN e.g. 'jaro_winkler_sim(surname_l, surname_r) &gt; 0.88'</p> <p>Examples: <code>['forename_l = forename_r', 'jaro_winkler_sim(surname_l, surname_r) &gt; 0.88']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#label_for_charts","title":"label_for_charts","text":"<p>A label for this comparison level, which will appear on charts as a reminder of what the level represents</p> <p>Examples: <code>['exact', 'postcode exact']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#u_probability","title":"u_probability","text":"<p>the u probability for this comparison level - i.e. the proportion of records that match this level amongst truly non-matching records</p> <p>Examples: <code>[0.9]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#m_probability","title":"m_probability","text":"<p>the m probability for this comparison level - i.e. the proportion of records that match this level amongst truly matching records</p> <p>Examples: <code>[0.1]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#is_null_level","title":"is_null_level","text":"<p>If true, m and u values will not be estimated and instead the match weight will be zero for this column.  See treatment of nulls here on page 356, quote '. Under this MAR assumption, we can simply ignore missing data.': https://imai.fas.harvard.edu/research/files/linkage.pdf</p> <p>Default value: <code>False</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_adjustment_column","title":"tf_adjustment_column","text":"<p>Make term frequency adjustments for this comparison level using this input column</p> <p>Default value: <code>None</code></p> <p>Examples: <code>['first_name', 'postcode']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_adjustment_weight","title":"tf_adjustment_weight","text":"<p>Make term frequency adjustments using this weight. A weight of 1.0 is a full adjustment.  A weight of 0.0 is no adjustment.  A weight of 0.5 is a half adjustment</p> <p>Default value: <code>1.0</code></p> <p>Examples: <code>['first_name', 'postcode']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_minimum_u_value","title":"tf_minimum_u_value","text":"<p>Where the term frequency adjustment implies a u value below this value, use this minimum value instead</p> <p>This prevents excessive weight being assigned to very unusual terms, such as a collision on a typo</p> <p>Default value: <code>0.0</code></p> <p>Examples: <code>[0.001, 1e-09]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#blocking_rules_to_generate_predictions","title":"blocking_rules_to_generate_predictions","text":"<p>A list of one or more blocking rules to apply. A Cartesian join is applied if <code>blocking_rules_to_generate_predictions</code> is empty or not supplied.</p> <p>Each rule is a SQL expression representing the blocking rule, which will be used to create a join.  The left table is aliased with <code>l</code> and the right table is aliased with <code>r</code>. For example, if you want to block on a <code>first_name</code> column, the blocking rule would be</p> <p><code>l.first_name = r.first_name</code>.</p> <p>To block on first name and the first letter of surname, it would be</p> <p><code>l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)</code>.</p> <p>Note that Splink deduplicates the comparisons generated by the blocking rules.</p> <p>If empty or not supplied, all comparisons between the input dataset(s) will be generated and blocking will not be used. For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared.</p> <p>Default value: <code>[]</code></p> <p>Examples: <code>[['l.first_name = r.first_name AND l.surname = r.surname', 'l.dob = r.dob']]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#additional_columns_to_retain","title":"additional_columns_to_retain","text":"<p>A list of columns not being used in the probabalistic matching comparisons that you want to include in your results.</p> <p>By default, splink drops columns which are not used by any comparisons.  This gives you the option to retain columns which are not used by the model.  A common example is if the user has labelled data (training data) and wishes to retain the labels in the outputs</p> <p>Default value: <code>[]</code></p> <p>Examples: <code>[['cluster', 'col_2'], ['other_information']]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#bayes_factor_column_prefix","title":"bayes_factor_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the Bayes factors</p> <p>Default value: <code>bf_</code></p> <p>Examples: <code>['bf_', '__bf__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#term_frequency_adjustment_column_prefix","title":"term_frequency_adjustment_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the term frequency adjustments</p> <p>Default value: <code>tf_</code></p> <p>Examples: <code>['tf_', '__tf__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_vector_value_column_prefix","title":"comparison_vector_value_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the comparison vector values</p> <p>Default value: <code>gamma_</code></p> <p>Examples: <code>['gamma_', '__gamma__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#sql_dialect","title":"sql_dialect","text":"<p>The SQL dialect in which sql_conditions are written.  Must be a valid sqlglot dialect</p> <p>Default value: <code>None</code></p> <p>Examples: <code>['spark', 'duckdb', 'presto', 'sqlite']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"blog/index.html","title":"Blog","text":"","tags":["Blog","News"]},{"location":"blog/2023/07/27/splink-updates---july-2023.html","title":"Splink Updates - July 2023","text":""},{"location":"blog/2023/07/27/splink-updates---july-2023.html#splink-updates-july-2023","title":"Splink Updates - July 2023","text":""},{"location":"blog/2023/07/27/splink-updates---july-2023.html#welcome-to-the-splink-blog","title":"Welcome to the Splink Blog!","text":"<p>Its hard to keep up to date with all of the new features being added to Splink, so we have launched this blog to share a round up of latest developments every few months.</p> <p>So, without further ado, here are some of the highlights from the first half of 2023!</p> <p>Latest Splink version: v3.9.4</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#massive-speed-gains-in-em-training","title":"Massive speed gains in EM training","text":"<p>There\u2019s now an option to make EM training much faster - in one example we\u2019ve seen at 1000x fold speedup.  Kudos to external contributor @aymonwuolanne from the Australian Bureau of Statistics!</p> <p>To make use of this, set the <code>estimate_without_term_frequencies</code> parameter to True; for example:</p> <pre><code>linker.estimate_parameters_using_expectation_maximisation(..., estimate_without_term_frequencies=True)\n</code></pre> <p>Note: If True, the EM algorithm ignores term frequency adjustments during the iterations. Instead, the adjustments are added once the EM algorithm has converged. This will result in slight difference in the final parameter estimations.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#out-of-the-box-comparisons","title":"Out-of-the-box Comparisons","text":"<p>Splink now contains lots of new out-of-the-box comparisons for dates, names, postcodes etc. The Comparison Template Library (CTL) provides suggested settings for common types of data used in linkage models.</p> <p>For example, a Comparison for <code>\"first_name\"</code> can now be written as:</p> <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\"first_name\")\n</code></pre> <p>Check out these new functions in the CTL Topic Guide and CTL Documentation.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#blocking-rule-library","title":"Blocking Rule Library","text":"<p>Blocking has, historically, been a point of confusion for users so we have been working behind the scenes to make that easier! The recently launched Blocking Rules Library (BRL) provides a set of functions for defining Blocking Rules (similar to the Comparison Library functions).</p> <p>For example, a Blocking Rule for <code>\"date_of_birth\"</code> can now be written as:</p> <pre><code>import splink.duckdb.blocking_rule_library as brl\n\nbrl.exact_match_rule(\"date_of_birth\")\n</code></pre> <p>Note: from Splink v3.9.6, <code>exact_match_rule</code> has been superseded by <code>block_on</code>. We advise using this going forward.</p> <p>Check out these new functions in the BRL Documentation as well as some new Blocking Topic Guides to better explain what Blocking Rules are, how they are used in Splink, and how to choose them.</p> <p>Keep a look out, as there are more improvements in the pipeline for Blocking in the coming months!</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#postgres-support","title":"Postgres Support","text":"<p>With a massive thanks to external contributor @hanslemm, Splink now supports  Postgres. To get started, check out the Postgres Topic Guide.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#clerical-labelling-tool-beta","title":"Clerical Labelling Tool (beta)","text":"<p>Clerical labelling is an important tool for generating performance metrics for linkage models (False Positive Rate, Recall, Precision etc.).</p> <p>Splink now has a (beta) GUI for clerical labelling which produces labels in a form that can be easily ingested into Splink to generate these performance metrics. Check out the example tool, linked Pull Request, and some previous tweets:</p> <p>Draft new Splink tool to speed up manual labelling of record linkage data. Example dashboard: https://t.co/yc1yHpa90X Grateful for any feedback whilst I'm still working on this, on Twitter or the draft PR: https://t.co/eXSNHHe2kcFree and open source pic.twitter.com/MEo4DmaxO9</p>\u2014 Robin Linacre (@RobinLinacre) April 28, 2023 <p>This tool is still in the beta phase, so is a work in progress and subject to change based on feedback we get from users. As a result, it is not thoroughly documented at this stage. We recommend checking out the links above to see a ready-made example of the tool. However, if you would like to generate your own, this example is a good starting point.</p> <p>We would love any feedback from users, so please comment on the PR or open a discussion.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#charts-in-altair-5","title":"Charts in Altair 5","text":"<p>Charts are now all fully-fledged Altair charts, making them much easier to work with.</p> <p>For example, a chart <code>c</code> can now be saved with:</p> <pre><code>c.save(\u201cchart.png\u201d, scale_factor=2)\n</code></pre> <p>where <code>json</code>, <code>html</code>, <code>png</code>, <code>svg</code> and <code>pdf</code> are all supported.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#reduced-duplication-in-comparison-libraries","title":"Reduced duplication in Comparison libraries","text":"<p>Historically, importing of the comparison libraries (CL, CTL, CLL) has included declaring the backend twice. For example:</p> <p></p><pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n</code></pre> This repetition has now been removed <pre><code>import splink.duckdb.comparison_level_library as cll\n</code></pre> The original structure still works, but throws a warning to switch to the new version."},{"location":"blog/2023/07/27/splink-updates---july-2023.html#in-built-datasets","title":"In-built datasets","text":"<p>When following along with the tutorial or example notebooks, one issue can be references of paths to data that does not exists on users machines. To overcome this issue, Splink now has a <code>splink_datasets</code> module which will store these datasets and make sure any users can copy and paste working code without fear of path issues. For example:</p> <p></p><pre><code>from splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n</code></pre> returns the fake 1000 row dataset that is used in the Splink tutorial. <p>For more information check out the in-built datasets Documentation.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#regular-expressions-in-comparisons","title":"Regular Expressions in Comparisons","text":"<p>When comparing records, some columns will have a particular structure (e.g. dates, postcodes, email addresses). It can be useful to compare sections of a column entry. Splink's string comparison level functions now include a <code>regex_extract</code> to extract a portion of strings to be compared. For example, an <code>exact_match</code> comparison that compares the first section of a postcode (outcode) can be written as:</p> <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\npc_comparison = cl.exact_match(\"postcode\", regex_extract=\"^[A-Z]{1,2}\")\n</code></pre> <p>Splink's string comparison level functions also now include a <code>valid_string_regex</code> parameter which sends any entries that do not conform to a specified structure to the null level. For example, a <code>levenshtein</code> comparison that ensures emails have an \"@\" symbol can be written as:</p> <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nemail_comparison = cl.levenshtein_at_thresholds(\"email\", valid_string_regex=\"^[^@]+\")\n</code></pre> <p>For more on how Regular Expressions can be used in Splink, check out the Regex topic guide.</p> <p>Note: from Splink v3.9.6, <code>valid_string_regex</code> has been renamed as <code>valid_string_pattern</code>.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#documentation-improvements","title":"Documentation Improvements","text":"<p>We have been putting a lot of effort into improving our documentation site, including launching this blog!</p> <p>Some of the improvements include:</p> <ul> <li>More Topic Guides covering things such as Record Linkage Theory, Guidance on Splink's backends and String Fuzzy Matching.</li> <li>A Contributors Guide to make contributing to Splink even easier. If you are interested in getting involved in open source, check the guide out!</li> <li>Adding tables to the Comparison libraries documentation to show the functions available for each SQL backend.</li> </ul> <p>Thanks to everyone who filled out our feedback survey. If you have any more feedback or ideas for how we can make the docs better please do let us know by raising an issue, starting a discussion or filling out the survey.</p>"},{"location":"blog/2023/07/27/splink-updates---july-2023.html#whats-in-the-pipeline","title":"What's in the pipeline?","text":"<ul> <li>   More Blocking improvements</li> <li>   Settings dictionary improvements</li> <li>   More guidance on how to evaluate Splink models and linkages</li> </ul>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html","title":"Splink Updates - December 2023","text":""},{"location":"blog/2023/12/06/splink-updates---december-2023.html#splink-updates-december-2023","title":"Splink Updates - December 2023","text":"<p>Welcome to the second installment of the Splink Blog! </p> <p>Here are some of the highlights from the second half of 2023, and a taste of what is in store for 2024!</p> <p>Latest Splink version: v3.9.10</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#charts-gallery","title":"Charts Gallery","text":"<p>The Splink docs site now has a Charts Gallery to show off all of the charts that come out-of-the-box with Splink to make linking easier. </p> <p></p> <p>Each chart now has an explanation of:</p> <ol> <li>What the chart shows</li> <li>How to interpret it</li> <li>Actions to take as a result</li> </ol> <p>This is the first step on a longer term journey to provide more guidance on how to evaluate Splink models and linkages, so watch this space for more in the coming months!</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#new-charts","title":"New Charts","text":"<p>We are always adding more charts to Splink - to understand how these charts are built see our new Charts Developer Guide.</p> <p>Two of our latest additions are:</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#confusion-matrix","title":"Confusion Matrix","text":"<p>When evaluating any classification model, a confusion matrix is a useful tool for summarizing performance by representing counts of true positive, true negative, false positive, and false negative predictions.</p> <p>Splink now has its own confusion matrix chart to show how model performance changes with a given match weight threshold. </p> <p></p> <p>Note, labelled data is required to generate this chart.</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#completeness-chart","title":"Completeness Chart","text":"<p>When linking multiple datasets together, one of the most important factors for a successful linkage is the number of common fields across the datasets. </p> <p>Splink now has the completeness chart which gives a simple view of how well populated fields are across datasets.</p> <p></p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#settings-validation","title":"Settings Validation","text":"<p>The Settings dictionary is central to everything in Splink. It defines everything from the sql dialect of your backend to how features are compared in Splink model. </p> <p>A common sticking point with users is how easy it is to make small errors when defining the Settings dictionary, resulting in unhelpful error messages.</p> <p>To address this issue, the Settings Validator provides clear, user-friendly feedback on what the issue is and how to fix it.</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#blocking-rule-library-improved","title":"Blocking Rule Library (Improved)","text":"<p>In our previous blog we introduced the Blocking Rule Library (BRL) built upon the <code>exact_match_rule</code> function. When testing this functionality we found it pretty verbose, particularly when blocking on multiple columns, so figured we could do better. From Splink v3.9.6 we introduced the <code>block_on</code> function to supercede <code>exact_match_rule</code>.</p> <p>For example, a block on <code>first_name</code> and <code>surname</code> now looks like:</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblock_on([\"first_name\", \"surname\"])  \n</code></pre> <p>as opposed to</p> <pre><code>import splink.duckdb.blocking_rule_library as brl\nbrl.and_(\n  brl.exact_match_rule(\"first_name\"),\n  brl.exact_match_rule(\"surname\")\n)\n</code></pre> <p>All of the tutorials, example notebooks and API docs have been updated to use <code>block_on</code>.</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#backend-specific-installs","title":"Backend Specific Installs","text":"<p>Some users have had difficulties downloading Splink due to additional dependencies, some of which may not be relevant for the backend they are using. To solve this, you can now install a minimal version of Splink for your given SQL engine.</p> <p>For example, to install Splink purely for Spark use the command:</p> <pre><code>pip install 'splink[spark]'\n</code></pre> <p>See the Getting Started page for further guidance.</p>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#drop-support-for-python-37","title":"Drop support for python 3.7","text":"<p>From Splink 3.9.7, support has been dropped for python 3.7. This decision has been made to manage dependency clashes in the back end of Splink.</p> <p>If you are working with python 3.7, please revert to Splink 3.9.6.</p> <pre><code>pip install splink==3.9.6\n</code></pre>"},{"location":"blog/2023/12/06/splink-updates---december-2023.html#whats-in-the-pipeline","title":"What's in the pipeline?","text":"<ul> <li>   Work on Splink 4 is currently underway</li> <li>   More guidance on how to evaluate Splink models and linkages</li> </ul>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html","title":"Ethics in Data Linking","text":""},{"location":"blog/2024/01/23/ethics-in-data-linking.html#ethics-in-data-linking","title":"Ethics in Data Linking","text":"<p>Welcome to the next installment of the Splink Blog where we\u2019re talking about Data Ethics!</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#why-should-we-care-about-ethics","title":"Why should we care about ethics?","text":"<p>Splink was developed in-house at the UK Government\u2019s Ministry of Justice. As data scientists in government, we are accountable to the public and have a duty to maintain public trust. This includes upholding high standards of data ethics in our work.</p> <p>Furthermore, data linkage is generally used at the start of analytical projects so any design decisions that are made, or biases introduced, will have consequences for all downstream use cases of that data. With this in mind, it is important to try and address any potential ethical issues at the linking stage.</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#ethics-and-splink","title":"Ethics and Splink","text":""},{"location":"blog/2024/01/23/ethics-in-data-linking.html#what-do-we-already-have-in-place","title":"What do we already have in place?","text":"<p>Data ethics has been a foundational consideration throughout Splink\u2019s development. For example, the decision to make Splink open-source was motivated by an ambition to make our data linking software fully transparent, accessible and auditable to users both inside and outside of government. The fact that this also empowers external users to expand and improve upon Splink\u2019s functionality is another huge benefit!</p> <p>Another core principle guiding the development of Splink has been explainability. Under the hood we use the Felligi-Sunter model which is an industry-standard, well-researched, explainable methodology. This, in combination with interactive charts such as the waterfall chart, where model results can be easily broken down and visualised for individual record pairs, make Splink predictions easily interrogatable and explainable. Being able to interrogate predictions is especially valuable when things go wrong - if an incorrect link has been made you can trace it back see exactly why the model made the decision.</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#what-else-should-we-be-considering","title":"What else should we be considering?","text":"<p>To continue our exploration of ethical issues, we recently had a team away day focused on data ethics. We aimed to better understand where ethical concerns (e.g. bias) could arise in our own Splink linkage pipelines and what further steps we could take to empower users to be able to better understand and possibly mitigate these issues within their own projects. </p> <p>We discussed a typical data linking pipeline, as used in the Ministry of Justice, from data collection at source through to the generation of Splink cluster IDs. It became clear that there are considerations to make at each stage of a pipeline that can have an ethical implications such as:</p> <p></p> <p>For example, a higher occurrence of misspellings for names of non-UK origin during data collection can impact the accuracy of links for certain groups.</p> <p>As you can see, the entire data linking process has many stages with lots of moving parts, resulting in numerous opportunities for ethical issues to arise. </p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#what-are-we-going-to-do-about-it","title":"What are we going to do about it?","text":"<p>Summarised below are the key areas of ethical concern we identified and how we plan to address them.</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#evaluation","title":"Evaluation","text":"<p>Splink is not plug and play. As a software, it provides many configuration options to support its users, from blocking rules to term frequency adjustments. However, with greater flexibility comes greater variation in model design. From an explainability and quality assurance perspective, it is important to understand how different choices on model build interact and can influence results.</p> <p>It isn\u2019t trivial to unpick the interplaying factors that affect Splink\u2019s outputs, but as a first step we are building a framework and guidance to demonstrate how changes to a model's settings can impact predictions. We hope this will give users a better understanding of model sensitivity and more confidence in explaining and justifying the results of their models. We also hope this will serve as a stepping stone to tools that help evaluate models in a production setting (e.g. model drift).</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#bias","title":"Bias","text":"<p>Bias is a key area of ethical concern within data linking and one that crops up at many stages during a typical linking pipeline; from data collection to downstream linking. It is important to identify, quantify and, where possible, mitigate bias in input sources, model building and outputs. However, sources of bias are specific to a given use-case, and therefore finding general solutions to mitigating bias is challenging.</p> <p>This year we are embarking on a collaboration with the Alan Turing Institute to get expert support on assessing bias in our linking pipelines. The long-term goal is to create general tooling to help Splink users gain a better understanding of how bias could be being introduced into their models. Improved model evaluation (see above) is a first step in the development of these tools.</p>"},{"location":"blog/2024/01/23/ethics-in-data-linking.html#communication","title":"Communication","text":"<p>Sharing both our current knowledge and future discoveries on the ethics of data linking with Splink is important to help support our users and the data linking community more broadly. This blog is the first step on that journey for us.</p> <p>As already mentioned, Splink comes with a variety of tools that support explainability. We will be updating the Splink documentation to convey the significance of these resources from a data ethics perspective to help give existing users, potential adopters and their customers greater confidence in building Splink models and model predictions.</p> <p>Please visit the Ethics in Data Linking discussion on Splink's GitHub repo to get involved in the conversation and share your thoughts - we'd love to hear them!</p> <p>If you want to stay up to date with the latest Splink blogs subscribe to our new  RSS feed! </p>"},{"location":"charts/index.html","title":"Charts Gallery","text":""},{"location":"charts/index.html#charts-gallery","title":"Charts Gallery","text":""},{"location":"charts/index.html#exploratory-analysis","title":"Exploratory Analysis","text":"<p><code>completeness chart</code></p> <p><code>missingness chart</code></p> <p><code>profile columns</code></p>"},{"location":"charts/index.html#blocking","title":"Blocking","text":"<p><code>cumulative num comparisons from blocking rules chart</code></p>"},{"location":"charts/index.html#comparison-helpers","title":"Comparison Helpers","text":"<p><code>comparator score chart</code></p> <p><code>comparator score threshold chart</code></p> <p><code>phonetic match chart</code></p>"},{"location":"charts/index.html#evaluation","title":"Evaluation","text":""},{"location":"charts/index.html#model-evaluation","title":"Model Evaluation","text":"<p><code>match weights chart</code></p> <p><code>m u parameters chart</code></p> <p><code>parameter estimate comparisons chart</code></p> <p><code>tf adjustment chart</code></p> <p><code>unlinkables chart</code></p>"},{"location":"charts/index.html#edge-link-evaluation","title":"Edge (Link) Evaluation","text":""},{"location":"charts/index.html#overall","title":"Overall","text":"<p><code>accuracy chart from labels table</code></p> <p><code>confusion matrix from labels table</code></p> <p><code>precision recall chart from labels table</code></p> <p><code>roc chart fromm labels table</code></p>"},{"location":"charts/index.html#spot-checking","title":"Spot Checking","text":"<p><code>comparison viewer dashboard</code></p> <p><code>waterfall chart</code></p>"},{"location":"charts/index.html#cluster-evaluation","title":"Cluster Evaluation","text":""},{"location":"charts/index.html#overall_1","title":"Overall","text":""},{"location":"charts/index.html#spot-checking_1","title":"Spot Checking","text":"<p><code>cluster studio dashboard</code></p>"},{"location":"charts/index.html#all-charts","title":"All Charts","text":"<p><code>accuracy chart from labels table</code></p> <p><code>cluster studio dashboard</code></p> <p><code>confusion matrix from labels table</code></p> <p><code>comparator score chart</code></p> <p><code>comparator score threshold chart</code></p> <p><code>comparison viewer dashboard</code></p> <p><code>completeness chart</code></p> <p><code>cumulative num comparisons from blocking rules chart</code></p> <p><code>m u parameters chart</code></p> <p><code>match weights chart</code></p> <p><code>missingness chart</code></p> <p><code>parameter estimate comparisons chart</code></p> <p><code>phonetic match chart</code></p> <p><code>precision recall chart from labels table</code></p> <p><code>profile columns</code></p> <p><code>roc chart from labels table</code></p> <p><code>tf adjustment chart</code></p> <p><code>unlinkables chart</code></p> <p><code>waterfall chart</code></p>"},{"location":"charts/accuracy_chart_from_labels_table.html","title":"accuracy chart from labels table","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets, splink_dataset_labels\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.register_labels_table(df_labels)\n\nlinker.accuracy_chart_from_labels_table(labels_table, add_metrics=['f1'])\n</code></pre>"},{"location":"charts/accuracy_chart_from_labels_table.html#accuracy_chart_from_labels_table","title":"<code>accuracy_chart_from_labels_table</code>","text":"<p>At a glance</p> <p>Useful for: Selecting an optimal match weight threshold for generating linked clusters.</p> <p>API Documentation: accuracy_chart_from_labels_table()</p> <p>What is needed to generate the chart? A <code>linker</code> with some data and a corresponding labelled dataset</p>"},{"location":"charts/accuracy_chart_from_labels_table.html#worked-example","title":"Worked Example","text":""},{"location":"charts/accuracy_chart_from_labels_table.html#what-the-chart-shows","title":"What the chart shows","text":"<p>For a given match weight threshold, a record pair with a score above this threshold will be labelled a match and below the threshold will be labelled a non-match. For all possible match weight thresholds, this chart shows various accuracy metrics comparing the Splink scores against clerical labels. </p> <p>Precision and recall are shown by default, but various additional metrics can be added: specificity, negative predictive value (NPV), accuracy, \\(F_1\\), \\(F_2\\), \\(F_{0.5}\\), \\(P_4\\) and \\(\\phi\\) (Matthews correlation coefficient).</p>"},{"location":"charts/accuracy_chart_from_labels_table.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Precision can be maximised by increasing the match threshold (reducing false positives).</p> <p>Recall can be maximised by decreasing the match threshold (reducing false negatives). </p> <p>Additional metrics can be used to find the optimal compromise between these two, looking for the threshold at which peak accuracy is achieved. </p> <p>Confusion matrix</p> <p>See confusion_matrix_from_labels_table for a more explicit visualisation of the impact of match threshold on false positives and false negatives.</p>"},{"location":"charts/accuracy_chart_from_labels_table.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>Having identified an optimal match weight threshold, this can be applied when generating linked clusters using cluster_pairwise_predictions_at_thresholds().</p>"},{"location":"charts/cluster_studio_dashboard.html","title":"cluster studio dashboard","text":"<p>Work in Progress</p> <p>This page is currently under construction. </p> <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n    \"retain_matching_columns\":True,\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\ndf_predictions = linker.predict(threshold_match_probability=0.2)\ndf_clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)\n\nlinker.cluster_studio_dashboard(df_predictions, df_clusters, \"img/cluster_studio.html\", sampling_method=\"by_cluster_size\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./img/cluster_studio.html\", width=\"100%\", height=1200\n)\n</code></pre>"},{"location":"charts/cluster_studio_dashboard.html#cluster_studio_dashboard","title":"<code>cluster_studio_dashboard</code>","text":"<p>At a glance</p> <p>Useful for: </p> <p>API Documentation: cluster_studio_dashboard()</p> <p>What is needed to generate the chart? </p>"},{"location":"charts/cluster_studio_dashboard.html#worked-example","title":"Worked Example","text":""},{"location":"charts/cluster_studio_dashboard.html#what-the-chart-shows","title":"What the chart shows","text":""},{"location":"charts/cluster_studio_dashboard.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":""},{"location":"charts/cluster_studio_dashboard.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":""},{"location":"charts/comparison_viewer_dashboard.html","title":"comparison viewer dashboard","text":"<p>Work in Progress</p> <p>This page is currently under construction. </p> <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n    \"retain_matching_columns\":True,\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\ndf_predictions = linker.predict(threshold_match_probability=0.2)\n\nlinker.comparison_viewer_dashboard(df_predictions, \"img/scv.html\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./img/scv.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/comparison_viewer_dashboard.html#comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard</code>","text":"<p>At a glance</p> <p>Useful for: </p> <p>API Documentation: comparison_viewer_dashboard()</p> <p>What is needed to generate the chart? </p>"},{"location":"charts/comparison_viewer_dashboard.html#worked-example","title":"Worked Example","text":""},{"location":"charts/comparison_viewer_dashboard.html#what-the-chart-shows","title":"What the chart shows","text":""},{"location":"charts/comparison_viewer_dashboard.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":""},{"location":"charts/comparison_viewer_dashboard.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":""},{"location":"charts/completeness_chart.html","title":"completeness chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\n# Split a simple dataset into two, separate datasets which can be linked together.\ndf_l = df.sample(frac=0.5)\ndf_r = df.drop(df_l.index)\n\nsettings = {\n    \"link_type\": \"link_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker([df_l, df_r], settings, input_table_aliases=[\"df_left\", \"df_right\"])\n\nlinker.completeness_chart(cols=[\"first_name\", \"surname\", \"dob\", \"city\", \"email\"])\n</code></pre> What the chart tooltip shows <p>The tooltip shows a number of values based on the panel that the user is hovering over, including:</p> <ul> <li>The dataset and column name</li> <li>The count and percentage of non-null values in the column for the relelvant dataset.</li> </ul>"},{"location":"charts/completeness_chart.html#completeness_chart","title":"<code>completeness_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at which columns are populated across datasets. </p> <p>API Documentation: completeness_chart()</p> <p>What is needed to generate the chart? A <code>linker</code> with some data.</p>"},{"location":"charts/completeness_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/completeness_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>completeness_chart</code> shows the proportion of populated (non-null) values in the columns of multiple datasets.</p>"},{"location":"charts/completeness_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Each panel represents the percentage of non-null values in a given dataset-column combination. The darker the panel, the lower the percentage of non-null values.</p>"},{"location":"charts/completeness_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>Only choose features that are sufficiently populated across all datasets in a linkage model.</p>"},{"location":"charts/confusion_matrix_from_labels_table.html","title":"confusion matrix from labels table","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nimport splink.duckdb.blocking_rule_library as brl\nfrom splink.datasets import splink_datasets, splink_dataset_labels\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        brl.exact_match_rule(\"first_name\"),\n        brl.exact_match_rule(\"surname\"),\n        brl.exact_match_rule(\"city\"),\n        brl.exact_match_rule(\"dob\"),\n        brl.exact_match_rule(\"email\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = brl.and_(\n                            brl.exact_match_rule(\"first_name\"), \n                            brl.exact_match_rule(\"surname\")\n                            )\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = brl.exact_match_rule(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.register_labels_table(df_labels)\n\nlinker.confusion_matrix_from_labels_table(labels_table)\n</code></pre>"},{"location":"charts/confusion_matrix_from_labels_table.html#confusion_matrix_from_labels_table","title":"<code>confusion_matrix_from_labels_table</code>","text":"<p>At a glance</p> <p>Useful for: Summarising how Splink predictions compare with labelled data</p> <p>API Documentation: confusion_matrix_from_labels_table_chart()</p> <p>What is needed to generate the chart? A <code>linker</code> with some data and a corresponding labelled dataset</p>"},{"location":"charts/confusion_matrix_from_labels_table.html#worked-example","title":"Worked Example","text":""},{"location":"charts/confusion_matrix_from_labels_table.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The line chart on the left shows how match probability varies as a function of match weight. </p> <p>Hovering over this chart selects a match weight threshold and compares the results against labelled data by updating the confusion matrix shown on the right.</p>"},{"location":"charts/confusion_matrix_from_labels_table.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Lowering the threshold to the extreme ensures many more matches are generated - this maximises the True Positives (high recall) but at the expense of some False Positives (low precision).</p> <p>You can then see the effect on the confusion matrix of raising the match threshold. As more predicted matches become non-matches at the higher threshold, True Positives become False Negatives, but False Positives become True Negatives. </p> <p>This demonstrates the trade-off between Type 1 (FP) and Type 2 (FN) errors when selecting a match threshold, or precision vs recall.</p>"},{"location":"charts/confusion_matrix_from_labels_table.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>This chart is best used to illustrate the effect of match threshold on linking performance against labelled data.</p> <p>In order to make a decision about the optimal threshold to use, see accuracy_chart_from_labels_table_chart() to use this confusion matrix to calculate various performance metrics.</p>"},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html","title":"cumulative num comparisons from blocking rules chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n        block_on(\"email\")\n    ]\n}\n\nlinker = DuckDBLinker(df, settings)\n\nlinker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <p>Alternatively, Blocking Rules can be passed into the chart directly:</p> <pre><code>brs = [\n        block_on([\"first_name\", \"dob\"]),\n        block_on(\"surname\"),\n        block_on(\"email\")\n]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(brs)\n</code></pre> What the chart tooltip shows <p></p> <p>The tooltip shows a number of statistics based on the bar that the user is hovering over, including:</p> <ul> <li>The blocking rule as an SQL statement.</li> <li>The number of additional pairwise comparisons generated by the blocking rule.</li> <li>The cumulative number of pairwise comparisons generated by the blocking rule and the previous blocking rules.</li> <li>The total number of possible pariwise comparisons (i.e. the Cartesian product). This represents the number of comparisons which would need to be evaluated if no blocking was implemented.</li> <li>The percentage of possible pairwise comparisons excluded by the blocking rule and the previous blocking rules (i.e. the Reduction Ratio). This is calculated as \\(1-\\frac{\\textsf{cumulative comparisons}}{\\textsf{total possible comparisons}}\\).</li> </ul> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"surname\"),\n        block_on(\"first_name\"),\n        block_on(\"email\")\n    ]\n}\n\nlinker = DuckDBLinker(df, settings)\n\nlinker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <p>The total number of comparisons is the same (3,664), but now 1,638 have been generated by the <code>surname</code> blocking rule. This suggests that 287 record comparisons have the same <code>first_name</code> and <code>surname</code>.</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on([\"first_name\", \"dob\"]),\n        block_on(\"surname\"),\n        block_on(\"email\")\n    ]\n}\n\nlinker = DuckDBLinker(df, settings)\n\nlinker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <p>Here, the total number of records pairs considered by the model have been reduced from 3,664 to 2,213.</p> <p>Further Reading</p> <p> For a deeper dive on blocking, please refer to the Blocking Topic Guides.</p> <p> For more on the blocking tools in Splink, please refer to the Blocking API documentation.</p>"},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html#cumulative_num_comparisons_from_blocking_rules_chart_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart_chart</code>","text":"<p>At a glance</p> <p>Useful for: Counting the number of comparisons generated by Blocking Rules.</p> <p>API Documentation: cumulative_num_comparisons_from_blocking_rules_chart()</p> <p>What is needed to generate the chart? A <code>linker</code> with some data and a settings dictionary defining some Blocking Rules.</p>"},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>cumulative_num_comparisons_from_blocking_rules_chart</code> shows the count of pairwise comparisons generated by a set of blocking rules.</p>"},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Blocking rules are order dependent, therefore each bar in this chart shows the additional comparisons generated ontop of the previous blocking rules.</p> <p>For example, the chart above shows an exact match on <code>surname</code> generates an additional 1351 comparisons. If we reverse the order of the <code>surname</code> and <code>first_name</code> blocking rules:</p>"},{"location":"charts/cumulative_num_comparisons_from_blocking_rules_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>The main aim of this chart is to understand how many comparisons are generated by blocking rules that the Splink model will consider. The number of comparisons is the main primary driver of the amount of computational resource required for Splink model training, predictions etc. (i.e. how long things will take to run). </p> <p>The number of comparisons that are appropriate for a model varies. In general, if a model is taking hours to run (unless you are working with 100+ million records), it could be helpful to reduce the number of comparisons by defining more restrictive blocking rules. </p> <p>For instance, there are many people who could share the same <code>first_name</code> in the example above you may want to add an additonal requirement for a match on <code>dob</code> as well to reduce the number of records the model needs to consider.</p>"},{"location":"charts/m_u_parameters_chart.html","title":"m u parameters chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nlinker.m_u_parameters_chart()\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/m_u_parameters_chart.html#m_u_parameters_chart","title":"<code>m_u_parameters_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the m and u values generated by a Splink model.</p> <p>API Documentation: m_u_parameters_chart()</p> <p>What is needed to generate the chart? A trained Splink model.</p>"},{"location":"charts/m_u_parameters_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/m_u_parameters_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>m_u_parameters_chart</code> shows the results of a trained Splink model:</p> <ul> <li>The left chart shows the estimated m probabilities from the Splink model </li> <li>The right chart shows the estimated u probabilities from the Splink model.</li> </ul> <p>Each comparison within a model is represented in trained m and u values that have been estimated during the Splink model training for each comparison level.</p> What the chart tooltip shows"},{"location":"charts/m_u_parameters_chart.html#estimated-m-probability-tooltip","title":"Estimated m probability tooltip","text":"<p>The tooltip of the left chart shows information based on the comparison level bar that the user is hovering over, including:</p> <ul> <li>An explanation of the m probability for the comparison level.</li> <li>The name of the comparison and comparison level.</li> <li>The comparison level condition as an SQL statement.</li> <li>The m and u proability for the comparison level.</li> <li>The resulting bayes factor and match weight for the comparison level.</li> </ul>"},{"location":"charts/m_u_parameters_chart.html#estimated-u-probability-tooltip","title":"Estimated u probability tooltip","text":"<p>The tooltip of the right chart shows information based on the comparison level bar that the user is hovering over, including:</p> <ul> <li>An explanation of the u probability from the comparison level.</li> <li>The name of the comparison and comparison level.</li> <li>The comparison level condition as an SQL statement.</li> <li>The m and u proability for the comparison level.</li> <li>The resulting bayes factor and match weight for the comparison level.</li> </ul>"},{"location":"charts/m_u_parameters_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Each bar of the left chart shows the probability of a given comparison level when two records are a match. This can also be interpreted as the proportion of matching records which are allocated to the comparison level (as stated in the x axis label).</p> <p>Similarly, each bar of the right chart shows the probability of a given comparison level when two records are not a match. This can also be interpreted as the proportion of non-matching records which are allocated to the comparison level (as stated in the x axis label).</p> <p>Further Reading</p> <p>For a more comprehensive introduction to m and u probabilities, check out the Fellegi Sunter model topic guide.</p>"},{"location":"charts/m_u_parameters_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>As with the <code>match_weights_chart</code>, one of the most effective methods to assess a Splink model is to walk through each of the comparison levels of the <code>m_u_parameters_chart</code> and sense check the m and u probabilities that have been allocated by the model.</p> <p>For example, for all non-matching pairwise comparisons (which form the vast majority of all pairwise comparisons), it makes sense that the exact match and fuzzy levels occur very rarely. Furthermore, <code>dob</code> and <code>city</code> are lower cardinality features (i.e. have fewer possible values) than names so \"All other comparisons\" is less likely.</p> <p>If there are any m or u values that appear unusual, check out the values generated for each training session in the <code>parameter_estimate_comparisons_chart</code>.</p>"},{"location":"charts/m_u_parameters_chart.html#related-charts","title":"Related Charts","text":"<p><code>match weights chart</code></p> <p><code>parameter estimate comparisons chart</code></p>"},{"location":"charts/match_weights_chart.html","title":"match weights chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nlinker.match_weights_chart()\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/match_weights_chart.html#match_weights_chart","title":"<code>match_weights_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the whole Splink model definition.</p> <p>API Documentation: match_weights_chart()</p> <p>What is needed to generate the chart? A trained Splink model.</p>"},{"location":"charts/match_weights_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/match_weights_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>match_weights_chart</code> show the results of a trained Splink model. Each comparison within a model is represented in a bar chart, with a bar showing the evidence for two records being a match (i.e. match weight) for each comparison level.</p> What the chart tooltip shows <p></p> <p>The tooltip shows information based on the comparison level bar that the user is hovering over, including:</p> <ul> <li>The name of the comparison and comaprison level.</li> <li>The comparison level condition as an SQL statement.</li> <li>The m and u proability for the comparison level.</li> <li>The resulting bayes factor and match weight for the comparison level.</li> </ul>"},{"location":"charts/match_weights_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Each bar in the <code>match_weights_chart</code> shows the evidence of a match provided by each level in a Splink model (i.e. match weight). As such, the match weight chart provides a summary for the entire Splink model, as it shows the match weights for every type of comparison defined within the model.</p> <p>Any Splink score generated to compare two records will add up the evidence (i.e. match weights) for each comparison to come up with a final match weight score, which can then be converted into a probability of a match.</p> <p>The first bar chart is the Prior Match Weight, which is the . This can be thought of in the same way as the y-intercept of a simple regression model</p> <p>This chart is an aggregation of the <code>m_u_parameters_chart</code>. The match weight for a comparison level is simply \\(log_2(\\frac{m}{u})\\).</p>"},{"location":"charts/match_weights_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>Some heuristics to help assess Splink models with the <code>match_weights_chart</code>:</p>"},{"location":"charts/match_weights_chart.html#match-weights-gradually-reducing-within-a-comparison","title":"Match weights gradually reducing within a comparison","text":"<p>Comparison levels are order dependent, therefore they are constructed that the most \"similar\" levels come first and get gradually less \"similar\". As a result, we would generally expect that match weight will reduce as we go down the levels in a comparison. </p>"},{"location":"charts/match_weights_chart.html#very-similar-comparison-levels","title":"Very similar comparison levels","text":"<p>Comparisons are broken up into comparison levels to show different levels of similarity between records. As these levels are associated with different levels of similarity, we expect the amount of evidence (i.e. match weight) to vary between comparison levels. Two levels with the same match weight does not provide the model with any additional information which could make it perform better. </p> <p>Therefore, if two levels of a comparison return the same match weight, these should be combined into a single level.</p>"},{"location":"charts/match_weights_chart.html#very-different-comparison-levels","title":"Very different comparison levels","text":"<p>Levels that have a large variation between comparison levels have a significant impact on the model results. For example, looking at the <code>email</code> comparison in the chart above, the difference in match weight between an exact/fuzzy match and \"All other comparisons\" is &gt; 13, which is quite extreme. This generally happens with highly predictive features (e.g. email, national insurance number, social security number).</p> <p>If there are a number of highly predictive features, it is worth looking at simplifying your model using these more predictive features. In some cases, similar results may be obtained with a deterministic rather than a probabilistic linkage model.</p>"},{"location":"charts/match_weights_chart.html#logical-walk-through","title":"Logical Walk-through","text":"<p>One of the most effective methods to assess a splink model is to walk through each of the comparison levels of the <code>match_weights_chart</code> and sense check the amount of evidence (i.e. match weight) that has been allocated by the model.</p> <p>For example, in the chart above, we would expect records with the same <code>dob</code> to provide more evidence of a match that <code>first_name</code> or <code>surname</code>. Conversely, given how people can move location, we would expect that <code>city</code> would be less predictive than people's fixed, personally identifying characteristics like <code>surname</code>, <code>dob</code> etc.</p>"},{"location":"charts/match_weights_chart.html#anything-look-strange","title":"Anything look strange?","text":"<p>If anything still looks unusual, check out:</p> <ul> <li>the underlying m and u values in the <code>m_u_parameters_chart</code></li> <li>the values from each training session in the <code>parameter_estimate_comparisons_chart</code></li> </ul>"},{"location":"charts/match_weights_chart.html#related-charts","title":"Related Charts","text":"<p><code>m u parameters chart</code></p> <p><code>parameter estimate comparisons chart</code></p>"},{"location":"charts/missingness_chart.html","title":"missingness chart","text":"<pre><code>from splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\n\ndf = splink_datasets.historical_50k\nlinker = DuckDBLinker(df)\nlinker.missingness_chart()\n</code></pre>"},{"location":"charts/missingness_chart.html#missingness_chart","title":"<code>missingness_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at how many missing values are present in a dataset.</p> <p>API Documentation: missingness_chart()</p> <p>What is needed to generate the chart?: A <code>linker</code> with some data.</p>"},{"location":"charts/missingness_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/missingness_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>missingness_chart</code> show the proportion of missing values in the columns of a dataset.</p> What the chart tooltip shows <p></p> <p>The tooltip shows a number of statistics based on the bar that the user is hovering over, including:</p> <ul> <li>The column name.</li> <li>The count and percentage of null values in the column.</li> <li>The total number of rows in the column (including null values).</li> </ul>"},{"location":"charts/missingness_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>Each bar of the chart is the percentage of missing values in the columns of the selected dataset.</p>"},{"location":"charts/missingness_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>In general, columns with high missingness will not have significant impact on the final model results. This is because there will be so many cases where at least one of the records in any pairwise comparison has a missing value that few will actually get scored by the model. If it is a particularly strong identifier (e.g. National Insurance Number, Social Security Number) it can still be worth including a column with high missingness in your model.</p> <p>In cases with particularly high missingness there is a risk that there will not be sufficient pairwise examples to train the Splink model. In this case, it is generally advised to  remove the column from the model comparisons.</p>"},{"location":"charts/parameter_estimate_comparisons_chart.html","title":"parameter estimate comparisons chart","text":"<p>Work in Progress</p> <p>This page is currently under construction. </p> <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"email\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\n\nlinker.parameter_estimate_comparisons_chart()\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/parameter_estimate_comparisons_chart.html#parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the m and u value estimates across multiple Splink model training sessions.</p> <p>API Documentation: parameter_estimate_comparisons_chart()</p> <p>What is needed to generate the chart? A trained Splink model.</p>"},{"location":"charts/parameter_estimate_comparisons_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/parameter_estimate_comparisons_chart.html#what-the-chart-shows","title":"What the chart shows","text":""},{"location":"charts/parameter_estimate_comparisons_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":""},{"location":"charts/parameter_estimate_comparisons_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":""},{"location":"charts/parameter_estimate_comparisons_chart.html#related-charts","title":"Related Charts","text":"<p><code>m u parameters chart</code></p> <p><code>match weights chart</code></p>"},{"location":"charts/precision_recall_chart_from_labels_table.html","title":"precision recall chart from labels table","text":"<p>Work in Progress</p> <p>This page is currently under construction. </p> <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets, splink_dataset_labels\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.register_labels_table(df_labels)\n\nlinker.precision_recall_chart_from_labels_table(labels_table)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/precision_recall_chart_from_labels_table.html#precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table</code>","text":"<p>At a glance</p> <p>Useful for: </p> <p>API Documentation: precision_recall_chart_from_labels_table()</p> <p>What is needed to generate the chart? </p>"},{"location":"charts/precision_recall_chart_from_labels_table.html#worked-example","title":"Worked Example","text":""},{"location":"charts/precision_recall_chart_from_labels_table.html#what-the-chart-shows","title":"What the chart shows","text":""},{"location":"charts/precision_recall_chart_from_labels_table.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":""},{"location":"charts/precision_recall_chart_from_labels_table.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":""},{"location":"charts/profile_columns.html","title":"profile columns","text":"<pre><code>import altair as alt\nfrom splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\n\ndf = splink_datasets.historical_50k\nlinker = DuckDBLinker(df)\n\nlinker.profile_columns()\n</code></pre> What the chart tooltip shows <pre><code>linker.profile_columns(\"birth_place\")\n</code></pre> <p>Here we can see that \"london\" is the most common value, with many multiples more entires than the other values. In this case two records both having a <code>birth_place</code> of \"london\" gives far less evidence for a match than both having a rarer <code>birth_place</code> (e.g. \"felthorpe\").</p> <p>To take this skew into account, we can build Splink models with Term Frequency Adjustments. These adjustments will increase the amount of evidence for rare matching values and reduce the amount of evidence for common matching values.</p> <p>To understand how these work in more detail, check out the Term Frequency Adjustments Topic Guide</p> <pre><code>linker.profile_columns(\"dob\")\n</code></pre> <p>Here we can see a large skew towards dates which are the 1st January. We can narrow down the profiling to show the distribution of month and day to explore this further:</p> <pre><code>linker.profile_columns(\"substr(dob, 6, 10)\")\n</code></pre> <p>Here we can see that over 35% of all dates in this dataset are the 1st January. This is fairly common in manually entered datasets where if only the year of birth is known, people will generally enter the 1st January for that year.</p> <p>There are a number of ways to deal with this issue, but the most effective method we have found is to have a separate comparison level in a splink for matches on the 1st January. For example, there is a <code>separate_1st_january</code> parameter in the <code>date_comparison</code> function:</p> <pre><code>import splink.duckdb.comparison_template_library as ctl\n\ndob_comparison = ctl.date_comparison(\"dob\", separate_1st_january=True)\n</code></pre> <pre><code>print(dob_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Dob within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"dob\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"dob_l\" IS NULL OR \"dob_r\" IS NULL\n    - 'Exact match and 1st Jan' with SQL rule: (\"dob_l\" = \"dob_r\") AND (SUBSTR(dob_l, 6, 5) = '01-01')\n    - 'Exact match' with SQL rule: \"dob_l\" = \"dob_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"dob_l\", \"dob_r\") &lt;= 1\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"dob_l\",\n              \"dob_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"dob_l\",\n              \"dob_r\")) &lt;= 1\n\n    - 'Within 10 years' with SQL rule: \n            abs(date_diff('year', \"dob_l\",\n              \"dob_r\")) &lt;= 10\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre>"},{"location":"charts/profile_columns.html#profile_columns","title":"<code>profile_columns</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the distribution of values in columns. </p> <p>API Documentation: profile_columns()</p> <p>What is needed to generate the chart?: A <code>linker</code> with some data.</p>"},{"location":"charts/profile_columns.html#worked-example","title":"Worked Example","text":""},{"location":"charts/profile_columns.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>profile_columns</code> chart shows 3 charts for each selected column:</p> <ul> <li>The left chart shows the distribution of all values in the column. The width of each \"step\" represents the proportion of all (non-null) values of a given value while the height of each \"step\" gives the count of the same given value.</li> <li>The middle chart shows the counts of the ten most common values in the column. These correspond to the 10 leftmost \"steps\" in the left chart.</li> <li>The right chart shows the counts of the ten least common values in the column. These correspond to the 10 rightmost \"steps\" in the left chart.</li> </ul>"},{"location":"charts/profile_columns.html#left-chart","title":"Left chart:","text":"<p>This tooltip shows a number of statistics based on the column value of the \"step\" that the user is hovering over, including:</p> <ul> <li>The number of occurances of the given value.</li> <li>The precentile of the column value (excluding and including null values).</li> <li>The total number of rows in the column (excluding and including null values).</li> </ul>"},{"location":"charts/profile_columns.html#middle-and-right-chart","title":"Middle and right chart:","text":"<p>This tooltip shows a number of statistics based on the column value of the bar that the user is hovering over, including:</p> <ul> <li>The column value</li> <li>The count of the column value.</li> <li>The total number of rows in the column (excluding and including null values).</li> </ul>"},{"location":"charts/profile_columns.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>The distribution of values in your data is important for two main reasons:</p> <ol> <li> <p>Columns with higher cardinality (number of distinct values) are usually more useful for data linking. For instance, date of birth is a much stronger linkage variable than gender.</p> </li> <li> <p>The skew of values is important. If you have a <code>birth_place</code> column that has 1,000 distinct values, but 75% of them are London, this is much less useful for linkage than if the 1,000 values were equally distributed</p> </li> </ol>"},{"location":"charts/profile_columns.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>In an ideal world, all of the columns in datasets used for linkage would be high cardinality with a low skew (i.e. many distinct values that are evenly distributed). This is rarely the case with real-life datasets, but there a number of steps to extract the most predictive value, particularly with skewed data.</p>"},{"location":"charts/profile_columns.html#skewed-string-columns","title":"Skewed String Columns","text":"<p>Consider the skew of <code>birth_place</code> in our example:</p>"},{"location":"charts/profile_columns.html#skewed-date-columns","title":"Skewed Date Columns","text":"<p>Dates can also be skewed, but tend to be dealt with slightly differently.</p> <p>Consider the <code>dob</code> column from our example:</p>"},{"location":"charts/profile_columns.html#low-cardinality-columns","title":"Low cardinality columns","text":"<p>Unfortunately, there is not much that can be done to improve low cardinality data. Ultimately, they will provide some evidence of a match between records, but need to be used in conjunction with some more predictive, higher cardinality fields.</p>"},{"location":"charts/roc_chart_from_labels_table.html","title":"roc chart fromm labels table","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets, splink_dataset_labels\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.register_labels_table(df_labels)\n\nlinker.roc_chart_from_labels_table(labels_table)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/roc_chart_from_labels_table.html#roc_chart_from_labels_table_chart","title":"<code>roc_chart_from_labels_table_chart</code>","text":"<p>At a glance</p> <p>Useful for: Assessing the relationship between True and False Positive Rates.</p> <p>API Documentation: roc_chart_from_labels_table_chart()</p> <p>What is needed to generate the chart? A trained <code>linker</code> and a corresponding labelled dataset.</p>"},{"location":"charts/roc_chart_from_labels_table.html#worked-example","title":"Worked Example","text":""},{"location":"charts/roc_chart_from_labels_table.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The chart plots the True Positive Rate against False Positive Rate for clerically reviewed records. Each point on the curve reflects the choice of a match weight threshold for a match and the subsequent True/False Positive Rates.</p> What the chart tooltip shows <p></p> <p>The tooltip shows information based on the point on the curve that the user is hoverng over, including:</p> <ul> <li>The match weight and match probability threshold</li> <li>The False and True Positive Rate</li> <li>The count of True Positives, True Negatives, False Positives and False Negatives</li> <li>Precision, Recall and F1 score</li> </ul>"},{"location":"charts/roc_chart_from_labels_table.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>A ROC chart shows how the number of False Positives and False Negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.</p> <p>For a perfect classifier, we should be able to get 100% of True Positives without gaining any False Positives (see \"ideal class descriminator\" in the chart below).</p> <p>On the other hand, for a random classifier we would expect False Positives and False Negatives to be roughly equal (see \"no predictive value\" in the chart below).</p> <p>In reality, most models sit somethere between these two extremes.</p> <p></p>"},{"location":"charts/roc_chart_from_labels_table.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>If the ROC curve resembles the \"No predictive value\" example above, your model is not performing very well. In this case, it is worth reassessing your modesl (comparisons, comparison levels, blocking rules etc.) to see if there is a better solution.</p> <p>It is also worth considering the impact of your labelled data on this chart. For labels, it is important to consider a variety of pairwise comparisons (which includes True/False Positives and True/False Negatives). For example, it you only label pairwise comparisons that are true matches, this chart will not give any insights (as there will be no False Positives). </p>"},{"location":"charts/template.html","title":"Template","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n</code></pre> What the chart tooltip shows"},{"location":"charts/template.html#xxxxx_chart","title":"<code>XXXXX_chart</code>","text":"<p>At a glance</p> <p>Useful for: </p> <p>API Documentation: XXXXXX_chart()</p> <p>What is needed to generate the chart? </p>"},{"location":"charts/template.html#worked-example","title":"Worked Example","text":""},{"location":"charts/template.html#what-the-chart-shows","title":"What the chart shows","text":""},{"location":"charts/template.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":""},{"location":"charts/template.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":""},{"location":"charts/tf_adjustment_chart.html","title":"tf adjustment chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", term_frequency_adjustments = True),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nlinker.tf_adjustment_chart(\"first_name\", vals_to_include = [\"Robert\", \"Grace\"])\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>/Users/rosskennedy/splink/splink/linker.py:3126: UserWarning: Values ['Robert', 'Grace'] from `vals_to_include` were not found in the dataset so are not included in the chart.\n  return tf_adjustment_chart(\n</code>\n</pre> What the tooltip shows"},{"location":"charts/tf_adjustment_chart.html#tf_adjustment_chart","title":"<code>tf_adjustment_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the impact of Term Frequency Adjustments on Match Weights.</p> <p>API Documentation: tf_adjustment_chart()</p> <p>What is needed to generate the chart?: A trained Splink model, including comparisons with term frequency adjustments.</p>"},{"location":"charts/tf_adjustment_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/tf_adjustment_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>tf_adjustment_chart</code> shows the impact of Term Frequency Adjustments on the Match Weight of a comparison. It is made up of two charts for each selected comparison:</p> <ul> <li>The left chart shows the match weight for two records with a matching <code>first_name</code> including a term frequency adjustment. The black horizontal line represents the base match weight (i.e. with no term frequency adjustment applied). By default this chart contains the 10 most frequent and 10 least frequent values in a comparison as well as any values assigned in the <code>vals_to_include</code> parameter.</li> <li>The right chart shows the distribution of match weights across all of the values of <code>first_name</code>.</li> </ul>"},{"location":"charts/tf_adjustment_chart.html#left-chart","title":"Left chart","text":"<p>The tooltip shows a number of statistics based on the column value of the point theat the user is hovering over, including:</p> <ul> <li>The column value</li> <li>The base match weight (i.e. with no term frequency adjustment) for a match on the column.</li> <li>The term frequency adjustment for the column value.</li> <li>The final match weight (i.e. the combined base match weight and term frequency adjustment)</li> </ul>"},{"location":"charts/tf_adjustment_chart.html#right-chart","title":"Right chart","text":"<p>The tooltip shows a number of statistics based on the bar that the user is hovering over, including:</p> <ul> <li>The final match weight bucket (in steps of 0.5).</li> <li>The number of records with a final match weight in the final match weight bucket.</li> </ul>"},{"location":"charts/tf_adjustment_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>The most common terms (on the left of the first chart) will have a negative term frequency adjustment and the values on the chart and represent the lowest match weight for a match for the selected comparison. Conversely, the least common terms (on the right of the first chart) will have a positive term frequency adjustment and the values on the chart represent the highest match weight for a match for the selected comparison.</p> <p>Given that the first chart only shows the most and least frequently occuring values, the second chart is provided to show the distribution of final match weights (including term frequency adjustments) across all values in the dataset.</p>"},{"location":"charts/tf_adjustment_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>There are no direct actions that need to be taken as a result of this chart. It is intended to give the user an indication of the size of the impact of Term Frequency Adjustments on comparisons, as seen in the Waterfall Chart.</p>"},{"location":"charts/unlinkables_chart.html","title":"unlinkables chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nlinker.unlinkables_chart()\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/unlinkables_chart.html#unlinkables_chart","title":"<code>unlinkables_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at how many records have insufficient information to be linked to themselves.</p> <p>API Documentation: unlinkables_chart()</p> <p>What is needed to generate the chart? A trained Splink model</p>"},{"location":"charts/unlinkables_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/unlinkables_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>unlinkables_chart</code> shows the proportion of records with insufficient information to be matched to themselves at differing match thresholds.</p> What the chart tooltip shows <p></p> <p>This tooltip shows a number of statistics based on the match weight of the selected point of the line, including:</p> <ul> <li>The chosen match weight and corresponding match probability.</li> <li>The proportion of records of records that cannot be linked to themselves given the chosen match weight threshold for a match.</li> </ul>"},{"location":"charts/unlinkables_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>This chart gives an indication of both data quality and/or model predictiveness within a Splink model. If a high proportion of records are not linkable to themselves at a low match threshold (e.g. 0 match weight/50% probability) we can conclude that either/or:</p> <ul> <li>the data quality is low enough such that a significant proportion of records are unable to be linked to themselves</li> <li>the parameters of the Splink model are such that features have not been assigned enough weight, and therefore will not perform well</li> </ul> <p>This chart also gives an indication of the number of False Negatives (i.e. missed links) at a given threshold, assuming sufficient data quality. For example:</p> <ul> <li>we know that a record should be linked to itself, so seeing that a match weight \\(\\approx\\) 10 gives 16% of records unable to link to themselves</li> <li>exact matches generally provide the strongest matches, therefore, we can expect that any \"fuzzy\" matches to have lower match scores. As a result, we can deduce that the propoertion of False Negatives will be higher than 16%.</li> </ul>"},{"location":"charts/unlinkables_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>If the level of unlinkable records is extremely high at low match weight thresholds, you have a poorly performing model. This may be an issue that can be resolved by tweaking the models comparisons, but if the poor performance is primarily down to poor data quality, there is very little that can be done to improve the model.</p> <p>When interpretted as an indicator of False Negatives, this chart can be used to establish an upper bound for match weight, depending on the propensity for False Negatives in the particular use case.</p>"},{"location":"charts/waterfall_chart.html","title":"waterfall chart","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", term_frequency_adjustments=True),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\"),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n    \"retain_matching_columns\":True,\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\ndf_predictions = linker.predict(threshold_match_probability=0.2)\nrecords_to_view  = df_predictions.as_record_dict(limit=5)\n\nlinker.waterfall_chart(records_to_view, filter_nulls=False)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre>"},{"location":"charts/waterfall_chart.html#waterfall_chart","title":"<code>waterfall_chart</code>","text":"<p>At a glance</p> <p>Useful for: Looking at the breakdown of the match weight for a pair of records.</p> <p>API Documentation: waterfall_chart()</p> <p>What is needed to generate the chart? A trained Splink model</p>"},{"location":"charts/waterfall_chart.html#worked-example","title":"Worked Example","text":""},{"location":"charts/waterfall_chart.html#what-the-chart-shows","title":"What the chart shows","text":"<p>The <code>waterfall_chart</code> shows the amount of evidence of a match that is provided by each comparison for a pair of records. Each bar represents a comparison and the corresponding amount of evidence (i.e. match weight) of a match for the pair of values displayed above the bar.</p> What the chart tooltip shows <p></p> <p>The tooltip contains information based on the bar that the user is hovering over, including:</p> <ul> <li>The comparison column (or columns)</li> <li>The column values from the pair of records being compared</li> <li>The comparison level as a label, SQL statement and the corresponding comparison vector value</li> <li>The bayes factor (i.e. how many times more likely is a match based on this evidence)</li> <li>The match weight for the comparison level</li> <li>The cumulative match probability from the chosen comparison and all of the previous comparisons.</li> </ul>"},{"location":"charts/waterfall_chart.html#how-to-interpret-the-chart","title":"How to interpret the chart","text":"<p>The first bar (labelled \"Prior\") is the match weight if no additional knowledge of features is taken into account, and can be thought of as similar to the y-intercept in a simple regression.</p> <p>Each subsequent bar shows the match weight for a comparison. These bars can be positive or negative depending on whether the given comparison gives positive or negative evidence for the two records being a match.</p> <p>Additional bars are added for comparisons with term frequency adjustments. For example, the chart above has term frequency adjustments for <code>first_name</code> so there is an extra <code>tf_first_name</code> bar showing how the frequency of a given name impacts the amount of evidence for the two records being a match.</p> <p>The final bar represents total match weight for the pair of records. This match weight can also be translated into a final match probablility, and the corresponding match probability is shown on the right axis (note the logarithmic scale).</p>"},{"location":"charts/waterfall_chart.html#actions-to-take-as-a-result-of-the-chart","title":"Actions to take as a result of the chart","text":"<p>This chart is useful for spot checking pairs of records to see if the Splink model is behaving as expected.</p> <p>If a pair of records look like they are incorrectly being assigned as a match/non-match, it is a sign that the Splink model is not working optimally. If this is the case, it is worth revisiting the model training step. </p> <p>Some common scenarios include:</p> <ul> <li> <p>If a comparison isn't capturing a specific edge case (e.g. fuzzy match), add a comparison level to capture this case and retrain the model.</p> </li> <li> <p>If the match weight for a comparison is looking unusual, refer to the <code>match_weights_chart</code> to see the match weight in context with the rest of the comparison levels within that comparison. If it is still looking unusual, you can dig deeper with the <code>parameter_estimate_comparisons_chart</code> to see if the model training runs are consistent. If there is a lot of variation between model training sessions, this can suggest some instability in the model. In this case, try some different model training rules and/or comparison levels.</p> </li> <li> <p>If the \"Prior\" match weight is too small or large compared to the match weight provided by the comparisons, try some different determininstic rules and recall inputs to the <code>estimate_probability_two_records_match</code> function.</p> </li> <li> <p>If you are working with a model with term frequency adjustments and want to dig deeper into the impact of term frequency on the model as a whole (i.e. not just for a single pairwise comparison), check out the <code>tf_adjustment_chart</code>.</p> </li> </ul>"},{"location":"demos/examples/examples_index.html","title":"Introduction","text":"","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#example-notebooks","title":"Example Notebooks","text":"<p>This section provides a series of examples to help you get started with Splink. You can find the underlying notebooks in the demos folder of the Splink repo.</p> <p>You can try these demos live in your web browser using the following link:</p> <p></p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#duckdb-examples","title":"DuckDB examples","text":"","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#entity-type-persons","title":"Entity type: Persons","text":"<p>Deduplicating 50,000 records of realistic data based on historical persons</p> <p>Using the <code>link_only</code> setting to link, but not dedupe, two datasets</p> <p>Real time record linkage</p> <p>Accuracy analysis and ROC charts using a ground truth (cluster) column</p> <p>Estimating m probabilities from pairwise labels</p> <p>Deduplicating 50,000 records with Deterministic Rules</p> <p>Deduplicating the febrl3 dataset. Note this dataset comes from febrl, as referenced in A.2 here and replicated here.</p> <p>Linking the febrl4 datasets. As above, these datasets are from febrl, replicated here.</p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#entity-type-financial-transactions","title":"Entity type: Financial transactions","text":"<p>Linking financial transactions</p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#pyspark-examples","title":"PySpark examples","text":"<p>Deduplication of a small dataset using PySpark. Entity type is persons.</p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#athena-examples","title":"Athena examples","text":"<p>Deduplicating 50,000 records of realistic data based on historical persons</p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/examples_index.html#sqlite-examples","title":"SQLite examples","text":"<p>Deduplicating 50,000 records of realistic data based on historical persons</p>","tags":["Examples","DuckDB","Spark","Athena"]},{"location":"demos/examples/athena/deduplicate_50k_synthetic.html","title":"Deduplicate 50k rows historical persons","text":"<pre><code>from splink.athena.athena_linker import AthenaLinker\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nimport pandas as pd\npd.options.display.max_rows = 1000\ndf = pd.read_parquet(\"./data/historical_figures_with_errors_50k.parquet\")\n</code></pre> <p>Create a boto3 session to be used within the linker</p> <pre><code>import boto3\nmy_session = boto3.Session(region_name=\"eu-west-1\")\n</code></pre> <pre><code># Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n}\n</code></pre> <pre><code># Set the output bucket and the additional filepath to write outputs to\n############################################\n# EDIT THESE BEFORE ATTEMPTING TO RUN THIS #\n############################################\n\nbucket = \"my_s3_bucket\"\ndatabase = \"my_athena_database\"\nfilepath = \"athena_testing\"  # file path inside of your bucket\naws_filepath = f\"s3://{bucket}/{filepath}\"\n\n# Sessions are generated with a unique ID...\nlinker = AthenaLinker(\n    input_table_or_tables=df,\n    boto3_session=my_session,\n    # the bucket to store splink's parquet files\n    output_bucket=bucket,\n    # the database to store splink's outputs\n    output_database=database,\n    # folder to output data to\n    output_filepath=filepath,  \n    # table name within your database\n    # if blank, it will default to __splink__input_table_randomid\n    input_table_aliases=\"__splink__testings\",\n    settings_dict=settings,\n)\n\nlinker.profile_columns(\n    [\"first_name\", \"postcode_fake\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <pre><code>linker.drop_all_tables_created_by_splink(delete_s3_folders=True)\n</code></pre> <pre><code>import splink.athena.athena_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n    \"comparisons\": [\n        cl.levenshtein_at_thresholds(\"first_name\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"surname\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"dob\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"postcode_fake\", 2,term_frequency_adjustments=True),\n        cl.exact_match(\"birth_place\", term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"max_iterations\": 10,\n    \"em_convergence\": 0.01\n}\n</code></pre> <pre><code># Write our dataframe to s3/our backing database\nimport awswrangler as wr\nwr.s3.to_parquet(\n    df,  # pandas dataframe\n    path=f\"{aws_filepath}/historical_figures_with_errors_50k\",\n    dataset=True,\n    database=database,\n    table=\"historical_figures_with_errors_50k\",\n    mode=\"overwrite\",\n    compression=\"snappy\",\n)\n</code></pre> <pre><code># Initialise our linker with historical_figures_with_errors_50k from our database\nlinker = AthenaLinker(\n    input_table_or_tables=\"historical_figures_with_errors_50k\",  \n    settings_dict=settings,\n    boto3_session=my_session,\n    output_bucket=bucket,  # the bucket to store splink's parquet files \n    output_database=database,  # the database to store splink's outputs\n    output_filepath=filepath  # folder to output data to\n)\n</code></pre> <pre><code>linker.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000136.\nThis means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match.  With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - postcode_fake (no m values are trained).\n    - birth_place (no m values are trained).\n    - occupation (no m values are trained).\n</code>\n</pre> <pre><code>blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_names = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.533 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.0419 in the m_probability of birth_place, level `All other comparisons`\nIteration 3: Largest change in params was -0.0154 in the m_probability of birth_place, level `All other comparisons`\nIteration 4: Largest change in params was 0.00489 in the m_probability of birth_place, level `Exact match`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <pre><code>blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.356 in the m_probability of first_name, level `Exact match`\nIteration 2: Largest change in params was 0.0401 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.00536 in the m_probability of first_name, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre><code>df_predict = linker.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 19.465751 0.999999 Q5536981-1 Q5536981-4 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 1 33.572592 1.000000 Q5536981-1 Q5536981-5 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 2 33.572592 1.000000 Q5536981-1 Q5536981-6 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 3 33.572592 1.000000 Q5536981-1 Q5536981-7 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 4 22.025628 1.000000 Q5536981-1 Q5536981-8 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 <p>5 rows \u00d7 47 columns</p> <p>You can also view rows in this dataset as a waterfall chart as follows:</p> <pre><code>from splink.charts import waterfall_chart\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.waterfall_chart(records_to_plot, filter_nulls=False)\n</code></pre> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 642\nCompleted iteration 2, root rows count 119\nCompleted iteration 3, root rows count 35\nCompleted iteration 4, root rows count 6\nCompleted iteration 5, root rows count 0\n</code>\n</pre> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"dashboards/50k_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./dashboards/50k_cluster.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\",match_weight_round_to_nearest=0.02)\n</code></pre> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.999,\n    include_false_negatives=False,\n    include_false_positives=True,\n).as_record_dict()\nlinker.waterfall_chart(records)\n</code></pre> <pre><code># Some of the false negatives will be because they weren't detected by the blocking rules\nrecords = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.5,\n    include_false_negatives=True,\n    include_false_positives=False,\n).as_record_dict(limit=50)\n\nlinker.waterfall_chart(records)\n</code></pre> <p>And finally, clean up all tables except <code>df_predict</code></p> <pre><code>linker.drop_tables_in_current_splink_run(tables_to_exclude=df_predict)\n</code></pre>"},{"location":"demos/examples/athena/deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","title":"Linking a dataset of real historical persons","text":"<p>In this example, we deduplicate a more realistic dataset. The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors introduced.</p>"},{"location":"demos/examples/athena/deduplicate_50k_synthetic.html#athenalinker-setup","title":"AthenaLinker Setup","text":"<p>To work nicely with Athena, you need to outline various filepaths, buckets and the database(s) you wish to interact with.</p> <p>The AthenaLinker has three required inputs: * input_table_or_tables - the input table to use for linking. This can either be a table in a database or a pandas dataframe * output_database - the database to output all of your splink tables to. * output_bucket - the s3 bucket you wish any parquet files produced by splink to be output to.</p> <p>and two optional inputs: * output_filepath - the s3 filepath to output files to. This is an extension of output_bucket and dictate the full filepath your files will be output to. * input_table_aliases - the name of your table within your database, should you choose to use a pandas df as an input.</p>"},{"location":"demos/examples/athena/deduplicate_50k_synthetic.html#perform-garbage-collection","title":"Perform garbage collection","text":"<p>To clean up your selected database and its backing data on AWS, you can use <code>drop_all_tables_created_by_splink</code>. This allows splink to automatically search for any tables prefixed with <code>__splink__df...</code> in your given database and delete them.</p> <p>Alternatively, if you want to delete splink tables from another database that you didn't select in the initialisation step, you can run <code>drop_splink_tables_from_database(database_name)</code>.</p>"},{"location":"demos/examples/athena/deduplicate_50k_synthetic.html#you-can-also-read-data-directly-from-a-database","title":"You can also read data directly from a database","text":"<p>Simply add your data to your database and enter the name of the resulting table into the linker object.</p> <p>This can be done with either: &gt; wr.catalog.create_parquet_table(...)</p> <p>or</p> <p>&gt; wr.s3.to_parquet(...)</p> <p>See the awswrangler API for more info.</p>"},{"location":"demos/examples/duckdb/accuracy_analysis_from_labels_column.html","title":"Evaluation from ground truth column","text":"<pre><code>from splink.datasets import splink_datasets\nimport altair as alt\nalt.renderers.enable(\"html\")\n\ndf = splink_datasets.fake_1000\n\ndf.head(2)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 <pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.duckdb.blocking_rule_library import block_on\nimport splink.duckdb.comparison_template_library as ctl\nimport splink.duckdb.comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &amp;lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=5)\n</code></pre> <pre><code>session_dob = linker.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\nsession_email = linker.estimate_parameters_using_expectation_maximisation(block_on(\"email\"))\n</code></pre> <pre><code>linker.truth_space_table_from_labels_column(\n    \"cluster\", match_weight_round_to_nearest=0.1\n).as_pandas_dataframe(limit=5)\n</code></pre> truth_threshold match_probability row_count p n tp tn fp fn P_rate ... precision recall specificity npv accuracy f1 f2 f0_5 p4 phi 0 -24.3 4.841400e-08 4353.0 2031.0 2322.0 2031.0 0.0 2322.0 0.0 0.466575 ... 0.466575 1.000000 0.000000 1.000000 0.466575 0.636278 0.813898 0.522296 0.000000 0.000000 1 -23.8 6.846774e-08 4353.0 2031.0 2322.0 2030.0 0.0 2322.0 1.0 0.466575 ... 0.466452 0.999508 0.000000 0.000000 0.466345 0.636065 0.813562 0.522146 0.000000 -0.016208 2 -23.7 7.338190e-08 4353.0 2031.0 2322.0 2030.0 234.0 2088.0 1.0 0.466575 ... 0.492958 0.999508 0.100775 0.995745 0.520101 0.660270 0.829113 0.548560 0.286607 0.221379 3 -22.6 1.572975e-07 4353.0 2031.0 2322.0 2030.0 428.0 1894.0 1.0 0.466575 ... 0.517329 0.999508 0.184324 0.997669 0.564668 0.681780 0.842463 0.572573 0.427302 0.307690 4 -22.5 1.685873e-07 4353.0 2031.0 2322.0 2030.0 583.0 1739.0 1.0 0.466575 ... 0.538604 0.999508 0.251077 0.998288 0.600276 0.700000 0.853443 0.593324 0.510093 0.366792 <p>5 rows \u00d7 25 columns</p> <pre><code>linker.roc_chart_from_labels_column(\"cluster\")\n</code></pre> <pre><code>linker.precision_recall_chart_from_labels_column(\"cluster\")\n</code></pre> <pre><code># Plot some false positives\nlinker.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_pandas_dataframe(limit=5)\n</code></pre> clerical_match_score found_by_blocking_rules match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 1.0 False -24.284246 4.894558e-08 417 418 Florence Brown 0 0.216572 ... 0.00123 0.429162 1.0 fb@reose.cem f@b@reese.com 0 0.001067 108 108 2 1 1.0 False -22.077164 2.260015e-07 796 797 Taylor NaN -1 1.000000 ... 0.00738 0.429162 1.0 jt40o@combs.net jt40@cotbs.nm 0 0.001067 201 201 2 2 1.0 False -19.750689 1.133573e-06 452 454 NaN Davies -1 1.000000 ... 0.01599 0.429162 1.0 rd@lewis.com idlewrs.cocm 0 0.001067 115 115 2 3 1.0 True -15.659150 1.932492e-05 594 595 Grace Grace 4 85.509553 ... 0.00123 0.429162 1.0 gk@frey-robinson.org rgk@frey-robinon.org 0 0.001067 146 146 0 4 1.0 False -14.411473 4.588751e-05 150 151 Alfie Kelly 0 0.216572 ... 0.04920 0.429162 1.0 alfiekelly@walters.com NaN -1 1.000000 40 40 2 <p>5 rows \u00d7 32 columns</p> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_record_dict(limit=5)\n\nlinker.waterfall_chart(records)\n</code></pre>"},{"location":"demos/examples/duckdb/accuracy_analysis_from_labels_column.html#evaluation-when-you-have-fully-labelled-data","title":"Evaluation when you have fully labelled data","text":"<p>In this example, our data contains a fully-populated ground-truth column called <code>cluster</code> that enables us to perform accuracy analysis of the final model</p>"},{"location":"demos/examples/duckdb/deduplicate_50k_synthetic.html","title":"Deduplicate 50k rows historical persons","text":"<pre><code>from splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\nimport altair as alt\n\nimport pandas as pd \npd.options.display.max_rows = 1000\ndf = splink_datasets.historical_50k\n</code></pre> <pre><code>from splink.duckdb.blocking_rule_library import block_on\n\n# Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on([\"first_name\", \"surname\"]),\n        block_on([\"surname\", \"dob\"]),\n        block_on([\"first_name\", \"dob\"]),\n        block_on([\"postcode_fake\", \"first_name\"]),\n    ],\n}\nlinker = DuckDBLinker(df, settings)\n\nlinker.profile_columns(\n    [\"first_name\", \"postcode_fake\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <pre><code>import splink.duckdb.comparison_template_library as ctl\nimport splink.duckdb.comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on([\"first_name\", \"surname\"]),\n        block_on([\"surname\", \"dob\"]),\n        block_on([\"first_name\", \"dob\"]),\n        block_on([\"postcode_fake\", \"first_name\"]),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", term_frequency_adjustments=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments=True),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True, invalid_dates_as_null=True),\n        ctl.postcode_comparison(\"postcode_fake\"),\n        cl.exact_match(\"birth_place\", term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"max_iterations\": 10,\n    \"em_convergence\": 0.01\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>linker.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000136.\nThis means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match.  With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - postcode_fake (no m values are trained).\n    - birth_place (no m values are trained).\n    - occupation (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = block_on([\"first_name\", \"surname\"])\ntraining_session_names = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\n(l.\"first_name\" = r.\"first_name\") AND (l.\"surname\" = r.\"surname\")\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.52 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.0446 in the m_probability of birth_place, level `All other comparisons`\nIteration 3: Largest change in params was -0.0185 in the m_probability of birth_place, level `All other comparisons`\nIteration 4: Largest change in params was 0.00717 in the m_probability of birth_place, level `Exact match`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = block_on(\"dob\")\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"dob\" = r.\"dob\"\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nIteration 1: Largest change in params was -0.362 in the m_probability of first_name, level `Exact match first_name`\nIteration 2: Largest change in params was 0.0426 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.00794 in the m_probability of surname, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>The final match weights can be viewed in the match weights chart:</p> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre><code>df_predict = linker.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 12.731320 0.999853 Q2296770-1 Q2296770-14 thomas thomas 4 0.028667 0.028667 43.19637 ... 1.000000 1.0 politician politician 1 0.088932 0.088932 25.927656 0.388868 0 1 21.634289 1.000000 Q2296770-3 Q2296770-7 tom tom 4 0.012948 0.012948 43.19637 ... 0.159229 1.0 politician NaN -1 0.088932 NaN 1.000000 1.000000 0 2 9.131145 0.998220 Q2296770-10 Q2296770-14 thomas thomas 4 0.028667 0.028667 43.19637 ... 1.000000 1.0 politician politician 1 0.088932 0.088932 25.927656 0.388868 0 3 13.443772 0.999910 Q2296770-13 Q2296770-7 tom tom 4 0.012948 0.012948 43.19637 ... 0.159229 1.0 politician NaN -1 0.088932 NaN 1.000000 1.000000 0 4 35.771776 1.000000 Q1443188-1 Q1443188-3 frank frank 4 0.006335 0.006335 43.19637 ... 0.159229 1.0 liturgist liturgist 1 0.000237 0.000237 25.927656 145.760504 0 <p>5 rows \u00d7 41 columns</p> <p>You can also view rows in this dataset as a waterfall chart as follows:</p> <pre><code>from splink.charts import waterfall_chart\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.waterfall_chart(records_to_plot, filter_nulls=False)\n</code></pre> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 671\nCompleted iteration 2, root rows count 147\nCompleted iteration 3, root rows count 43\nCompleted iteration 4, root rows count 12\nCompleted iteration 5, root rows count 3\nCompleted iteration 6, root rows count 2\nCompleted iteration 7, root rows count 1\nCompleted iteration 8, root rows count 0\n</code>\n</pre> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"dashboards/50k_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./dashboards/50k_cluster.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\",match_weight_round_to_nearest=0.02)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.999,\n    include_false_negatives=False,\n    include_false_positives=True,\n).as_record_dict()\nlinker.waterfall_chart(records)\n</code></pre> <pre><code># Some of the false negatives will be because they weren't detected by the blocking rules\nrecords = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.5,\n    include_false_negatives=True,\n    include_false_positives=False,\n).as_record_dict(limit=50)\n\nlinker.waterfall_chart(records)\n</code></pre>"},{"location":"demos/examples/duckdb/deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","title":"Linking a dataset of real historical persons","text":"<p>In this example, we deduplicate a more realistic dataset.  The data is based on historical persons scraped from wikidata.  Duplicate records are introduced with a variety of errors introduced.</p>"},{"location":"demos/examples/duckdb/deterministic_dedupe.html","title":"Deterministic dedupe","text":"<pre><code>from splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\nimport altair as alt\nalt.renderers.enable('html')\n\nimport pandas as pd \npd.options.display.max_rows = 1000\ndf = splink_datasets.historical_50k\ndf.head()\n</code></pre> unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon tq13 8df male politician 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8hu None politician 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df None politician <p>When defining the settings object, simply pass your deterministic rules into <code>blocking_rules_to_generate_predictions</code>. </p> <p>For a deterministic linkage, the entire linkage methodology is based on these rules, so there is no need to define <code>comparisons</code> nor any other parameters required for model training in a probabilistic model.</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\n\n# Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on([\"first_name\", \"surname\", \"dob\"]),\n        block_on([\"surname\", \"dob\", \"postcode_fake\"]),\n        block_on([\"first_name\", \"dob\", \"occupation\"]),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\nlinker = DuckDBLinker(df, settings)\n\nlinker.debug_mode = False\n</code></pre> <p>Once the <code>linker</code> object is defined, you can profile the dataset columns.</p> <pre><code>linker.profile_columns(\n    [\"first_name\", \"surname\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <p>In a deterministic linkage, the blocking rules chart shows how many records have been matched by each of the deterministic rules.</p> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <p>The results of the linkage can be viewed with the <code>deterministic_link</code> function.</p> <pre><code>df_predict = linker.deterministic_link()\ndf_predict.as_pandas_dataframe().head()\n</code></pre> unique_id_l unique_id_r first_name_l first_name_r surname_l surname_r occupation_l occupation_r postcode_fake_l postcode_fake_r dob_l dob_r match_key match_probability 0 Q2296770-1 Q2296770-6 thomas thomas chudleigh chudleigh politician politician tq13 8df tq13 8df 1630-08-01 1630-08-01 0 1.0 1 Q2296770-2 Q2296770-6 thomas thomas chudleigh chudleigh politician politician tq13 8df tq13 8df 1630-08-01 1630-08-01 0 1.0 2 Q2296770-3 Q2296770-7 tom tom chudleigh chudleigh politician NaN tq13 8df tq13 8df 1630-08-01 1630-08-01 0 1.0 3 Q2296770-4 Q2296770-6 thomas thomas chudleigh chudleigh politician politician tq13 8hu tq13 8df 1630-08-01 1630-08-01 0 1.0 4 Q2296770-5 Q2296770-6 thomas thomas chudleigh chudleigh politician politician tq13 8df tq13 8df 1630-08-01 1630-08-01 0 1.0 <p>Which can be used to generate clusters. </p> <p>Note, for deterministic linkage, each comparison has been assigned a match probability of 1, so to generate clusters, set <code>threshold_match_probability=1</code> in the <code>cluster_pairwise_predictions_at_threshold</code> function.</p> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=1)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 94\nCompleted iteration 2, root rows count 10\nCompleted iteration 3, root rows count 0\n</code>\n</pre> <pre><code>clusters.as_pandas_dataframe(limit=5)\n</code></pre> cluster_id unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q33436042-4 Q33436042-4 Q33436042 charlie louis wiliam merlin charlie merlin charlie merlin 1822-01-01 radstock NaN male NaN 1 Q7791916-15 Q7791916-15 Q7791916 tom longman tom longman tom longman 1698-01-01 bristol bs5 6rq male publisher 2 Q97991018-1 Q97991018-3 Q97991018 john hare john hare john hare 1857-05-31 canterbury ct4 6jr male religious 3 Q363965-1 Q363965-3 Q363965 robert t. a. innes robert innes robert innes 1861-11-10 edinburgh eh3 5jz male astronomer 4 Q457399-4 Q457399-4 Q457399 charles eliot charles eliot charles eliot 1862-01-09 sibford gower ox15 6pr male NaN <p>These results can then be passed into the <code>Cluster Studio Dashboard</code>.</p> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"dashboards/50k_deterministic_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./dashboards/50k_deterministic_cluster.html\", width=\"100%\", height=1200\n)  \n</code></pre>"},{"location":"demos/examples/duckdb/deterministic_dedupe.html#linking-a-dataset-of-real-historical-persons-with-deterrministic-rules","title":"Linking a dataset of real historical persons with Deterrministic Rules","text":"<p>While Splink is primarily a tool for probabilistic records linkage, there is functionality to perform deterministic (i.e. rules based) linkage.</p> <p>In this example, we deduplicate a more realistic dataset.  The data is based on historical persons scraped from wikidata.  Duplicate records are introduced with a variety of errors introduced. The probabilistic dedupe of the same dataset can be found at <code>Deduplicate 50k rows historical persons</code>.</p>"},{"location":"demos/examples/duckdb/febrl3.html","title":"Febrl3 Dedupe","text":"<pre><code>import pandas as pd\nimport altair as alt\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.febrl3\ndf = df.rename(columns=lambda x: x.strip())\n\ndf[\"cluster\"] = df[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\n\n# dob and ssn needs to be a string for fuzzy comparisons like levenshtein to be applied\ndf[\"date_of_birth\"] = df[\"date_of_birth\"].astype(str).str.strip()\ndf[\"date_of_birth\"] = df[\"date_of_birth\"].replace(\"\", None)\n\ndf[\"soc_sec_id\"] = df[\"soc_sec_id\"].astype(str).str.strip()\ndf[\"soc_sec_id\"] = df[\"soc_sec_id\"].replace(\"\", None)\n\ndf[\"postcode\"] = df[\"postcode\"].astype(str).str.strip()\ndf[\"postcode\"] = df[\"postcode\"].replace(\"\", None)\ndf.head(2)\n</code></pre> rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1496-org mitchell green 7 wallaby place delmar cleveland 2119 sa 19560409 1804974 rec-1496 1 rec-552-dup-3 harley mccarthy 177 pridhamstreet milton marsden 3165 nsw 19080419 6089216 rec-552 <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"dedupe_only\",\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>linker.missingness_chart()\n</code></pre> <pre><code>linker.profile_columns(list(df.columns))\n</code></pre> <pre><code>from splink.duckdb.blocking_rule_library import block_on\n\nblocking_rules = [\n        block_on(\"soc_sec_id\"),\n        block_on(\"given_name\"),\n        block_on(\"surname\"),\n        block_on(\"date_of_birth\"),\n        block_on(\"postcode\"),\n]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\n\nsettings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        ctl.name_comparison(\"given_name\", term_frequency_adjustments=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments=True),\n        ctl.date_comparison(\"date_of_birth\", \n                            damerau_levenshtein_thresholds=[],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True,\n                            date_format=\"%Y%m%d\"),\n        cl.levenshtein_at_thresholds(\"soc_sec_id\", [2]),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n        cl.exact_match(\"postcode\", term_frequency_adjustments=True),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>deterministic_rules = [\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\",\n    \"l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.9)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000528.\nThis means that amongst all possible pairwise record comparisons, one in 1,893.56 are expected to match.  With 12,497,500 total possible comparisons, we expect a total of around 6,600.00 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - date_of_birth (no m values are trained).\n    - soc_sec_id (no m values are trained).\n    - street_number (no m values are trained).\n    - postcode (no m values are trained).\n</code>\n</pre> <pre><code>em_blocking_rule_1 = block_on(\"substr(date_of_birth,1,3)\")\nem_blocking_rule_2 = block_on(\"substr(postcode,1,2)\")\nsession_dob = linker.estimate_parameters_using_expectation_maximisation(em_blocking_rule_1)\nsession_postcode = linker.estimate_parameters_using_expectation_maximisation(em_blocking_rule_2)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nSUBSTR(l.\"date_of_birth\", 1, 3) = SUBSTR(r.\"date_of_birth\", 1, 3)\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - soc_sec_id\n    - street_number\n    - postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - date_of_birth\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 1: Largest change in params was -0.508 in probability_two_random_records_match\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 2: Largest change in params was -0.0388 in the m_probability of soc_sec_id, level `All other comparisons`\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 3: Largest change in params was -0.00602 in the m_probability of soc_sec_id, level `All other comparisons`\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 4: Largest change in params was -0.000955 in the m_probability of soc_sec_id, level `All other comparisons`\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 5: Largest change in params was -0.000155 in the m_probability of soc_sec_id, level `All other comparisons`\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>Iteration 6: Largest change in params was -2.55e-05 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 6 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - date_of_birth (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nSUBSTR(l.\"postcode\", 1, 2) = SUBSTR(r.\"postcode\", 1, 2)\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - date_of_birth\n    - soc_sec_id\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - postcode\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nIteration 1: Largest change in params was -0.227 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.0159 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 3: Largest change in params was -0.001 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 4: Largest change in params was -7.04e-05 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 4 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>results = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\")\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code>pred_errors_df = linker.prediction_errors_from_labels_column(\"cluster\").as_pandas_dataframe()\nlen(pred_errors_df)\npred_errors_df.head()\n</code></pre> clerical_match_score found_by_blocking_rules match_weight match_probability rec_id_l rec_id_r given_name_l given_name_r gamma_given_name tf_given_name_l ... postcode_l postcode_r gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode cluster_l cluster_r match_key 0 1.0 True -8.735600 0.002340 rec-1320-dup-1 rec-1320-dup-4 amber kexel 0 0.0044 ... 461 4061 0 0.0002 0.0006 0.216174 1.0 rec-1320 rec-1320 0 1 1.0 True -3.475139 0.082505 rec-941-dup-0 rec-941-dup-3 coby cobuy 3 0.0010 ... 3078 3088 0 0.0010 0.0008 0.216174 1.0 rec-941 rec-941 0 2 1.0 True -0.199954 0.465406 rec-1899-dup-0 rec-1899-org thomas matthew 0 0.0094 ... 6117 6171 0 0.0002 0.0004 0.216174 1.0 rec-1899 rec-1899 0 3 1.0 True -5.459610 0.022220 rec-1727-dup-1 rec-1727-org campblel joshua 0 0.0002 ... 3189 3198 0 0.0008 0.0008 0.216174 1.0 rec-1727 rec-1727 0 4 1.0 True -6.614888 0.010100 rec-75-dup-0 rec-75-dup-4 samara willing 0 0.0014 ... 3765 3756 0 0.0012 0.0004 0.216174 1.0 rec-75 rec-75 0 <p>5 rows \u00d7 45 columns</p> <pre><code>records = linker.prediction_errors_from_labels_column(\"cluster\").as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre>"},{"location":"demos/examples/duckdb/febrl3.html#deduplicating-the-febrl3-dataset","title":"Deduplicating the febrl3 dataset","text":"<p>See A.2  here and here for the source of this data</p>"},{"location":"demos/examples/duckdb/febrl4.html","title":"Febrl4 link-only","text":"<p>Firstly let's read in the data and have a little look at it</p> <pre><code>import pandas as pd\nimport altair as alt\nfrom splink.datasets import splink_datasets\nfrom IPython.display import IFrame\nalt.renderers.enable('html')\n\ndf_a = splink_datasets.febrl4a\ndf_b = splink_datasets.febrl4b\n\ndef prepare_data(data):\n    data = data.rename(columns=lambda x: x.strip())\n    data[\"cluster\"] = data[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\n    data[\"date_of_birth\"] = data[\"date_of_birth\"].astype(str).str.strip()\n    data[\"date_of_birth\"] = data[\"date_of_birth\"].replace(\"\", None)\n\n    data[\"soc_sec_id\"] = data[\"soc_sec_id\"].astype(str).str.strip()\n    data[\"soc_sec_id\"] = data[\"soc_sec_id\"].replace(\"\", None)\n\n    data[\"postcode\"] = data[\"postcode\"].astype(str).str.strip()\n    data[\"postcode\"] = data[\"postcode\"].replace(\"\", None)\n    return data\ndfs = [\n    prepare_data(dataset)\n    for dataset in [df_a, df_b]\n]\n\ndisplay(dfs[0].head(2))\ndisplay(dfs[1].head(2))\n</code></pre> rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1070-org michaela neumann 8 stanley street miami winston hills 4223 nsw 19151111 5304218 rec-1070 1 rec-1016-org courtney painter 12 pinkerton circuit bega flats richlands 4560 vic 19161214 4066625 rec-1016 rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-561-dup-0 elton 3 light setreet pinehill windermere 3212 vic 19651013 1551941 rec-561 1 rec-2642-dup-0 mitchell maxon 47 edkins street lochaoair north ryde 3355 nsw 19390212 8859999 rec-2642 <p>Next, to better understand which variables will prove useful in linking, we have a look at how populated each column is, as well as the distribution of unique values within each</p> <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nbasic_settings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"link_only\",\n    # NB as we are linking one-one, we know the probability that a random pair will be a match\n    # hence we could set:\n    # \"probability_two_random_records_match\": 1/5000,\n    # however we will not specify this here, as we will use this as a check that\n    # our estimation procedure returns something sensible\n}\n\nlinker = DuckDBLinker(dfs, basic_settings)\n</code></pre> <pre><code>linker.missingness_chart()\n</code></pre> <pre><code>cols_to_profile = list(dfs[0].columns)\ncols_to_profile = [col for col in cols_to_profile if col not in (\"rec_id\", \"cluster\")]\nlinker.profile_columns(cols_to_profile)\n</code></pre> <p>Next let's come up with some candidate blocking rules, which define which record comparisons are generated, and have a look at how many comparisons each will generate.</p> <p>For blocking rules that we use in prediction, our aim is to have the union of all rules cover all true matches, whilst avoiding generating so many comparisons that it becomes computationally intractable - i.e. each true match should have at least one of the following conditions holding.</p> <pre><code>blocking_rules = [\n    \"l.given_name = r.given_name AND l.surname = r.surname\",\n    \"l.date_of_birth = r.date_of_birth\",\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.state = r.state AND l.address_1 = r.address_1\",\n    \"l.street_number = r.street_number AND l.address_1 = r.address_1\",\n    \"l.postcode = r.postcode\",\n]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <p>The broadest rule, having a matching postcode, unsurpisingly gives the largest number of comparisons. For this small dataset we still have a very manageable number, but if it was larger we might have needed to include a further <code>AND</code> condition with it to break the number of comparisons further.</p> <p>Now we get the full settings by including the blocking rules, as well as deciding the actual comparisons we will be including in our model.</p> <p>We will define two models, each with a separate linker with different settings, so that we can compare performance. One will be a very basic model, whilst the other will include a lot more detail.</p> <pre><code>import splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nimport splink.duckdb.comparison_level_library as cll\n\n# the simple model only considers a few columns, and only two comparison levels for each\nsimple_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        cl.exact_match(\"given_name\", term_frequency_adjustments=True),\n        cl.exact_match(\"surname\", term_frequency_adjustments=True),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n# the detailed model considers more columns, using the information we saw in the exploratory phase\n# we also include further comparison levels to account for typos and other differences\ndetailed_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        ctl.name_comparison(\"given_name\", term_frequency_adjustments=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments=True),\n        ctl.date_comparison(\"date_of_birth\", \n                            damerau_levenshtein_thresholds=[],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True,\n                            date_format=\"%Y%m%d\"),\n        cl.damerau_levenshtein_at_thresholds(\"soc_sec_id\", [1, 2]),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n        cl.damerau_levenshtein_at_thresholds(\"postcode\", [1, 2], term_frequency_adjustments=True),\n        # we don't consider further location columns as they will be strongly correlated with postcode\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n\n\nlinker_simple = DuckDBLinker(dfs, simple_model_settings)\nlinker_detailed = DuckDBLinker(dfs, detailed_model_settings)\n</code></pre> <p>We need to furnish our models with parameter estimates so that we can generate results. We will focus on the detailed model, generating the values for the simple model at the end</p> <p>We can instead estimate the probability two random records match, and compare with the known value of 1/5000 = 0.0002, to see how well our estimation procedure works.</p> <p>To do this we come up with some deterministic rules - the aim here is that we generate very few false positives (i.e. we expect that the majority of records with at least one of these conditions holding are true matches), whilst also capturing the majority of matches - our guess here is that these two rules should capture 80% of all matches.</p> <pre><code>deterministic_rules = [\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\",\n]\n\nlinker_detailed.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000239.\nThis means that amongst all possible pairwise record comparisons, one in 4,185.85 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,972.50 matching pairs\n</code>\n</pre> <p>Even playing around with changing these deterministic rules, or the nominal recall leaves us with an answer which is pretty close to our known value</p> <p>Next we estimate <code>u</code> and <code>m</code> values for each comparison, so that we can move to generating predictions</p> <pre><code>linker_detailed.estimate_u_using_random_sampling(max_pairs=1e7)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - date_of_birth (no m values are trained).\n    - soc_sec_id (no m values are trained).\n    - street_number (no m values are trained).\n    - postcode (no m values are trained).\n</code>\n</pre> <p>When training the <code>m</code> values using expectation maximisation, we need somre more blocking rules to reduce the total number of comparisons. For each rule, we want to ensure that we have neither proportionally too many matches, or too few.</p> <p>We must run this multiple times using different rules so that we can obtain estimates for all comparisons - if we block on e.g. <code>date_of_birth</code>, then we cannot compute the <code>m</code> values for the <code>date_of_birth</code> comparison, as we have only looked at records where these match.</p> <pre><code>session_dob = linker_detailed.estimate_parameters_using_expectation_maximisation(\n    \"l.date_of_birth = r.date_of_birth\"\n)\nsession_pc = linker_detailed.estimate_parameters_using_expectation_maximisation(\n    \"l.postcode = r.postcode\"\n)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.date_of_birth = r.date_of_birth\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - soc_sec_id\n    - street_number\n    - postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - date_of_birth\n\nIteration 1: Largest change in params was -0.319 in probability_two_random_records_match\nIteration 2: Largest change in params was 0.0034 in the m_probability of given_name, level `All other comparisons`\nIteration 3: Largest change in params was 7.12e-05 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - date_of_birth (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.postcode = r.postcode\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - date_of_birth\n    - soc_sec_id\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - postcode\n\nIteration 1: Largest change in params was 0.0258 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 2: Largest change in params was 0.000194 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 3: Largest change in params was 1.23e-06 in the m_probability of date_of_birth, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>If we wish we can have a look at how our parameter estimates changes over these training sessions</p> <pre><code>session_dob.m_u_values_interactive_history_chart()\n</code></pre> <p>For variables that aren't used in the <code>m</code>-training blocking rules, we have two estimates --- one from each of the training sessions (see for example <code>street_number</code>). We can have a look at how the values compare between them, to ensure that we don't have drastically different values, which may be indicative of an issue.</p> <pre><code>linker_detailed.parameter_estimate_comparisons_chart()\n</code></pre> <p>We repeat our parameter estimations for the simple model in much the same fashion</p> <pre><code>linker_simple.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\nlinker_simple.estimate_u_using_random_sampling(max_pairs=1e7)\nsession_ssid = linker_simple.estimate_parameters_using_expectation_maximisation(\n    \"l.given_name = r.given_name\"\n)\nsession_pc = linker_simple.estimate_parameters_using_expectation_maximisation(\n    \"l.street_number = r.street_number\"\n)\nlinker_simple.parameter_estimate_comparisons_chart()\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000239.\nThis means that amongst all possible pairwise record comparisons, one in 4,185.85 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,972.50 matching pairs\n----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - street_number (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.given_name = r.given_name\n\nParameter estimates will be made for the following comparison(s):\n    - surname\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - given_name\n\nIteration 1: Largest change in params was -0.105 in the m_probability of surname, level `Exact match`\nIteration 2: Largest change in params was -0.0403 in the m_probability of surname, level `Exact match`\nIteration 3: Largest change in params was 0.0295 in the m_probability of surname, level `All other comparisons`\nIteration 4: Largest change in params was 0.0205 in the m_probability of surname, level `All other comparisons`\nIteration 5: Largest change in params was -0.0136 in the m_probability of surname, level `Exact match`\nIteration 6: Largest change in params was -0.00885 in the m_probability of surname, level `Exact match`\nIteration 7: Largest change in params was -0.00571 in the m_probability of surname, level `Exact match`\nIteration 8: Largest change in params was 0.00368 in the m_probability of surname, level `All other comparisons`\nIteration 9: Largest change in params was 0.00237 in the m_probability of surname, level `All other comparisons`\nIteration 10: Largest change in params was -0.00154 in the m_probability of surname, level `Exact match`\nIteration 11: Largest change in params was -0.000999 in the m_probability of surname, level `Exact match`\nIteration 12: Largest change in params was -0.000651 in the m_probability of surname, level `Exact match`\nIteration 13: Largest change in params was -0.000425 in the m_probability of surname, level `Exact match`\nIteration 14: Largest change in params was 0.000278 in the m_probability of surname, level `All other comparisons`\nIteration 15: Largest change in params was 0.000182 in the m_probability of surname, level `All other comparisons`\nIteration 16: Largest change in params was -0.00012 in the m_probability of surname, level `Exact match`\nIteration 17: Largest change in params was -7.86e-05 in the m_probability of surname, level `Exact match`\n\nEM converged after 17 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.street_number = r.street_number\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - street_number\n\nIteration 1: Largest change in params was 0.0728 in the m_probability of given_name, level `All other comparisons`\nIteration 2: Largest change in params was -0.0472 in the m_probability of given_name, level `Exact match`\nIteration 3: Largest change in params was 0.031 in the m_probability of given_name, level `All other comparisons`\nIteration 4: Largest change in params was -0.0181 in the m_probability of given_name, level `Exact match`\nIteration 5: Largest change in params was -0.01 in the m_probability of given_name, level `Exact match`\nIteration 6: Largest change in params was -0.00546 in the m_probability of given_name, level `Exact match`\nIteration 7: Largest change in params was 0.00301 in the m_probability of given_name, level `All other comparisons`\nIteration 8: Largest change in params was 0.00171 in the m_probability of given_name, level `All other comparisons`\nIteration 9: Largest change in params was 0.000999 in the m_probability of given_name, level `All other comparisons`\nIteration 10: Largest change in params was -0.000606 in the m_probability of given_name, level `Exact match`\nIteration 11: Largest change in params was -0.000394 in the m_probability of surname, level `Exact match`\nIteration 12: Largest change in params was 0.00029 in the m_probability of surname, level `All other comparisons`\nIteration 13: Largest change in params was 0.00021 in the m_probability of surname, level `All other comparisons`\nIteration 14: Largest change in params was -0.00015 in the m_probability of surname, level `Exact match`\nIteration 15: Largest change in params was -0.000107 in the m_probability of surname, level `Exact match`\nIteration 16: Largest change in params was 7.53e-05 in the m_probability of surname, level `All other comparisons`\n\nEM converged after 16 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code># import json\n# we can have a look at the full settings if we wish, including the values of our estimated parameters:\n# print(json.dumps(linker_detailed._settings_obj.as_dict(), indent=2))\n# we can also get a handy summary of of the model in an easily readable format if we wish:\n# print(linker_detailed._settings_obj.human_readable_description)\n# (we suppress output here for brevity)\n</code></pre> <p>We can now visualise some of the details of our models. We can look at the match weights, which tell us the relative importance for/against a match for each of our comparsion levels.</p> <p>Comparing the two models will show the added benefit we get in the more detailed model --- what in the simple model is classed as 'all other comparisons' is instead broken down further, and we can see that the detail of how this is broken down in fact gives us quite a bit of useful information about the likelihood of a match.</p> <pre><code>linker_simple.match_weights_chart()\n</code></pre> <pre><code>linker_detailed.match_weights_chart()\n</code></pre> <p>As well as the match weights, which give us an idea of the overall effect of each comparison level, we can also look at the individual <code>u</code> and <code>m</code> parameter estimates, which tells us about the prevalence of coincidences and mistakes (for further details/explanation about this see this article). We might want to revise aspects of our model based on the information we ascertain here.</p> <p>Note however that some of these values are very small, which is why the match weight chart is often more useful for getting a decent picture of things.</p> <pre><code># linker_simple.m_u_parameters_chart()\nlinker_detailed.m_u_parameters_chart()\n</code></pre> <p>It is also useful to have a look at unlinkable records - these are records which do not contain enough information to be linked at some match probability threshold. We can figure this out be seeing whether records are able to be matched with themselves.</p> <p>This is of course relative to the information we have put into the model - we see that in our simple model, at a 99% match threshold nearly 10% of records are unlinkable, as we have not included enough information in the model for distinct records to be adequately distinguished; this is not an issue in our more detailed model.</p> <pre><code>linker_simple.unlinkables_chart()\n</code></pre> <pre><code>linker_detailed.unlinkables_chart()\n</code></pre> <p>Our simple model doesn't do terribly, but suffers if we want to have a high match probability --- to be 99% (match weight ~7) certain of matches we have ~10% of records that we will be unable to link.</p> <p>Our detailed model, however, has enough nuance that we can at least self-link records.</p> <pre><code>predictions = linker_detailed.predict()\ndf_predictions = predictions.as_pandas_dataframe()\ndf_predictions.head(5)\n</code></pre> match_weight match_probability source_dataset_l source_dataset_r rec_id_l rec_id_r given_name_l given_name_r gamma_given_name tf_given_name_l ... gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode address_1_l address_1_r state_l state_r match_key 0 45.265873 1.0 __splink__input_table_0 __splink__input_table_1 rec-4405-org rec-4405-dup-0 charles charles 4 0.0017 ... 3 0.0002 0.0002 739.211191 5.692728 salkauskas crescent salkauskas crescent nsw nsw 0 1 50.067581 1.0 __splink__input_table_0 __splink__input_table_1 rec-3585-org rec-3585-dup-0 mikayla mikayla 4 0.0011 ... 3 0.0008 0.0008 739.211191 1.423182 randwick road randwick road vic vic 0 2 37.545864 1.0 __splink__input_table_0 __splink__input_table_1 rec-298-org rec-298-dup-0 blake blake 4 0.0038 ... 2 0.0005 0.0005 11.541654 1.000000 cutlack street belmont park belted galloway stud vic vic 0 3 49.248563 1.0 __splink__input_table_0 __splink__input_table_1 rec-4866-org rec-4866-dup-0 charlie charlie 4 0.0023 ... 3 0.0012 0.0012 739.211191 0.948788 hawkesbury crescent vic vic 0 4 47.422741 1.0 __splink__input_table_0 __splink__input_table_1 rec-420-org rec-420-dup-0 kate kate 4 0.0010 ... 3 0.0002 0.0002 739.211191 5.692728 sinclair street sinclair street sa nsw 0 <p>5 rows \u00d7 47 columns</p> <p>We can see how our model performs at different probability thresholds, with a couple of options depending on the space we wish to view things</p> <pre><code># linker_detailed.roc_chart_from_labels_column(\"cluster\")\nlinker_detailed.precision_recall_chart_from_labels_column(\"cluster\")\n</code></pre> <p>and we can easily see how many individuals we identify and link by looking at clusters generated at some threshold match probability of interest - in this example 99%</p> <pre><code>clusters = linker_detailed.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.99)\ndf_clusters = clusters.as_pandas_dataframe().sort_values(\"cluster_id\")\ndf_clusters.groupby(\"cluster_id\").size().value_counts()\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 0\n</code>\n</pre> <pre>\n<code>2    4958\n1      84\ndtype: int64</code>\n</pre> <p>In this case, we happen to know what the true links are, so we can manually inspect the ones that are doing worst to see what our model is not capturing - i.e. where we have false negatives.</p> <p>Similarly, we can look at the non-links which are performing the best, to see whether we have an issue with false positives.</p> <p>Ordinarily we would not have this luxury, and so would need to dig a bit deeper for clues as to how to improve our model, such as manually inspecting records across threshold probabilities, </p> <pre><code>df_predictions[\"cluster_l\"] = df_predictions[\"rec_id_l\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\ndf_predictions[\"cluster_r\"] = df_predictions[\"rec_id_r\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\ndf_true_links = df_predictions[df_predictions[\"cluster_l\"] == df_predictions[\"cluster_r\"]].sort_values(\"match_probability\")\n</code></pre> <pre><code>records_to_view = 3\nlinker_detailed.waterfall_chart(df_true_links.head(records_to_view).to_dict(orient=\"records\"))\n</code></pre> <pre><code>df_non_links = df_predictions[df_predictions[\"cluster_l\"] != df_predictions[\"cluster_r\"]].sort_values(\"match_probability\", ascending=False)\nlinker_detailed.waterfall_chart(df_non_links.head(records_to_view).to_dict(orient=\"records\"))\n</code></pre> <pre><code># we need to append a full name column to our source data frames\n# so that we can use it for term frequency adjustments\ndfs[0][\"full_name\"] = dfs[0][\"given_name\"] + \"_\" + dfs[0][\"surname\"]\ndfs[1][\"full_name\"] = dfs[1][\"given_name\"] + \"_\" + dfs[1][\"surname\"]\n\n\nextended_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        {\n            \"output_column_name\": \"Full name\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"(given_name_l IS NULL OR given_name_r IS NULL) and (surname_l IS NULL OR surname_r IS NULL)\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": True,\n                },\n                # full name match\n                cll.exact_match_level(\"full_name\", term_frequency_adjustments=True),\n                # typos - keep levels across full name rather than scoring separately\n                cll.jaro_winkler_level(\"full_name\", 0.9),\n                cll.jaro_winkler_level(\"full_name\", 0.7),\n                # name switched\n                cll.columns_reversed_level(\"given_name\", \"surname\"),\n                # name switched + typo\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) &amp;gt;= 1.8\",\n                    \"label_for_charts\": \"switched + jaro_winkler_similarity &amp;gt;= 1.8\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) &amp;gt;= 1.4\",\n                    \"label_for_charts\": \"switched + jaro_winkler_similarity &amp;gt;= 1.4\"\n                },\n                # single name match\n                cll.exact_match_level(\"given_name\", term_frequency_adjustments=True),\n                cll.exact_match_level(\"surname\", term_frequency_adjustments=True),\n                # single name cross-match\n                {\n                    \"sql_condition\": \"given_name_l = surname_r OR surname_l = given_name_r\",\n                    \"label_for_charts\": \"single name cross-matches\"\n                },                # single name typos\n                cll.jaro_winkler_level(\"given_name\", 0.9),\n                cll.jaro_winkler_level(\"surname\", 0.9),\n                # the rest\n                cll.else_level()\n            ]\n        },\n        ctl.date_comparison(\"date_of_birth\", \n                            damerau_levenshtein_thresholds=[],\n                            cast_strings_to_date=True,\n                            invalid_dates_as_null=True,\n                            date_format=\"%Y%m%d\"),\n        {\n            \"output_column_name\": \"Social security ID\",\n            \"comparison_levels\": [\n                cll.null_level(\"soc_sec_id\"),\n                cll.exact_match_level(\"soc_sec_id\", term_frequency_adjustments=True),\n                cll.damerau_levenshtein_level(\"soc_sec_id\", 1),\n                cll.damerau_levenshtein_level(\"soc_sec_id\", 2),\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Street number\",\n            \"comparison_levels\": [\n                cll.null_level(\"street_number\"),\n                cll.exact_match_level(\"street_number\", term_frequency_adjustments=True),\n                cll.damerau_levenshtein_level(\"street_number\", 1),\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Postcode\",\n            \"comparison_levels\": [\n                cll.null_level(\"postcode\"),\n                cll.exact_match_level(\"postcode\", term_frequency_adjustments=True),\n                cll.damerau_levenshtein_level(\"postcode\", 1),\n                cll.damerau_levenshtein_level(\"postcode\", 2),\n                cll.else_level()\n            ]\n        },\n        # we don't consider further location columns as they will be strongly correlated with postcode\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code># train\nlinker_advanced = DuckDBLinker(dfs, extended_model_settings)\nlinker_advanced.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\n# we increase target rows to improve accuracy for u values in full name comparison, as we have subdivided the data more finely\nlinker_advanced.estimate_u_using_random_sampling(max_pairs=1e8)\nsession_dob = linker_advanced.estimate_parameters_using_expectation_maximisation(\n    \"l.date_of_birth = r.date_of_birth\"\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000239.\nThis means that amongst all possible pairwise record comparisons, one in 4,185.85 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,972.50 matching pairs\n----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - Full name (no m values are trained).\n    - date_of_birth (no m values are trained).\n    - Social security ID (no m values are trained).\n    - Street number (no m values are trained).\n    - Postcode (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.date_of_birth = r.date_of_birth\n\nParameter estimates will be made for the following comparison(s):\n    - Full name\n    - Social security ID\n    - Street number\n    - Postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - date_of_birth\n\nWARNING:\nLevel single name cross-matches on comparison Full name not observed in dataset, unable to train m value\n\nIteration 1: Largest change in params was -0.465 in the m_probability of Full name, level `Exact match`\nIteration 2: Largest change in params was 0.00242 in the m_probability of Social security ID, level `All other comparisons`\nIteration 3: Largest change in params was 4.5e-05 in the m_probability of Social security ID, level `All other comparisons`\n\nEM converged after 3 iterations\nm probability not trained for Full name - single name cross-matches (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n\nYour model is not yet fully trained. Missing estimates for:\n    - Full name (some m values are not trained).\n    - date_of_birth (no m values are trained).\n</code>\n</pre> <pre><code>session_pc = linker_advanced.estimate_parameters_using_expectation_maximisation(\n    \"l.postcode = r.postcode\"\n)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.postcode = r.postcode\n\nParameter estimates will be made for the following comparison(s):\n    - Full name\n    - date_of_birth\n    - Social security ID\n    - Street number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - Postcode\n\nWARNING:\nLevel single name cross-matches on comparison Full name not observed in dataset, unable to train m value\n\nIteration 1: Largest change in params was 0.0256 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 2: Largest change in params was 0.000392 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 3: Largest change in params was 1.06e-05 in the m_probability of Social security ID, level `All other comparisons`\n\nEM converged after 3 iterations\nm probability not trained for Full name - single name cross-matches (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n\nYour model is not yet fully trained. Missing estimates for:\n    - Full name (some m values are not trained).\n</code>\n</pre> <pre><code>linker_advanced.parameter_estimate_comparisons_chart()\n</code></pre> <pre><code>linker_advanced.match_weights_chart()\n</code></pre> <pre><code>predictions_adv = linker_advanced.predict()\ndf_predictions_adv = predictions_adv.as_pandas_dataframe()\nclusters_adv = linker_advanced.cluster_pairwise_predictions_at_threshold(predictions_adv, threshold_match_probability=0.99)\ndf_clusters_adv = clusters_adv.as_pandas_dataframe().sort_values(\"cluster_id\")\ndf_clusters_adv.groupby(\"cluster_id\").size().value_counts()\n</code></pre> <pre>\n<code>\n -- WARNING --\nYou have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\nComparison: 'Full name':\n    m values not fully trained\nCompleted iteration 1, root rows count 0\n</code>\n</pre> <pre>\n<code>2    4960\n1      80\ndtype: int64</code>\n</pre> <p>This is a pretty modest improvement on our previous model - however it is worth re-iterating that we should not necessarily expect to recover all matches, as in several cases it may be unreasonable for a model to have reasonable confidence that two records refer to the same entity.</p> <p>If we wished to improve matters we could iterate on this process - investigating where our model is not performing as we would hope, and seeing how we can adjust these areas to address these shortcomings.</p>"},{"location":"demos/examples/duckdb/febrl4.html#linking-the-febrl4-datasets","title":"Linking the febrl4 datasets","text":"<p>See A.2 here and here for the source of this data.</p> <p>It consists of two datasets, A and B, of 5000 records each, with each record in dataset A having a corresponding record in dataset B. The aim will be to capture as many of those 5000 true links as possible, with minimal false linkages.</p> <p>It is worth noting that we should not necessarily expect to capture all links. There are some links that although we know they do correspond to the same person, the data is so mismatched between them that we would not reasonably expect a model to link them, and indeed should a model do so may indicate that we have overengineered things using our knowledge of true links, which will not be a helpful reference in situations where we attempt to link unlabelled data, as will usually be the case.</p>"},{"location":"demos/examples/duckdb/febrl4.html#exploring-data-and-defining-model","title":"Exploring data and defining model","text":""},{"location":"demos/examples/duckdb/febrl4.html#estimating-model-parameters","title":"Estimating model parameters","text":""},{"location":"demos/examples/duckdb/febrl4.html#predictions","title":"Predictions","text":"<p>Now that we have had a look into the details of the models, we will focus on only our more detailed model, which should be able to capture more of the genuine links in our data</p>"},{"location":"demos/examples/duckdb/febrl4.html#further-refinements","title":"Further refinements","text":"<p>Looking at the non-links we have done well in having no false positives at any substantial match probability --- however looking at some of the true links we can see that there are a few that we are not capturing with sufficient match probability.</p> <p>We can see that there are a few features that we are not capturing/weighting appropriately * single-character transpostions, particularly in postcode (which is being lumped in with more 'severe typos'/probable non-matches) * given/sur-names being swapped with typos * given/sur-names being cross-matches on one only, with no match on the other cross</p> <p>We will quickly see if we can incorporate these features into a new model. As we are now going into more detail with the inter-relationship between given name and surname, it is probably no longer sensible to model them as independent comparisons, and so we will need to switch to a combined comparison on full name.</p>"},{"location":"demos/examples/duckdb/link_only.html","title":"Linking two tables of persons","text":"<pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.fake_1000\n\n# Split a simple dataset into two, separate datasets which can be linked together.\ndf_l = df.sample(frac=0.5)\ndf_r = df.drop(df_l.index)\n\ndf_l.head(2)\n</code></pre> unique_id first_name surname dob city email cluster 681 681 Elizabeth Sahw 2006-04-21 NaN e.shaw@smith-hall.biz 174 655 655 Dylan Robert 1990-10-26 Birmingham NaN 166 <pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.duckdb.blocking_rule_library import block_on\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\n\nsettings = {\n    \"link_type\": \"link_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\",),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],       \n}\n\nlinker = DuckDBLinker([df_l, df_r], settings, input_table_aliases=[\"df_left\", \"df_right\"])\n</code></pre> <pre><code>linker.completeness_chart(cols=[\"first_name\", \"surname\", \"dob\", \"city\", \"email\"])\n</code></pre> <pre><code>deterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &amp;lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.00326.\nThis means that amongst all possible pairwise record comparisons, one in 306.48 are expected to match.  With 250,000 total possible comparisons, we expect a total of around 815.71 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=1)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>session_dob = linker.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\nsession_email = linker.estimate_parameters_using_expectation_maximisation(block_on(\"email\"))\nsession_first_name = linker.estimate_parameters_using_expectation_maximisation(block_on(\"first_name\"))\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"dob\" = r.\"dob\"\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.434 in the m_probability of surname, level `Exact match surname`\nIteration 2: Largest change in params was 0.122 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0454 in the m_probability of first_name, level `All other comparisons`\nIteration 4: Largest change in params was 0.0153 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00601 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.00259 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.00117 in probability_two_random_records_match\nIteration 8: Largest change in params was 0.000537 in probability_two_random_records_match\nIteration 9: Largest change in params was 0.000249 in probability_two_random_records_match\nIteration 10: Largest change in params was 0.000116 in probability_two_random_records_match\nIteration 11: Largest change in params was 5.41e-05 in probability_two_random_records_match\n\nEM converged after 11 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - dob (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"email\" = r.\"email\"\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - dob\n    - city\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - email\n\nIteration 1: Largest change in params was -0.492 in the m_probability of dob, level `Exact match`\nIteration 2: Largest change in params was 0.0862 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0181 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0064 in the m_probability of surname, level `All other comparisons`\nIteration 5: Largest change in params was 0.00312 in the m_probability of surname, level `All other comparisons`\nIteration 6: Largest change in params was 0.00176 in the m_probability of surname, level `All other comparisons`\nIteration 7: Largest change in params was 0.0011 in the m_probability of surname, level `All other comparisons`\nIteration 8: Largest change in params was 0.000735 in the m_probability of surname, level `All other comparisons`\nIteration 9: Largest change in params was 0.000516 in the m_probability of surname, level `All other comparisons`\nIteration 10: Largest change in params was 0.000374 in the m_probability of surname, level `All other comparisons`\nIteration 11: Largest change in params was 0.000279 in the m_probability of surname, level `All other comparisons`\nIteration 12: Largest change in params was 0.000212 in the m_probability of surname, level `All other comparisons`\nIteration 13: Largest change in params was 0.000164 in the m_probability of surname, level `All other comparisons`\nIteration 14: Largest change in params was 0.000128 in the m_probability of surname, level `All other comparisons`\nIteration 15: Largest change in params was 0.000101 in the m_probability of surname, level `All other comparisons`\nIteration 16: Largest change in params was 8.03e-05 in the m_probability of surname, level `All other comparisons`\n\nEM converged after 16 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"first_name\" = r.\"first_name\"\n\nParameter estimates will be made for the following comparison(s):\n    - surname\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n\nIteration 1: Largest change in params was -0.164 in the m_probability of surname, level `All other comparisons`\nIteration 2: Largest change in params was -0.00765 in the m_probability of surname, level `All other comparisons`\nIteration 3: Largest change in params was -0.00102 in the m_probability of surname, level `All other comparisons`\nIteration 4: Largest change in params was -0.000162 in the m_probability of surname, level `All other comparisons`\nIteration 5: Largest change in params was -2.68e-05 in the m_probability of surname, level `All other comparisons`\n\nEM converged after 5 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability source_dataset_l source_dataset_r unique_id_l unique_id_r first_name_l first_name_r gamma_first_name surname_l ... dob_l dob_r gamma_dob city_l city_r gamma_city email_l email_r gamma_email match_key 0 15.720147 0.999981 df_left df_right 655 656 Dylan Dylan 4 Robert ... 1990-10-26 1990-10-26 5 Birmingham Birmingham 1 NaN droberts73@taylor-lang.com -1 0 1 18.533203 0.999997 df_left df_right 686 685 Rosie Rosie 4 Johnston ... 1978-11-23 1978-11-23 5 Sheffield Sheffield 1 NaN rosiej32@robinson-moran.net -1 0 2 14.264530 0.999949 df_left df_right 815 819 Logan Logan 4 Morgan ... 1977-01-29 1976-12-30 3 Coventry Coventry 1 NaN loganmorgan43@icbride-kmng.com -1 0 3 25.783560 1.000000 df_left df_right 766 765 Adam Adam 4 Edwards ... 1971-08-28 1971-08-28 5 Cardiff Cardiff 1 adam.edwards38@bullock-edwards.com adam.edwards38@bullock-edward.com 2 0 4 7.646308 0.995033 df_left df_right 125 123 Harley Harley 4 Kaur ... 1973-11-26 1973-10-27 3 Mancseter Mancheeser 0 harleyk@houston.net harleyk@houston.net 3 0 <p>5 rows \u00d7 22 columns</p>"},{"location":"demos/examples/duckdb/link_only.html#linking-without-deduplication","title":"Linking without deduplication","text":"<p>A simple record linkage model using the <code>link_only</code> link type.</p>"},{"location":"demos/examples/duckdb/pairwise_labels.html","title":"Estimating m probabilities from labels","text":"<pre><code>import pandas as pd \nimport altair as alt\n\nfrom splink.datasets import splink_dataset_labels\npairwise_labels = splink_dataset_labels.fake_1000_labels\n\n# Choose labels indicating a match\npairwise_labels = pairwise_labels[pairwise_labels[\"clerical_match_score\"] == 1]\npairwise_labels\n</code></pre> unique_id_l source_dataset_l unique_id_r source_dataset_r clerical_match_score 0 0 fake_1000 1 fake_1000 1.0 1 0 fake_1000 2 fake_1000 1.0 2 0 fake_1000 3 fake_1000 1.0 49 1 fake_1000 2 fake_1000 1.0 50 1 fake_1000 3 fake_1000 1.0 ... ... ... ... ... ... 3171 994 fake_1000 996 fake_1000 1.0 3172 995 fake_1000 996 fake_1000 1.0 3173 997 fake_1000 998 fake_1000 1.0 3174 997 fake_1000 999 fake_1000 1.0 3175 998 fake_1000 999 fake_1000 1.0 <p>2031 rows \u00d7 5 columns</p> <p>We now proceed to estimate the Fellegi Sunter model:</p> <pre><code>from splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\ndf.head(2)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 <pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.duckdb.blocking_rule_library import block_on\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &amp;lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code># Register the pairwise labels table with the database, and then use it to estimate the m values\nlabels_df = linker.register_labels_table(pairwise_labels, overwrite=True)\nlinker.estimate_m_from_pairwise_labels(labels_df)\n\n\n# If the labels table already existing in the dataset you could run\n# linker.estimate_m_from_pairwise_labels(\"labels_tablename_here\")\n</code></pre> <pre><code>training_blocking_rule = block_on(\"first_name\")\nlinker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>&lt;EMTrainingSession, blocking on l.\"first_name\" = r.\"first_name\", deactivating comparisons first_name&gt;</code>\n</pre> <pre><code>linker.parameter_estimate_comparisons_chart()\n</code></pre> <pre><code>linker.match_weights_chart()\n</code></pre>"},{"location":"demos/examples/duckdb/pairwise_labels.html#estimating-m-from-a-sample-of-pairwise-labels","title":"Estimating m from a sample of pairwise labels","text":"<p>In this example, we estimate the m probabilities of the model from a table containing pairwise record comparisons which we know are 'true' matches.  For example, these may be the result of work by a clerical team who have manually labelled a sample of matches.</p> <p>The table must be in the following format:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r df_1 1 df_2 2 df_1 1 df_2 3 <p>It is assumed that every record in the table represents a certain match.</p> <p>Note that the column names above are the defaults.  They should correspond to the values you've set for <code>unique_id_column_name</code> and  <code>source_dataset_column_name</code>, if you've chosen custom values.</p>"},{"location":"demos/examples/duckdb/quick_and_dirty_persons.html","title":"Quick and dirty persons model","text":"<pre><code>from splink.datasets import splink_datasets\ndf = splink_datasets.historical_50k\ndf.head(5)\n</code></pre> unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon tq13 8df male politician 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8hu None politician 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df None politician <pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.duckdb.blocking_rule_library import block_on\nimport splink.duckdb.comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"full_name\"),\n        block_on([\"substr(full_name,1,6)\", \"dob\", \"birth_place\"]),\n        block_on([\"dob\", \"birth_place\"]),\n        block_on(\"postcode_fake\"),\n    ],\n    \"comparisons\": [\n        cl.jaro_at_thresholds(\"full_name\", [0.9, 0.7], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"dob\", [1, 2]),\n        cl.levenshtein_at_thresholds(\"postcode_fake\", 2),\n        cl.jaro_winkler_at_thresholds(\"birth_place\", 0.9, term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],       \n\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.full_name = r.full_name\",\n    \"l.postcode_fake = r.postcode_fake and l.dob = r.dob\",\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=2e6)\n</code></pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre>\n<code>\n -- WARNING --\nYou have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\nComparison: 'full_name':\n    m values not fully trained\nComparison: 'dob':\n    m values not fully trained\nComparison: 'postcode_fake':\n    m values not fully trained\nComparison: 'birth_place':\n    m values not fully trained\nComparison: 'occupation':\n    m values not fully trained\n</code>\n</pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r full_name_l full_name_r gamma_full_name dob_l dob_r gamma_dob postcode_fake_l postcode_fake_r gamma_postcode_fake birth_place_l birth_place_r gamma_birth_place occupation_l occupation_r gamma_occupation match_key 0 33.962763 1.000000 Q90404618-1 Q90404618-3 emlie clifford emlie clifford 3 1861-01-01 1861-01-01 3 wr11 7qp wr11 7qw 1 wychavon wychavon 2 playwright playwright 1 0 1 33.962763 1.000000 Q90404618-2 Q90404618-3 emlie clifford emlie clifford 3 1861-01-01 1861-01-01 3 wr11 7qp wr11 7qw 1 wychavon wychavon 2 playwright playwright 1 0 2 16.224687 0.999987 Q55455287-1 Q55455287-8 jaido morata jaido morata 3 1836-01-01 1836-11-01 2 ta4 2uu ta4 2uu 2 somerset west and taunton NaN -1 writer writer 1 0 3 16.224687 0.999987 Q55455287-2 Q55455287-8 jaido morata jaido morata 3 1836-01-01 1836-11-01 2 ta4 2uu ta4 2uu 2 somerset west and taunton NaN -1 writer writer 1 0 4 16.224687 0.999987 Q55455287-3 Q55455287-8 jaido morata jaido morata 3 1836-01-01 1836-11-01 2 ta4 2uu ta4 2uu 2 somerset west and taunton NaN -1 writer writer 1 0"},{"location":"demos/examples/duckdb/quick_and_dirty_persons.html#historical-people-quick-and-dirty","title":"Historical people: Quick and dirty","text":"<p>This example shows how to get some initial record linkage results as quickly as possible.  </p> <p>There are many ways to improve the accuracy of this model.  But this may be a good place to start if you just want to give Splink a try and see what it's capable of.</p>"},{"location":"demos/examples/duckdb/real_time_record_linkage.html","title":"Real time record linkage","text":"<p>In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the <code>linker.compare_two_records</code> function, that allows you to interactively explore the results of a linkage model; and - the <code>linker.find_matches_to_new_records</code> that allows you to incrementally find matches to a small number of new records</p> <pre><code>import json\nfrom splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\nimport altair as alt\nalt.renderers.enable('html')\n\nwith open(\"../../demo_settings/real_time_settings.json\") as f:\n    trained_settings = json.load(f)\n\ndf = splink_datasets.fake_1000\n\nlinker = DuckDBLinker(df, trained_settings)\n</code></pre> <pre><code>linker.waterfall_chart(linker.predict().as_record_dict(limit=2))\n</code></pre> <pre><code>from splink.term_frequencies import compute_term_frequencies_from_concat_with_tf\nrecord_1  = {\n     'unique_id':1,\n     'first_name': \"Lucas\",\n     'surname': \"Smith\",\n     'dob': \"1984-01-02\",\n     'city': \"London\",\n     'email': \"lucas.smith@hotmail.com\"\n}\n\nrecord_2  = {\n     'unique_id':2,\n     'first_name': \"Lucas\",\n     'surname': \"Smith\",\n     'dob': \"1983-02-12\",\n     'city': \"Machester\",\n     'email': \"lucas.smith@hotmail.com\"\n}\n\nlinker._settings_obj_._retain_intermediate_calculation_columns = True\nlinker._settings_obj_._retain_matching_columns = True\n\nlinker.compute_tf_table(\"first_name\")\nlinker.compute_tf_table(\"surname\")\nlinker.compute_tf_table(\"dob\")\nlinker.compute_tf_table(\"city\")\nlinker.compute_tf_table(\"email\")\n\n\ndf_two = linker.compare_two_records(record_1, record_2)\ndf_two.as_pandas_dataframe()\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 0 13.161672 0.999891 1 2 Lucas Lucas 2 0.001203 0.001203 87.571229 ... NaN 0.446404 1.0 lucas.smith@hotmail.com lucas.smith@hotmail.com 1 NaN NaN 263.229168 1.0 <p>1 rows \u00d7 39 columns</p> <pre><code>import ipywidgets as widgets\nfields = [\"unique_id\", \"first_name\",\"surname\",\"dob\",\"email\",\"city\"]\n\nleft_text_boxes = []\nright_text_boxes = []\n\ninputs_to_interactive_output = {}\n\nfor f in fields:\n    wl = widgets.Text(description=f, value =str(record_1[f]))\n    left_text_boxes.append(wl)\n    inputs_to_interactive_output[f\"{f}_l\"] = wl\n    wr = widgets.Text( description=f, value =str(record_2[f]))\n    right_text_boxes.append(wr)\n    inputs_to_interactive_output[f\"{f}_r\"] = wr\n\n\nb1 = widgets.VBox(left_text_boxes)\nb2 = widgets.VBox(right_text_boxes)\nui = widgets.HBox([b1,b2])\n\ndef myfn(**kwargs):\n    my_args = dict(kwargs)\n\n    record_left = {}\n    record_right = {}\n\n    for key, value in my_args.items():\n        if value == '':\n            value = None\n        if key.endswith(\"_l\"):\n            record_left[key[:-2]] = value\n        if key.endswith(\"_r\"):\n            record_right[key[:-2]] = value\n\n\n    linker._settings_obj_._retain_intermediate_calculation_columns = True\n    linker._settings_obj_._retain_matching_columns = True\n\n    df_two = linker.compare_two_records(record_left, record_right)\n\n    recs = df_two.as_pandas_dataframe().to_dict(orient=\"records\")\n    from splink.charts import waterfall_chart\n    display(linker.waterfall_chart(recs, filter_nulls=False))\n\n\nout = widgets.interactive_output(myfn, inputs_to_interactive_output)\n\ndisplay(ui,out)\n</code></pre> <pre>\n<code>HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026</code>\n</pre> <pre>\n<code>Output()</code>\n</pre> <pre><code>record = {'unique_id': 123987,\n 'first_name': \"Robert\",\n 'surname': \"Alan\",\n 'dob': \"1971-05-24\",\n 'city': \"London\",\n 'email': \"robert255@smith.net\"\n}\n\n\n\ndf_inc = linker.find_matches_to_new_records([record], blocking_rules=[]).as_pandas_dataframe()\ndf_inc.sort_values(\"match_weight\", ascending=False)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 2 23.531793 1.000000 0 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 robert255@smith.net robert255@smith.net 1 0.001267 0.001267 263.229168 1.730964 4 14.550320 0.999958 1 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 3 10.388623 0.999255 3 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 0.446404 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 1 2.427256 0.843228 2 123987 Rob Robert 0 0.001203 0.00361 0.218767 ... 0.212792 10.484859 0.259162 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 0 -2.123090 0.186697 8 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 6 -2.205894 0.178139 754 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 j.c@whige.wort robert255@smith.net 0 0.001267 0.001267 0.423438 1.000000 5 -2.802309 0.125383 750 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 10.484859 0.259162 j.c@white.org robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 <p>7 rows \u00d7 39 columns</p> <pre><code>from splink.charts import waterfall_chart\n\n@widgets.interact(first_name='Robert', surname=\"Alan\", dob=\"1971-05-24\", city=\"London\", email=\"robert255@smith.net\")\ndef interactive_link(first_name, surname, dob, city, email):    \n\n    record = {'unique_id': 123987,\n     'first_name': first_name,\n     'surname': surname,\n     'dob': dob,\n     'city': city,\n     'email': email,\n     'group': 0}\n\n    for key in record.keys():\n        if type(record[key]) == str:\n            if record[key].strip() == \"\":\n                record[key] = None\n\n\n    df_inc = linker.find_matches_to_new_records([record], blocking_rules=[f\"(true)\"]).as_pandas_dataframe()\n    df_inc = df_inc.sort_values(\"match_weight\", ascending=False)\n    recs = df_inc.to_dict(orient=\"records\")\n\n\n\n    display(linker.waterfall_chart(recs, filter_nulls=False))\n</code></pre> <pre>\n<code>interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre>"},{"location":"demos/examples/duckdb/real_time_record_linkage.html#real-time-linkage","title":"Real time linkage","text":""},{"location":"demos/examples/duckdb/real_time_record_linkage.html#step-1-load-a-pre-trained-linkage-model","title":"Step 1: Load a pre-trained linkage model","text":""},{"location":"demos/examples/duckdb/real_time_record_linkage.html#step-comparing-two-records","title":"Step  Comparing two records","text":"<p>It's now possible to compute a match weight for any two records using <code>linker.compare_two_records()</code></p>"},{"location":"demos/examples/duckdb/real_time_record_linkage.html#step-3-interactive-comparisons","title":"Step 3: Interactive comparisons","text":"<p>One interesting applicatin of <code>compare_two_records</code> is to create a simple interface that allows the user to input two records interactively, and get real time feedback.</p> <p>In the following cell we use <code>ipywidets</code> for this purpose.  \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728</p>"},{"location":"demos/examples/duckdb/real_time_record_linkage.html#finding-matching-records-interactively","title":"Finding matching records interactively","text":"<p>It is also possible to search the records in the input dataset rapidly using the <code>linker.find_matches_to_new_records()</code> function</p>"},{"location":"demos/examples/duckdb/real_time_record_linkage.html#interactive-interface-for-finding-records","title":"Interactive interface for finding records","text":"<p>Again, we can use <code>ipywidgets</code> to build an interactive interface for the <code>linker.find_matches_to_new_records</code> function</p>"},{"location":"demos/examples/duckdb/transactions.html","title":"Linking financial transactions","text":"<pre><code>from splink.datasets import splink_datasets\nfrom splink.duckdb.linker import DuckDBLinker\nimport altair as alt\n\ndf_origin = splink_datasets.transactions_origin\ndf_destination = splink_datasets.transactions_destination\n\ndisplay(df_origin.head(2))\ndisplay(df_destination.head(2))\n</code></pre> ground_truth memo transaction_date amount unique_id 0 0 MATTHIAS C paym 2022-03-28 36.36 0 1 1 M CORVINUS dona 2022-02-14 221.91 1 ground_truth memo transaction_date amount unique_id 0 0 MATTHIAS C payment BGC 2022-03-29 36.36 0 1 1 M CORVINUS BGC 2022-02-16 221.91 1 <p>In the following chart, we can see this is a challenging dataset to link: - There are only 151 distinct transaction dates, with strong skew - Some 'memos' are used multiple times (up to 48 times) - There is strong skew in the 'amount' column, with 1,400 transactions of around 60.00</p> <pre><code># Simple settings just for exploratory analysis\nsettings = {\"link_type\": \"link_only\"}\nlinker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=[\"__ori\", \"_dest\"])\nlinker.profile_columns([\"transaction_date\", \"memo\", \"round(amount/5, 0)*5\"])\n</code></pre> <pre><code># Design blocking rules that allow for differences in transaction date and amounts\nfrom splink.duckdb.blocking_rule_library import block_on, and_\n\nblocking_rule_date_1 = \"\"\"\n    strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount &amp;gt; 0.7   and l.amount/r.amount &amp;lt; 1.3\n\"\"\"\n\n# Offset by half a month to ensure we capture case when the dates are e.g. 31st Jan and 1st Feb\nblocking_rule_date_2 = \"\"\"\n    strftime(l.transaction_date+15, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount &amp;gt; 0.7   and l.amount/r.amount &amp;lt; 1.3\n\"\"\"\n\nblocking_rule_memo = block_on(\"substr(memo,1,9)\")\n\nblocking_rule_amount_1 = \"\"\"\nround(l.amount/2,0)*2 = round(r.amount/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date)\n\"\"\"\n\nblocking_rule_amount_2 = \"\"\"\nround(l.amount/2,0)*2 = round((r.amount+1)/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date + 4)\n\"\"\"\n\nblocking_rule_cheat = block_on(\"unique_id\")\n\n\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(\n    [\n        blocking_rule_date_1,\n        blocking_rule_date_2,\n        blocking_rule_memo,\n        blocking_rule_amount_1,\n        blocking_rule_amount_2,\n        blocking_rule_cheat,\n    ]\n)\n</code></pre> <pre><code># Full settings for linking model\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_level_library as cll\n\ncomparison_amount = {\n    \"output_column_name\": \"amount\",\n    \"comparison_levels\": [\n        cll.null_level(\"amount\"),\n        cll.exact_match_level(\"amount\"),\n        cll.percentage_difference_level(\"amount\",0.01),\n        cll.percentage_difference_level(\"amount\",0.03),\n        cll.percentage_difference_level(\"amount\",0.1),\n        cll.percentage_difference_level(\"amount\",0.3),\n        cll.else_level()\n    ],\n    \"comparison_description\": \"Amount percentage difference\",\n}\n\nsettings = {\n    \"link_type\": \"link_only\",\n    \"probability_two_random_records_match\": 1 / len(df_origin),\n    \"blocking_rules_to_generate_predictions\": [\n        blocking_rule_date_1,\n        blocking_rule_date_2,\n        blocking_rule_memo,\n        blocking_rule_amount_1,\n        blocking_rule_amount_2,\n        blocking_rule_cheat\n    ],\n    \"comparisons\": [\n        comparison_amount,\n        cl.jaccard_at_thresholds(\n            \"memo\", [0.9, 0.7]\n        ),\n        cl.datediff_at_thresholds(\"transaction_date\", \n                                date_thresholds = [1, 4, 10, 30],\n                                date_metrics = [\"day\", \"day\", \"day\", \"day\"],\n                                include_exact_match_level=False\n                                )\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n    \"retain_matching_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=[\"__ori\", \"_dest\"])\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - amount (no m values are trained).\n    - memo (no m values are trained).\n    - transaction_date (no m values are trained).\n</code>\n</pre> <pre><code>linker.estimate_parameters_using_expectation_maximisation(block_on(\"memo\"))\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"memo\" = r.\"memo\"\n\nParameter estimates will be made for the following comparison(s):\n    - amount\n    - transaction_date\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - memo\n\nIteration 1: Largest change in params was -0.529 in the m_probability of amount, level `Exact match`\nIteration 2: Largest change in params was -0.237 in the m_probability of transaction_date, level `Within 1 day`\nIteration 3: Largest change in params was -0.0185 in the m_probability of transaction_date, level `Within 1 day`\nIteration 4: Largest change in params was 0.00459 in the m_probability of transaction_date, level `Within 30 days`\nIteration 5: Largest change in params was 0.00129 in the m_probability of transaction_date, level `Within 30 days`\nIteration 6: Largest change in params was 0.000332 in the m_probability of transaction_date, level `Within 30 days`\nIteration 7: Largest change in params was 8.32e-05 in the m_probability of transaction_date, level `Within 30 days`\n\nEM converged after 7 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - memo (no m values are trained).\n</code>\n</pre> <pre>\n<code>&lt;EMTrainingSession, blocking on l.\"memo\" = r.\"memo\", deactivating comparisons memo&gt;</code>\n</pre> <pre><code>session = linker.estimate_parameters_using_expectation_maximisation(block_on(\"amount\"))\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"amount\" = r.\"amount\"\n\nParameter estimates will be made for the following comparison(s):\n    - memo\n    - transaction_date\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - amount\n\nIteration 1: Largest change in params was -0.356 in the m_probability of memo, level `Exact match`\nIteration 2: Largest change in params was -0.0328 in the m_probability of memo, level `Exact match`\nIteration 3: Largest change in params was 0.0442 in the m_probability of memo, level `All other comparisons`\nIteration 4: Largest change in params was 0.0483 in the m_probability of memo, level `All other comparisons`\nIteration 5: Largest change in params was 0.0427 in the m_probability of memo, level `All other comparisons`\nIteration 6: Largest change in params was 0.0318 in the m_probability of memo, level `All other comparisons`\nIteration 7: Largest change in params was 0.0211 in the m_probability of memo, level `All other comparisons`\nIteration 8: Largest change in params was 0.0131 in the m_probability of memo, level `All other comparisons`\nIteration 9: Largest change in params was 0.00792 in the m_probability of memo, level `All other comparisons`\nIteration 10: Largest change in params was 0.00474 in the m_probability of memo, level `All other comparisons`\nIteration 11: Largest change in params was 0.00283 in the m_probability of memo, level `All other comparisons`\nIteration 12: Largest change in params was 0.00169 in the m_probability of memo, level `All other comparisons`\nIteration 13: Largest change in params was 0.00101 in the m_probability of memo, level `All other comparisons`\nIteration 14: Largest change in params was 0.000602 in the m_probability of memo, level `All other comparisons`\nIteration 15: Largest change in params was 0.00036 in the m_probability of memo, level `All other comparisons`\nIteration 16: Largest change in params was 0.000216 in the m_probability of memo, level `All other comparisons`\nIteration 17: Largest change in params was 0.000129 in the m_probability of memo, level `All other comparisons`\nIteration 18: Largest change in params was 7.74e-05 in the m_probability of memo, level `All other comparisons`\n\nEM converged after 18 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>df_predict = linker.predict(threshold_match_probability=0.001)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code>linker.comparison_viewer_dashboard(df_predict,\"dashboards/comparison_viewer_transactions.html\", overwrite=True)\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./dashboards/comparison_viewer_transactions.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre><code>pred_errors =  linker.prediction_errors_from_labels_column(\"ground_truth\", include_false_positives=True, include_false_negatives=False)\nlinker.waterfall_chart(pred_errors.as_record_dict(limit=5))\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code>pred_errors =  linker.prediction_errors_from_labels_column(\"ground_truth\", include_false_positives=False, include_false_negatives=True)\nlinker.waterfall_chart(pred_errors.as_record_dict(limit=5))\n</code></pre>"},{"location":"demos/examples/duckdb/transactions.html#linking-banking-transactions","title":"Linking banking transactions","text":"<p>This example shows how to perform a one-to-one link on banking transactions.  </p> <p>The data is fake data, and was generated has the following features:</p> <ul> <li>Money shows up in the destination account with some time delay</li> <li>The amount sent and the amount received are not always the same - there are hidden fees and foreign exchange effects</li> <li>The memo is sometimes truncated and content is sometimes missing</li> </ul> <p>Since each origin payment should end up in the destination account, the <code>probability_two_random_records_match</code> of the model is known.</p>"},{"location":"demos/examples/spark/deduplicate_1k_synthetic.html","title":"Deduplication using Pyspark","text":"<pre><code>from splink.spark.jar_location import similarity_jar_location\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import types\n\nconf = SparkConf()\n# This parallelism setting is only suitable for a small toy example\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.default.parallelism\", \"16\")\n\n\n# Add custom similarity functions, which are bundled with Splink\n# documented here: https://github.com/moj-analytical-services/splink_scalaudfs\npath = similarity_jar_location()\nconf.set(\"spark.jars\", path)\n\nsc = SparkContext.getOrCreate(conf=conf)\n\nspark = SparkSession(sc)\nspark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n</code></pre> <pre>\n<code>23/08/17 15:07:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n</code>\n</pre> <pre><code># Disable warnings for pyspark - you don't need to include this\nimport warnings\nspark.sparkContext.setLogLevel(\"ERROR\")\nwarnings.simplefilter(\"ignore\", UserWarning)\n</code></pre> <pre><code>from splink.datasets import splink_datasets\npandas_df = splink_datasets.fake_1000\n\ndf = spark.createDataFrame(pandas_df)\n</code></pre> <pre><code>import splink.spark.comparison_library as cl\nimport splink.spark.comparison_template_library as ctl\nfrom splink.spark.blocking_rule_library import block_on\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        \"l.surname = r.surname\",  # alternatively, you can write BRs in their SQL form\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"em_convergence\": 0.01\n}\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\nlinker = SparkLinker(df, settings, spark=spark)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &amp;lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n</code></pre> <pre>\n<code>--WARN-- \n You are using datediff comparison\n                        with str-casting and ANSI is not enabled. Bad dates\n                        e.g. 1999-13-54 will not trigger an exception but will\n                        classed as comparison level = \"ELSE\". Ensure date strings\n                        are cleaned to remove bad dates \n\nProbability two random records match is estimated to be  0.0806.                \nThis means that amongst all possible pairwise record comparisons, one in 12.41 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 40,246.67 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e5)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n\ntraining_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.698 in probability_two_random_records_match\nIteration 2: Largest change in params was 0.0569 in the m_probability of email, level `All other comparisons`\nIteration 3: Largest change in params was 0.0193 in the m_probability of email, level `All other comparisons`\nIteration 4: Largest change in params was 0.0081 in the m_probability of email, level `All other comparisons`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.531 in the m_probability of surname, level `Exact match surname`\nIteration 2: Largest change in params was 0.138 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0478 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0193 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00956 in probability_two_random_records_match\n\nEM converged after 5 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre>\n<code>                                                                                \n</code>\n</pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 9.061592 0.998132 405 406 NaN NaN 4 11.458919 eoookC Cooke ... 0 0.001 0.187 0.624618 1.000000 l.cooke@thompson-williams.info l.cooke@thompson-williams.info 3 8.470049 0 1 19.266762 0.999998 811 813 Elliott Elliott 4 11.458919 NaN NaN ... 1 0.006 0.006 5.892455 11.876543 e.b30@little.biz e.b30zlittle.bi@ 1 252.866021 0 2 8.780607 0.997731 8 10 NaN NaN 4 11.458919 Dean Dean ... 0 0.187 0.014 0.624618 1.000000 NaN evied56@harris-bailey.net 0 0.349378 0 3 13.961451 0.999937 829 830 Mason Mason 4 11.458919 NaN Smith ... 0 0.013 0.001 0.624618 1.000000 masons2@reed.com masons5@2@reed.com 1 252.866021 0 4 17.466985 0.999994 35 36 NaN NaN 4 11.458919 Bron Brrown ... 1 0.009 0.009 5.892455 7.917695 NaN lola.b@martinez-jones.net 0 0.349378 0 <p>5 rows \u00d7 28 columns</p>"},{"location":"demos/examples/spark/deduplicate_1k_synthetic.html#linking-in-spark","title":"Linking in Spark","text":""},{"location":"demos/examples/sqlite/deduplicate_50k_synthetic.html","title":"Deduplicate 50k rows historical persons","text":"<pre><code>from splink.datasets import splink_datasets\nfrom splink.sqlite.linker import SQLiteLinker\nimport altair as alt\n\nimport pandas as pd \npd.options.display.max_rows = 1000\ndf = splink_datasets.historical_50k.sample(10000) # reduce size of dataset to reduce CI runtime\n</code></pre> <pre><code># Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n}\nlinker = SQLiteLinker(df, settings)\n\nlinker.profile_columns(\n    [\"first_name\", \"postcode_fake\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <pre><code>import splink.sqlite.comparison_template_library as ctl\nimport splink.sqlite.comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", jaro_winkler_thresholds=[0.9], term_frequency_adjustments=True),\n        ctl.name_comparison(\"surname\", jaro_winkler_thresholds=[0.9], term_frequency_adjustments=True),\n        cl.damerau_levenshtein_at_thresholds(\"dob\", [1, 2], term_frequency_adjustments=True),\n        cl.damerau_levenshtein_at_thresholds(\"postcode_fake\", [1,2]),\n        cl.exact_match(\"birth_place\", term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"max_iterations\": 10,\n    \"em_convergence\": 0.01\n}\n\nlinker = SQLiteLinker(df, settings)\n</code></pre> <pre><code>linker.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000144.\nThis means that amongst all possible pairwise record comparisons, one in 6,955.02 are expected to match.  With 49,995,000 total possible comparisons, we expect a total of around 7,188.33 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - postcode_fake (no m values are trained).\n    - birth_place (no m values are trained).\n    - occupation (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_names = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\n</code>\n</pre> <pre>\n<code>Estimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.533 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.047 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0181 in the m_probability of birth_place, level `Exact match`\nIteration 4: Largest change in params was 0.00608 in the m_probability of birth_place, level `Exact match`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.353 in the m_probability of first_name, level `Exact match first_name`\nIteration 2: Largest change in params was 0.0376 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.00562 in the m_probability of postcode_fake, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>The final match weights can be viewed in the match weights chart:</p> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre><code>df_predict = linker.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 25.908286 1.000000 Q7422529-2 Q7422529-3 sarah sarah 3 0.001603 0.001603 46.376176 ... 160.817163 3.188489 None None -1 NaN None 1.0 1.0 0 1 2.815419 0.875610 Q334210-11 Q334210-13 reginald reginald 3 0.002304 0.002304 46.376176 ... 0.170301 1.000000 None None -1 NaN None 1.0 1.0 0 2 19.086449 0.999998 Q334210-11 Q334210-2 reginald reginald 3 0.002304 0.002304 46.376176 ... 0.170301 1.000000 None None -1 NaN None 1.0 1.0 0 3 24.895924 1.000000 Q334210-11 Q334210-9 reginald reginald 3 0.002304 0.002304 46.376176 ... 0.170301 1.000000 None None -1 NaN None 1.0 1.0 0 4 11.088896 0.999541 Q6233309-3 Q6233309-4 john john 3 0.051598 0.051598 46.376176 ... 0.170301 1.000000 politician None -1 0.09218 None 1.0 1.0 0 <p>5 rows \u00d7 44 columns</p> <p>You can also view rows in this dataset as a waterfall chart as follows:</p> <pre><code>from splink.charts import waterfall_chart\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.waterfall_chart(records_to_plot, filter_nulls=False)\n</code></pre> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 27\nCompleted iteration 2, root rows count 1\nCompleted iteration 3, root rows count 0\n</code>\n</pre> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"dashboards/50k_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./dashboards/50k_cluster.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\",match_weight_round_to_nearest=0.02)\n</code></pre> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.999,\n    include_false_negatives=False,\n    include_false_positives=True,\n).as_record_dict()\nlinker.waterfall_chart(records)\n</code></pre> <pre><code># Some of the false negatives will be because they weren't detected by the blocking rules\nrecords = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.5,\n    include_false_negatives=True,\n    include_false_positives=False,\n).as_record_dict(limit=50)\n\nlinker.waterfall_chart(records)\n</code></pre>"},{"location":"demos/examples/sqlite/deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","title":"Linking a dataset of real historical persons","text":"<p>In this example, we deduplicate a more realistic dataset.  The data is based on historical persons scraped from wikidata.  Duplicate records are introduced with a variety of errors introduced.</p> <p>Note, as explained in the backends topic guide, SQLite does not natively support string fuzzy matching functions such as <code>damareau-levenshtein</code> and <code>jaro-winkler</code> (as used in this example). Instead, these have been imported as python User Defined Functions (UDFs). One drawback of python UDFs is that they are considerably slower than native-SQL comparisons. As such, if you are hitting issues with large run times, consider switching to DuckDB (or some other backend).</p>"},{"location":"demos/tutorials/00_Tutorial_Introduction.html","title":"Introduction","text":""},{"location":"demos/tutorials/00_Tutorial_Introduction.html#introductory-tutorial","title":"Introductory tutorial","text":"<p>This is the introduction to a seven part tutorial which demonstrates how to de-duplicate a small dataset using simple settings.</p> <p>The aim of the tutorial is to demonstrate core Splink functionality succinctly, rather that comprehensively document all configuration options.</p> <p>The seven parts are:</p> <ul> <li> <p>1. Data prep pre-requisites</p> </li> <li> <p>2. Exploratory analysis</p> </li> <li> <p>3. Choosing blocking rules to optimise runtimes</p> </li> <li> <p>4. Estimating model parameters</p> </li> <li> <p>5. Predicting results</p> </li> <li> <p>6. Visualising predictions</p> </li> <li> <p>7. Evaluation</p> </li> </ul> <p>Throughout the tutorial, we use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop.</p> <p>You can find these tutorial notebooks in the <code>docs/demos/tutorials/</code> folder of the  splink repo, and you can run them live in your web browser by clicking the following link:</p> <p></p>"},{"location":"demos/tutorials/00_Tutorial_Introduction.html#end-to-end-demos","title":"End-to-end demos","text":"<p>After following the steps of the tutorial, it might prove useful to have a look at some of the example notebooks that show various use-case scenarios of Splink from start to finish.</p>"},{"location":"demos/tutorials/01_Prerequisites.html","title":"1. Data prep prerequisites","text":"<p>Splink requires that you clean your data and assign unique IDs to rows before linking. </p> <p>This section outlines the additional data cleaning steps needed before loading data into Splink.</p>"},{"location":"demos/tutorials/01_Prerequisites.html#data-prerequisites","title":"Data Prerequisites","text":""},{"location":"demos/tutorials/01_Prerequisites.html#unique-ids","title":"Unique IDs","text":"<ul> <li>Each input dataset must have a unique ID column, which is unique within the dataset.  By default, Splink assumes this column will be called <code>unique_id</code>, but this can be changed with the <code>unique_id_column_name</code> key in your Splink settings.  The unique id is essential because it enables Splink to keep track each row correctly. </li> </ul>"},{"location":"demos/tutorials/01_Prerequisites.html#conformant-input-datasets","title":"Conformant input datasets","text":"<ul> <li>Input datasets must be conformant, meaning they share the same column names and data formats. For instance, if one dataset has a \"date of birth\" column and another has a \"dob\" column, rename them to match. Ensure data type and number formatting are consistent across both columns. The order of columns in input dataframes is not important.</li> </ul>"},{"location":"demos/tutorials/01_Prerequisites.html#cleaning","title":"Cleaning","text":"<ul> <li>Ensure data consistency by cleaning your data. This process includes standardizing date formats, matching text case, and handling invalid data. For example, if one dataset uses \"yyyy-mm-dd\" date format and another uses \"mm/dd/yyyy,\" convert them to the same format before using Splink.  Try also to identify and rectify any obvious data entry errors, such as removing values such as 'Mr' or 'Mrs' from a 'first name' column.</li> </ul>"},{"location":"demos/tutorials/01_Prerequisites.html#ensure-nulls-are-consistently-and-correctly-represented","title":"Ensure nulls are consistently and correctly represented","text":"<ul> <li>Ensure null values (or other 'not known' indicators) are represented as true nulls, not empty strings. Splink treats null values differently from empty strings, so using true nulls guarantees proper matching across datasets.</li> </ul>"},{"location":"demos/tutorials/01_Prerequisites.html#further-details-on-data-cleaning-and-standardisation","title":"Further details on data cleaning and standardisation","text":"<p>Splink performs optimally with cleaned and standardized data. Here is a non-exhaustive list of suggestions for data cleaning rules to enhance matching accuracy:</p> <ul> <li>Trim leading and trailing whitespace from string values (e.g., \" john smith \" becomes \"john smith\").</li> <li>Remove special characters from string values (e.g., \"O'Hara\" becomes \"Ohara\").</li> <li>Standardise date formats as strings in \"yyyy-mm-dd\" format.</li> <li>Replace abbreviations with full words (e.g., standardize \"St.\" and \"Street\" to \"Street\").</li> </ul>"},{"location":"demos/tutorials/02_Exploratory_analysis.html","title":"2. Exploratory analysis","text":"<pre><code>from splink.datasets import splink_datasets\nimport altair as alt\n\ndf = splink_datasets.fake_1000\ndf.head(5)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 <pre><code># Initialise the linker, passing in the input dataset(s)\nfrom splink.duckdb.linker import DuckDBLinker\nlinker = DuckDBLinker(df)\n</code></pre> <p>It's important to understand the level of missingness in your data, because columns with higher levels of missingness are less useful for data linking.</p> <pre><code>linker.missingness_chart()\n</code></pre> <p>The above summary chart shows that in this dataset, the <code>email</code>, <code>city</code>, <code>surname</code> and <code>forename</code> columns contain nulls, but the level of missingness is relatively low (less than 22%).</p> <p>The distribution of values in your data is important for two main reasons:</p> <ol> <li> <p>Columns with higher cardinality (number of distinct values) are usually more useful for data linking.  For instance, date of birth is a much stronger linkage variable than gender.</p> </li> <li> <p>The skew of values is important.  If you have a <code>city</code> column that has 1,000 distinct values, but 75% of them are <code>London</code>, this is much less useful for linkage than if the 1,000 values were equally distributed</p> </li> </ol> <p>The <code>linker.profile_columns()</code> method creates summary charts to help you understand these aspects of your data. </p> <p>To profile all columns, leave the column_expressions argument empty.</p> <pre><code>linker.profile_columns(top_n=10, bottom_n=5)\n</code></pre> <p>This chart is very information-dense, but here are some key takehomes relevant to our linkage:</p> <ul> <li> <p>There is strong skew in the <code>city</code> field with around 20% of the values being <code>London</code>.  We therefore will probably want to use <code>term_frequency_adjustments</code> in our linkage model, so that it can weight a match on London differently to a match on, say, <code>Norwich</code>.</p> </li> <li> <p>Looking at the \"Bottom 5 values by value count\", we can see typos in the data in most fields.  This tells us this information was possibly entered by hand, or using Optical Character Recognition, giving us an insight into the type of data entry errors we may see.</p> </li> <li> <p>Email is a much more uniquely-identifying field than any others, with a maximum value count of 6.  It's likely to be a strong linking variable.</p> </li> </ul> <p>Further Reading</p> <p> For more on exploratory analysis tools in Splink, please refer to the Exploratory Analysis API documentation.</p> <p> For more on the charts used in this tutorial, please refer to the Charts Gallery.</p>"},{"location":"demos/tutorials/02_Exploratory_analysis.html#exploratory-analysis","title":"Exploratory analysis","text":"<p>The purpose of exploratory analysis is to understand your data and any idiosyncrasies which may be relevant to the task of data linking.</p> <p>Splink includes functionality to visualise and summarise your data, to identify characteristics most salient to data linking.</p> <p>In this notebook we perform some basic exploratory analysis, and interpret the results.</p>"},{"location":"demos/tutorials/02_Exploratory_analysis.html#read-in-the-data","title":"Read in the data","text":"<p>For the purpose of this tutorial we will use a 1,000 row synthetic dataset that contains duplicates.</p> <p>The first five rows of this dataset are printed below.</p> <p>Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.)</p>"},{"location":"demos/tutorials/02_Exploratory_analysis.html#instantiate-the-linker","title":"Instantiate the linker","text":"<p>Most of Splink's core functionality can be accessed as methods on a linker object.  For example, to make predictions, you would call <code>linker.predict()</code>.</p> <p>We therefore begin by instantiating the linker, passing in the data we wish to deduplicate.</p>"},{"location":"demos/tutorials/02_Exploratory_analysis.html#analyse-missingness","title":"Analyse missingness","text":""},{"location":"demos/tutorials/02_Exploratory_analysis.html#analyse-the-distribution-of-values-in-your-data","title":"Analyse the distribution of values in your data","text":""},{"location":"demos/tutorials/02_Exploratory_analysis.html#next-steps","title":"Next steps","text":"<p>At this point, we have begun to develop a strong understanding of our data.  It's time to move on to estimating a linkage model</p>"},{"location":"demos/tutorials/03_Blocking.html","title":"3. Blocking","text":"<pre><code>from splink.datasets import splink_datasets\nimport altair as alt\n\ndf = splink_datasets.fake_1000\n</code></pre> <pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.duckdb.blocking_rule_library import block_on\nsettings = {\"link_type\": \"dedupe_only\"}\nlinker = DuckDBLinker(df, settings)\n\nblocking_rule_1 = block_on([\"substr(first_name, 1,1)\", \"surname\"])\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_1)\nprint(f\"Number of comparisons generated by '{blocking_rule_1.blocking_rule_sql}': {count:,.0f}\")\n\nblocking_rule_2 = block_on(\"surname\")\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_2)\nprint(f\"Number of comparisons generated by '{blocking_rule_2.blocking_rule_sql}': {count:,.0f}\")\n\nblocking_rule_3 = block_on(\"email\")\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_3)\nprint(f\"Number of comparisons generated by '{blocking_rule_3.blocking_rule_sql}': {count:,.0f}\")\n\nblocking_rule_4 = block_on([\"city\", \"first_name\"])\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_4)\nprint(f\"Number of comparisons generated by '{blocking_rule_4.blocking_rule_sql}': {count:,.0f}\")\n</code></pre> <pre>\n<code>Number of comparisons generated by '(SUBSTR(l.\"first_name\", 1, 1) = SUBSTR(r.\"first_name\", 1, 1)) AND (l.\"surname\" = r.\"surname\")': 473\nNumber of comparisons generated by 'l.\"surname\" = r.\"surname\"': 1,638\nNumber of comparisons generated by 'l.\"email\" = r.\"email\"': 682\nNumber of comparisons generated by '(l.\"city\" = r.\"city\") AND (l.\"first_name\" = r.\"first_name\")': 315\n</code>\n</pre> <p>The maximum number of comparisons that you can compute will be affected by your choice of SQL backend, and how powerful your computer is.</p> <p>For linkages in DuckDB on a standard laptop, we suggest using blocking rules that create no more than about 20 million comparisons.  For Spark and Athena, try starting with fewer than a a billion comparisons, before scaling up.</p> <pre><code>blocking_rules = [blocking_rule_1, blocking_rule_2, blocking_rule_3]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <pre><code>linker.profile_columns(\"city || left(first_name,1)\")\n</code></pre> <p>Further Reading</p> <p> For a deeper dive on blocking, please refer to the Blocking Topic Guides.</p> <p> For more on the blocking tools in Splink, please refer to the Blocking API documentation.</p> <p> For more on the charts used in this tutorial, please refer to the Charts Gallery.</p>"},{"location":"demos/tutorials/03_Blocking.html#choosing-blocking-rules-to-optimise-runtime","title":"Choosing blocking rules to optimise runtime","text":"<p>To link records, we need to compare pairs of records, and decide which pairs are matches and non matches.</p> <p>For most large datasets, it is computationally intractable to compare every row with every other row, since the number of comparisons rises quadratically with the number of records.  </p> <p>Instead we rely on blocking rules, which specify which pairwise comparisons to generate.  For example, we could generate the subset of pairwise comparisons where either first name or surname matches.</p> <p>This is part of a two step process to link data:</p> <ol> <li> <p>Use blocking rules to generate candidate pairwise record comparisons</p> </li> <li> <p>Use a probabilistic linkage model to score these candidate pairs, to determine which ones should be linked</p> </li> </ol> <p>Blocking rules are the most important determinant of the performance of your linkage job.  </p> <p>When deciding on your blocking rules, you're trading off accuracy for performance:</p> <ul> <li>If your rules are too loose, your linkage job may fail.  </li> <li>If they're too tight, you may miss some valid links. </li> </ul> <p>This tutorial clarifies what blocking rules are, and how to choose good rules.</p>"},{"location":"demos/tutorials/03_Blocking.html#blocking-rules-in-splink","title":"Blocking rules in Splink","text":"<p>In Splink, blocking rules are specified as SQL expressions. </p> <p>For example, to generate the subset of record comparisons where the first name matches, we can specify the following blocking rule:</p> <p><code>l.first_name = r.first_name</code></p> <p>Since blocking rules are SQL expressions, they can be arbitrarily complex.  For example, you could create record comparisons where the initial of the first name and the surname match with the following rule:</p> <p><code>substr(l.first_name, 1,1) = substr(r.first_name, 1,1) and l.surname = r.surname</code></p> <p>As of v3.9.5, Splink also includes built in functions to construct these rules.</p> <p>For example, <code>substr(l.first_name, 1,1) = substr(r.first_name, 1,1) and l.surname = r.surname</code> can now be written as:</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\nsub_fn = \"substr(first_name, 1,1)\"\nblock_on([sub_fn, \"surname\"])\n</code></pre>"},{"location":"demos/tutorials/03_Blocking.html#devising-effective-blocking-rules","title":"Devising effective blocking rules","text":"<p>The aims of your blocking rules are twofold: 1. Eliminate enough non-matching comparison pairs so your record linkage job is small enough to compute 2. Eliminate as few truly matching pairs as possible (ideally none)</p> <p>It is usually impossible to find a single blocking rule which achieves both aims, so we recommend using multiple blocking rules.  </p> <p>When we specify multiple blocking rules, Splink will generate all comparison pairs that meet any one of the rules.</p> <p>For example, consider the following blocking rule:</p> <p><code>l.first_name = r.first_name and l.dob = r.dob</code></p> <p>This rule is likely to be effective in reducing the number of comparison pairs.  It will retain all truly matching pairs, except those with errors or nulls in either the <code>first_name</code> or <code>dob</code> fields.</p> <p>Now consider a second blocking rule:</p> <p><code>l.email and r.email</code>.</p> <p>This will retain all truly matching pairs, except those with errors or nulls in the <code>email</code> column.</p> <p>Individually, these blocking rules are problematic because they exclude true matches where the records contain typos of certain types.  But between them, they might do quite a good job.  </p> <p>For a true match to be eliminated by the use of these two blocking rules, it would have to have an error in both  email AND (first name or date of birth).  </p> <p>This is not completely implausible, but it is significantly less likely than if we'd just used a single rule.</p> <p>More generally, we can often specify multiple blocking rules such that it becomes highly implausible that a true match would not meet at least one of these blocking critera.  This is the recommended approach in Splink.  Generally we would recommend between about 3 and 10, though even more is possible.</p> <p>The question then becomes how to choose what to put in this list.</p>"},{"location":"demos/tutorials/03_Blocking.html#splink-tools-to-help-choose-your-blocking-rules","title":"Splink tools to help choose your blocking rules","text":"<p>Splink contains a number of tools to help you choose effective blocking rules.  Let's try them out, using our small test dataset:</p>"},{"location":"demos/tutorials/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-single-blocking-rule","title":"Counting the number of comparisons created by a single blocking rule","text":"<p>On large datasets, some blocking rules imply the creation of trillions of record comparisons, which would cause a linkage job to fail.</p> <p>Before using a blocking rule in a linkage job, it's therefore a good idea to count the number of records it generates to ensure it is not too loose:</p>"},{"location":"demos/tutorials/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-list-of-blocking-rules","title":"Counting the number of comparisons created by a list of blocking rules","text":"<p>As noted above, it's usually a good idea to use multiple blocking rules.  It's therefore useful to know how many record comparisons will be generated when these rules are applied.</p> <p>Since the same record comparison may be created by several blocking rules, and Splink automatically deduplicates these comparisons, we cannot simply total the number of comparisons generated by each rule individually.  </p> <p>Splink provides a chart that shows the marginal (additional) comparisons generated by each blocking rule, after deduplication:</p>"},{"location":"demos/tutorials/03_Blocking.html#understanding-why-certain-blocking-rules-create-large-numbers-of-comparisons","title":"Understanding why certain blocking rules create large numbers of comparisons","text":"<p>Finally, we can use the <code>profile_columns</code> function we saw in the previous tutorial to understand a specific blocking rule in more depth.</p> <p>Suppose we're interested in blocking on city and first initial.  </p> <p>Within each distinct value of <code>(city, first initial)</code>, all possible pairwise comparisons will be generated.</p> <p>So for instance, if there are 15 distinct records with <code>London,J</code> then these records will result in <code>n(n-1)/2 = 105</code> pairwise comparisons being generated.</p> <p>In a larger dataset, we might observe 10,000 <code>London,J</code> records, which would then be responsible for <code>49,995,000</code> comparisons.  </p> <p>These high-frequency values therefore have a disproportionate influence on the overall number of pairwise comparisons, and so it can be useful to analyse skew, as follows:</p>"},{"location":"demos/tutorials/03_Blocking.html#next-steps","title":"Next steps","text":"<p>Now we have chosen which records to compare, we can use those records to train a linkage model.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html","title":"4. Estimating model parameters","text":"<pre><code># Begin by reading in the tutorial data again\nfrom splink.duckdb.linker import DuckDBLinker\nfrom splink.datasets import splink_datasets\nimport altair as alt\ndf = splink_datasets.fake_1000\n</code></pre> <pre><code>import splink.duckdb.comparison_library as cl\n\nemail_comparison =  cl.levenshtein_at_thresholds(\"email\", 2)\nprint(email_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Email within levenshtein threshold 2 vs. anything else' of \"email\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"email_l\" IS NULL OR \"email_r\" IS NULL\n    - 'Exact match' with SQL rule: \"email_l\" = \"email_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"email_l\", \"email_r\") &lt;= 2\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <ol> <li><code>Comparison Template</code> functions which have been created for specific data types. For example, names.</li> </ol> <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\"first_name\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 1\n    - 'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9\n    - 'Jaro_winkler_similarity &gt;= 0.8' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <pre><code>from splink.duckdb.blocking_rule_library import block_on\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <p>In words, this setting dictionary says:</p> <ul> <li>We are performing a <code>dedupe_only</code> (the other options are <code>link_only</code>, or <code>link_and_dedupe</code>, which may be used if there are multiple input datasets).</li> <li>When comparing records, we will use information from the <code>first_name</code>, <code>surname</code>, <code>dob</code>, <code>city</code> and <code>email</code> columns to compute a match score.</li> <li>The <code>blocking_rules_to_generate_predictions</code> states that we will only check for duplicates amongst records where either the <code>first_name</code> or <code>surname</code> is identical.</li> <li>We have enabled term frequency adjustments for the 'city' column, because some values (e.g. <code>London</code>) appear much more frequently than others.</li> <li>We have set <code>retain_intermediate_calculation_columns</code> and <code>additional_columns_to_retain</code> to <code>True</code>  so that Splink outputs additional information that helps the user understand the calculations. If they were <code>False</code>, the computations would run faster.</li> </ul> <pre><code>deterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &amp;lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &amp;lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.00333.\nThis means that amongst all possible pairwise record comparisons, one in 300.13 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 1,664.29 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = block_on([\"first_name\", \"surname\"])\ntraining_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\n(l.\"first_name\" = r.\"first_name\") AND (l.\"surname\" = r.\"surname\")\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.541 in the m_probability of dob, level `Exact match`\nIteration 2: Largest change in params was 0.0359 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.00717 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.00156 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.000362 in probability_two_random_records_match\nIteration 6: Largest change in params was 8.64e-05 in probability_two_random_records_match\n\nEM converged after 6 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <p>In a second estimation pass, we block on dob. This allows us to estimate parameters for the <code>first_name</code> and <code>surname</code> comparisons.</p> <p>Between the two estimation passes, we now have parameter estimates for all comparisons.</p> <pre><code>from numpy import fix\n\n\ntraining_blocking_rule = block_on(\"dob\")\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.\"dob\" = r.\"dob\"\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.411 in the m_probability of surname, level `Exact match surname`\nIteration 2: Largest change in params was 0.111 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.0399 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0143 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00599 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.00275 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.00132 in probability_two_random_records_match\nIteration 8: Largest change in params was 0.000651 in probability_two_random_records_match\nIteration 9: Largest change in params was 0.000333 in probability_two_random_records_match\nIteration 10: Largest change in params was 0.000188 in probability_two_random_records_match\nIteration 11: Largest change in params was 0.000136 in the m_probability of first_name, level `All other comparisons`\nIteration 12: Largest change in params was 0.000145 in the m_probability of first_name, level `All other comparisons`\nIteration 13: Largest change in params was 0.000179 in the m_probability of first_name, level `All other comparisons`\nIteration 14: Largest change in params was 0.000207 in the m_probability of first_name, level `All other comparisons`\nIteration 15: Largest change in params was 0.000204 in the m_probability of first_name, level `All other comparisons`\nIteration 16: Largest change in params was 0.00017 in the m_probability of first_name, level `All other comparisons`\nIteration 17: Largest change in params was 0.000126 in the m_probability of first_name, level `All other comparisons`\nIteration 18: Largest change in params was 8.73e-05 in probability_two_random_records_match\n\nEM converged after 18 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>Note that Splink includes other algorithms for estimating m and u values, which are documented here.</p> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>linker.m_u_parameters_chart()\n</code></pre> <pre><code>settings = linker.save_model_to_json(\"../demo_settings/saved_model_from_demo.json\", overwrite=True)\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <p>In the above chart, we can see that about 1.3% of records in the input dataset are unlinkable at a threshold match weight of 6.11 (correponding to a match probability of around 98.6%)</p> <p>Further Reading</p> <p> For more on the model estimation tools in Splink, please refer to the Model Training API documentation.</p> <p> For a deeper dive on:</p> <ul> <li>choosing comparisons, please refer to the Comparisons Topic Guides</li> <li>the underlying model theory, please refer to the Fellegi Sunter Topic Guide</li> <li>model training, please refer to the Model Training Topic Guides (Coming Soon).</li> </ul> <p> For more on the charts used in this tutorial, please refer to the Charts Gallery.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#specifying-and-estimating-a-linkage-model","title":"Specifying and estimating a linkage model","text":"<p>In the last tutorial we looked at how we can use blocking rules to generate pairwise record comparisons.</p> <p>Now it's time to estimate a probabilistic linkage model to score each of these comparisons. The resultant match score is a prediction of whether the two records represent the same entity (e.g. are the same person).  </p> <p>The purpose of estimating the model is to learn the relative importance of different parts of your data for the purpose of data linking.  </p> <p>For example, a match on date of birth is a much stronger indicator that two records refer to the same entity than a match on gender.  A mismatch on gender may be a stronger indicate against two records referring than a mismatch on name, since names are more likely to be entered differently.</p> <p>The relative importance of different information is captured in the (partial) 'match weights', which can be learned from your data.  These match weights are then added up to compute the overall match score.</p> <p>The match weights are are derived from the <code>m</code> and <code>u</code> parameters of the underlying Fellegi Sunter model.  Splink uses various statistical routines to estimate these parameters.  Further details of the underlying theory can be found here, which will help you understand this part of the tutorial.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#specifying-a-linkage-model","title":"Specifying a linkage model","text":"<p>To build a linkage model, the user defines the partial match weights that <code>splink</code> needs to estimate.    This is done by defining how the information in the input records should be compared.</p> <p>To be concrete, here is an example comparison:</p> first_name_l first_name_r surname_l surname_r dob_l dob_r city_l city_r email_l email_r Robert Rob Allen Allen 1971-05-24 1971-06-24 nan London roberta25@smith.net roberta25@smith.net <p>What functions should we use to assess the similarity of <code>Rob</code> vs. <code>Robert</code> in the the <code>first_name</code> field?  </p> <p>Should similarity in the <code>dob</code> field be computed in the same way, or a different way?</p> <p>Your job as the developer of a linkage model is to decide what comparisons are most appropriate for the types of data you have.  </p> <p>Splink can then estimate how much weight to place on a fuzzy match of <code>Rob</code> vs. <code>Robert</code>, relative to an exact match on <code>Robert</code>, or a non-match.</p> <p>Defining these scenarios is done using <code>Comparison</code>s.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#comparisons","title":"Comparisons","text":"<p>The concept of a <code>Comparison</code> has a specific definition within Splink: it defines how data from one or more input columns is compared, using SQL expressions to assess similarity.</p> <p>For example, one <code>Comparison</code> may represent how similarity is assessed for a person's date of birth.  </p> <p>Another <code>Comparison</code> may represent the comparison of a person's name or location.</p> <p>A model is composed of many <code>Comparison</code>s, which between them assess the similarity of all of the columns being used for data linking.  </p> <p>Each <code>Comparison</code> contains two or more <code>ComparisonLevels</code> which define n discrete gradations of similarity between the input columns within the Comparison.</p> <p>As such <code>ComparisonLevels</code>are nested within <code>Comparisons</code> as follows:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on surname\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u2502    etc.\n</code></pre> <p>Our example data would therefore result in the following comparisons, for <code>dob</code> and <code>surname</code>:</p> dob_l dob_r comparison_level interpretation 1971-05-24 1971-05-24 Exact match great match 1971-05-24 1971-06-24 One character difference ok match 1971-05-24 2000-01-02 All other bad match surname_l surname_r comparison_level interpretation Rob Rob Exact match great match Rob Jane All other bad match Rob Robert All other bad match, this comparison has no notion of nicknames <p>More information about comparisons can be found here.</p> <p>We will now use these concepts to build a data linking model.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#specifying-the-model-using-comparisons","title":"Specifying the model using comparisons","text":"<p>Splink includes libraries of comparison functions to make it simple to get started. These are split into two categories:</p> <ol> <li><code>Comparison</code> functions which apply a particular fuzzy matching function. For example, levenshtein distance.</li> </ol>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#specifying-the-full-settings-dictionary","title":"Specifying the full settings dictionary","text":"<p><code>Comparisons</code> are specified as part of the Splink <code>settings</code>, a Python dictionary which controls all of the configuration of a Splink model:</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimate-the-parameters-of-the-model","title":"Estimate the parameters of the model","text":"<p>Now that we have specified our linkage model, we need to estimate the <code>probability_two_random_records_match</code>, <code>u</code>, and <code>m</code> parameters.</p> <ul> <li> <p>The <code>probability_two_random_records_match</code> parameter is the probability that two records taken at random from your input data represent a match (typically a very small number).</p> </li> <li> <p>The <code>u</code> values are the proportion of records falling into each <code>ComparisonLevel</code> amongst truly non-matching records.</p> </li> <li> <p>The <code>m</code> values are the proportion of records falling into each <code>ComparisonLevel</code> amongst truly matching records</p> </li> </ul> <p>You can read more about the theory of what these mean.</p> <p>We can estimate these parameters using unlabeled data. If we have labels, then we can estimate them even more accurately.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimation-of-probability_two_random_records_match","title":"Estimation of <code>probability_two_random_records_match</code>","text":"<p>In some cases, the <code>probability_two_random_records_match</code> will be known. For example, if you are linking two tables of 10,000 records and expect a one-to-one match, then you should set this value to <code>1/10_000</code> in your settings instead of estimating it.</p> <p>More generally, this parameter is unknown and needs to be estimated.  </p> <p>It can be estimated accurately enough for most purposes by combining a series of deterministic matching rules and a guess of the recall corresponding to those rules.  For further details of the rationale behind this appraoch see here.</p> <p>In this example, I guess that the following deterministic matching rules have a recall of about 70%:</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimation-of-u-probabilities","title":"Estimation of <code>u</code> probabilities","text":"<p>Once we have the <code>probability_two_random_records_match</code> parameter, we can estimate the <code>u</code> probabilities.</p> <p>We estimate <code>u</code> using the <code>estimate_u_using_random_sampling</code> method, which doesn't require any labels.</p> <p>It works by sampling random pairs of records, since most of these pairs are going to be non-matches. Over these non-matches we compute the distribution of <code>ComparisonLevel</code>s for each <code>Comparison</code>.</p> <p>For instance, for <code>gender</code>, we would find that the the gender matches 50% of the time, and mismatches 50% of the time. </p> <p>For <code>dob</code> on the other hand, we would find that the <code>dob</code> matches 1% of the time, has a \"one character difference\" 3% of the time, and everything else happens 96% of the time.</p> <p>The larger the random sample, the more accurate the predictions. You control this using the <code>max_pairs</code> parameter. For large datasets, we recommend using at least 10 million - but the higher the better and 1 billion is often appropriate for larger datasets.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimation-of-m-probabilities","title":"Estimation of <code>m</code> probabilities","text":"<p><code>m</code> is the trickiest of the parameters to estimate, because we have to have some idea of what the true matches are.</p> <p>If we have labels, we can directly estimate it.</p> <p>If we do not have labelled data, the <code>m</code> parameters can be estimated using an iterative maximum likelihood approach called Expectation Maximisation. </p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimating-directly","title":"Estimating directly","text":"<p>If we have labels, we can estimate <code>m</code> directly using the <code>estimate_m_from_label_column</code> method of the linker.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated.</p> <p>For example (in this tutorial we don't have labels, so we're not actually going to use this):</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#estimating-with-expectation-maximisation","title":"Estimating with Expectation Maximisation","text":"<p>This algorithm estimates the <code>m</code> values by generating pairwise record comparisons, and using them to maximise a likelihood function. </p> <p>Each estimation pass requires the user to configure an estimation blocking rule to reduce the number of record comparisons generated to a manageable level.</p> <p>In our first estimation pass, we block on <code>first_name</code> and <code>surname</code>, meaning we will generate all record comparisons that have <code>first_name</code> and <code>surname</code> exactly equal.   </p> <p>Recall we are trying to estimate the <code>m</code> values of the model, i.e. proportion of records falling into each <code>ComparisonLevel</code> amongst truly matching records.</p> <p>This means that, in this training session, we cannot estimate parameter estimates for the <code>first_name</code> or <code>surname</code> columns, since they will be equal for all the comparisons we do.</p> <p>We can, however, estimate parameter estimates for all of the other columns.  The output messages produced by Splink confirm this.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#visualising-model-parameters","title":"Visualising model parameters","text":"<p>Splink can generate a number of charts to help you understand your model.  For an introduction to these charts and how to interpret them, please see this video.</p> <p>The final estimated match weights can be viewed in the match weights chart:</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#saving-the-model","title":"Saving the model","text":"<p>We can save the model, including our estimated parameters, to a <code>.json</code> file, so we can use it in the next tutorial.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#detecting-unlinkable-records","title":"Detecting unlinkable records","text":"<p>An interesting application of our trained model that is useful to explore before making any predictions is to detect 'unlinkable' records.</p> <p>Unlinkable records are those which do not contain enough information to be linked.  A simple example would be a record containing only 'John Smith', and null in all other fields.  This record may link to other records, but we'll never know because there's not enough information to disambiguate any potential links.   Unlinkable records can be found by linking records to themselves - if, even when matched to themselves, they don't meet the match threshold score, we can be sure they will never link to anything.</p>"},{"location":"demos/tutorials/04_Estimating_model_parameters.html#next-steps","title":"Next steps","text":"<p>Now we have trained a model, we can move on to using it predict matching records.</p>"},{"location":"demos/tutorials/05_Predicting_results.html","title":"5. Predicting results","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nfrom splink.datasets import splink_datasets\nimport pandas as pd\npd.options.display.max_columns = 1000\ndf = splink_datasets.fake_1000\n</code></pre> <pre><code>linker = DuckDBLinker(df)\nlinker.load_model(\"../demo_settings/saved_model_from_demo.json\")\n</code></pre> <pre><code>df_predictions = linker.predict(threshold_match_probability=0.2)\ndf_predictions.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.655148 0.999845 4 5 Grace Grace 4 84.391685 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 4 90.597357 Hull NaN -1 0.001230 NaN 1.000000 1.000000 grace.kelly52@jones.com grace.kelly52@jones.com 3 252.361018 0 1 11.142456 0.999558 26 29 Thomas Thomas 4 84.391685 Gabriel Gabriel 4 88.441584 1976-09-15 1976-08-15 4 90.597357 Loodon NaN -1 0.001230 NaN 1.000000 1.000000 gabriel.t54@nnichls.info NaN -1 1.000000 0 2 11.142456 0.999558 28 29 Thomas Thomas 4 84.391685 Gabriel Gabriel 4 88.441584 1976-09-15 1976-08-15 4 90.597357 London NaN -1 0.212792 NaN 1.000000 1.000000 gabriel.t54@nichols.info NaN -1 1.000000 0 3 -1.153442 0.310131 37 860 Theodore Theodore 4 84.391685 Morris Marshall 0 0.237771 1978-08-19 1972-07-25 1 0.588069 Birmingham Birmingham 1 0.049200 0.0492 10.167002 1.120874 t.m39@brooks-sawyer.com NaN -1 1.000000 0 4 -1.153442 0.310131 39 860 Theodore Theodore 4 84.391685 Morris Marshall 0 0.237771 1978-08-19 1972-07-25 1 0.588069 Birmingham Birmingham 1 0.049200 0.0492 10.167002 1.120874 t.m39@brooks-sawyer.com NaN -1 1.000000 0 <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)\nclusters.as_pandas_dataframe(limit=10)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 11\nCompleted iteration 2, root rows count 1\nCompleted iteration 3, root rows count 0\n</code>\n</pre> cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 0 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 0 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 5 4 5 Grace Kelly 1991-04-26 NaN grace.kelly52@jones.com 1 NaN 6 6 6 Logan pMurphy 1973-08-01 NaN NaN 2 NaN 7 7 7 NaN NaN 2015-03-03 Portsmouth evied56@harris-bailey.net 3 0.017220 8 8 8 NaN Dean 2015-03-03 NaN NaN 3 NaN 9 8 9 Evie Dean 2015-03-03 Pootsmruth evihd56@earris-bailey.net 3 0.001230 <pre><code>sql = f\"\"\"\nselect * \nfrom {df_predictions.physical_name}\nlimit 2\n\"\"\"\nlinker.query_sql(sql)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.655148 0.999845 4 5 Grace Grace 4 84.391685 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 4 90.597357 Hull NaN -1 0.00123 NaN 1.0 1.0 grace.kelly52@jones.com grace.kelly52@jones.com 3 252.361018 0 1 11.142456 0.999558 26 29 Thomas Thomas 4 84.391685 Gabriel Gabriel 4 88.441584 1976-09-15 1976-08-15 4 90.597357 Loodon NaN -1 0.00123 NaN 1.0 1.0 gabriel.t54@nnichls.info NaN -1 1.000000 0 <p>Further Reading</p> <p> For more on the prediction tools in Splink, please refer to the Prediction API documentation.</p>"},{"location":"demos/tutorials/05_Predicting_results.html#predicting-which-records-match","title":"Predicting which records match","text":"<p>In the previous tutorial, we built and estimated a linkage model.</p> <p>In this tutorial, we will load the estimated model and use it to make predictions of which pairwise record comparisons match.</p>"},{"location":"demos/tutorials/05_Predicting_results.html#load-estimated-model-from-previous-tutorial","title":"Load estimated model from previous tutorial","text":""},{"location":"demos/tutorials/05_Predicting_results.html#predicting-match-weights-using-the-trained-model","title":"Predicting match weights using the trained model","text":"<p>We use <code>linker.predict()</code> to run the model.  </p> <p>Under the hood this will:</p> <ul> <li> <p>Generate all pairwise record comparisons that match at least one of the <code>blocking_rules_to_generate_predictions</code></p> </li> <li> <p>Use the rules specified in the <code>Comparisons</code> to evaluate the similarity of the input data</p> </li> <li> <p>Use the estimated match weights, applying term frequency adjustments where requested to produce the final <code>match_weight</code> and <code>match_probability</code> scores</p> </li> </ul> <p>Optionally, a <code>threshold_match_probability</code> or <code>threshold_match_weight</code> can be provided, which will drop any row where the predicted score is below the threshold.</p>"},{"location":"demos/tutorials/05_Predicting_results.html#clustering","title":"Clustering","text":"<p>The result of <code>linker.predict()</code> is a list of pairwise record comparisons and their associated scores. For instance, if we have input records A, B, C and D, it could be represented conceptually as: </p><pre><code>A -&amp;gt; B with score 0.9\nB -&amp;gt; C with score 0.95\nC -&amp;gt; D with score 0.1\nD -&amp;gt; E with score 0.99\n</code></pre> <p>Often, an alternative representation of this result is more useful, where each row is an input record, and where records link, they are assigned to the same cluster.</p> <p>With a score threshold of 0.5, the above data could be represented conceptually as:</p> <pre><code>ID, Cluster ID\nA,  1\nB,  1\nC,  1\nD,  2\nE,  2\n</code></pre> <p>The algorithm that converts between the pairwise results and the clusters is called connected components, and it is included in Splink.  You can use it as follows:</p>"},{"location":"demos/tutorials/05_Predicting_results.html#next-steps","title":"Next steps","text":"<p>Now we have made predictions with a model, we can move on to visualising it to understand how it is working.</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html","title":"6. Visualising predictions","text":"<pre><code># Rerun our predictions to we're ready to view the charts\nfrom splink.duckdb.linker import DuckDBLinker\nfrom splink.datasets import splink_datasets\nimport altair as alt\n\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_model(\"../demo_settings/saved_model_from_demo.json\")\ndf_predictions = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre><code>records_to_view  = df_predictions.as_record_dict(limit=5)\nlinker.waterfall_chart(records_to_view, filter_nulls=False)\n</code></pre> <pre><code>linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./scv.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre><code>df_clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)\n\nlinker.cluster_studio_dashboard(df_predictions, df_clusters, \"cluster_studio.html\", sampling_method=\"by_cluster_size\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./cluster_studio.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 11\nCompleted iteration 2, root rows count 1\nCompleted iteration 3, root rows count 0\n</code>\n</pre> <p>Further Reading</p> <p> For more on the visualisation tools in Splink, please refer to the Visualisation API documentation.</p> <p> For more on the charts used in this tutorial, please refer to the Charts Gallery</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html#visualising-predictions","title":"Visualising predictions","text":"<p>Splink contains a variety of tools to help you visualise your predictions.</p> <p>The idea is that, by developing an understanding of how your model works, you can gain confidence that the predictions it makes are sensible, or alternatively find examples of where your model isn't working, which may help you improve the model specification and fix these problems.</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html#waterfall-chart","title":"Waterfall chart","text":"<p>The waterfall chart provides a means of visualising individual predictions to understand how Splink computed the final matchweight for a particular pairwise record comparison.</p> <p>To plot a waterfall chart, the user chooses one or more records from the results of <code>linker.predict()</code>, and provides these records to the <code>linker.waterfall_chart()</code> function.</p> <p>For an introduction to waterfall charts and how to interpret them, please see this video.</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html#comparison-viewer-dashboard","title":"Comparison viewer dashboard","text":"<p>The comparison viewer dashboard takes this one step further by producing an interactive dashboard that contains example predictions from across the spectrum of match scores.</p> <p>An in-depth video describing how to interpret the dashboard can be found here.</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html#cluster-studio-dashboard","title":"Cluster studio dashboard","text":"<p>Cluster studio is an interactive dashboards that visualises the results of clustering your predictions.</p> <p>It provides examples of clusters of different sizes.  The shape and size of clusters can be indicative of problems with record linkage, so it provides a tool to help you find potential false positive and negative links.</p>"},{"location":"demos/tutorials/06_Visualising_predictions.html#next-steps","title":"Next steps","text":"<p>Now we have visualised the results of a model, we can move on to some more formal Quality Assurance procedures using labelled data.</p>"},{"location":"demos/tutorials/07_Evaluation.html","title":"7. Evaluation","text":"<pre><code># Rerun our predictions to we're ready to view the charts\nfrom splink.duckdb.linker import DuckDBLinker\nfrom splink.datasets import splink_datasets\n\nimport altair as alt\n\ndf = splink_datasets.fake_1000\nlinker = DuckDBLinker(df)\nlinker.load_model(\"../demo_settings/saved_model_from_demo.json\")\ndf_predictions = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre><code>from splink.datasets import splink_dataset_labels\n\ndf_labels = splink_dataset_labels.fake_1000_labels\ndf_labels.head(5)\nlabels_table = linker.register_labels_table(df_labels)\n</code></pre> <pre><code>linker.roc_chart_from_labels_table(labels_table)\n</code></pre> <pre><code>linker.precision_recall_chart_from_labels_table(labels_table)\n</code></pre> <pre><code>roc_table = linker.truth_space_table_from_labels_table(labels_table)\nroc_table.as_pandas_dataframe(limit=5)\n</code></pre> truth_threshold match_probability row_count p n tp tn fp fn P_rate ... precision recall specificity npv accuracy f1 f2 f0_5 p4 phi 0 -26.442571 1.096460e-08 3176.0 2031.0 1145.0 2031.0 0.0 1145.0 0.0 0.639484 ... 0.639484 1.0 0.000000 1.0 0.639484 0.780104 0.898673 0.689175 0.000000 0.000000 1 -25.337736 2.358204e-08 3176.0 2031.0 1145.0 2031.0 47.0 1098.0 0.0 0.639484 ... 0.649089 1.0 0.041048 1.0 0.654282 0.787209 0.902426 0.698082 0.143357 0.163229 2 -24.371460 4.607438e-08 3176.0 2031.0 1145.0 2031.0 154.0 991.0 0.0 0.639484 ... 0.672071 1.0 0.134498 1.0 0.687972 0.803879 0.911089 0.719244 0.366200 0.300653 3 -24.370218 4.611406e-08 3176.0 2031.0 1145.0 2031.0 199.0 946.0 0.0 0.639484 ... 0.682230 1.0 0.173799 1.0 0.702141 0.811102 0.914782 0.728531 0.433861 0.344341 4 -23.939989 6.213625e-08 3176.0 2031.0 1145.0 2031.0 230.0 915.0 0.0 0.639484 ... 0.689409 1.0 0.200873 1.0 0.711902 0.816154 0.917344 0.735071 0.474565 0.372134 <p>5 rows \u00d7 25 columns</p> <p>Further Reading</p> <p> For more on the quality assurance tools in Splink, please refer to the Evaluation API documentation.</p> <p> For more on the charts used in this tutorial, please refer to the Charts Gallery.</p> <p> For more on the Evaluation Metrics used in this tutorial, please refer to the Edge Metrics guide.</p>"},{"location":"demos/tutorials/07_Evaluation.html#evaluation-of-prediction-results","title":"Evaluation of prediction results","text":"<p>In the previous tutorial, we looked at various ways to visualise the results of our model.  </p> <p>These are useful for evaluating a linkage pipeline because they allow us to understand how our model works and verify that it is doing something sensible.  They can also be useful to identify examples where the model is not performing as expected.  </p> <p>In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis.  These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models.</p> <p>They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings.  For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset.</p>"},{"location":"demos/tutorials/07_Evaluation.html#load-in-labels","title":"Load in labels","text":"<p>The labels file contains a list of pairwise comparisons which represent matches and non-matches.</p> <p>The required format of the labels file is described here.</p>"},{"location":"demos/tutorials/07_Evaluation.html#receiver-operating-characteristic-curve","title":"Receiver operating characteristic curve","text":"<p>A ROC chart shows how the number of false positives and false negatives varies depending on the match threshold chosen.  The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.</p>"},{"location":"demos/tutorials/07_Evaluation.html#precision-recall-chart","title":"Precision-recall chart","text":"<p>An alternative representation of truth space is called a precision recall curve.</p> <p>This can be plotted as follows:</p>"},{"location":"demos/tutorials/07_Evaluation.html#truth-table","title":"Truth table","text":"<p>Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves.</p>"},{"location":"demos/tutorials/07_Evaluation.html#thats-it","title":"That's it!","text":"<p>That wraps up the Splink tutorial! Don't worry, there are still plenty of resources to help on the next steps of your Splink journey:</p> <p> For some end-to-end notebooks of Splink pipelines, check out our Examples</p> <p> For more deepdives into the different aspects of Splink, and record linkage more generally, check out our Topic Guides</p> <p> For a reference on all the functionality avalable in Splink, see our Documentation</p>"},{"location":"dev_guides/index.html","title":"Contributing to Splink","text":""},{"location":"dev_guides/index.html#contributing-to-splink","title":"Contributing to Splink","text":"<p>We encourage contributions from all users. Whether it be adding a feature, fixing a bug or fixing typos in our documentation we are extremely appreciative of the input of external contributors. Splink would not be as good without it!</p>"},{"location":"dev_guides/index.html#contributing-to-splink_1","title":"Contributing to Splink","text":"<p>Thank you for your interest in contributing to Splink! If this is your first time working with Splink, check our Contributors Guide.</p> <p>When making changes to Splink, there are a number of common operations that developers need to perform. The guides below lay out some of these common operations, and provides scripts to automate these processes. These include:</p> <ul> <li>Building a Virtual Environment - to replicate the conditions when Splink is installed by users.</li> <li>Linting and Formatting - to ensure consistent code style and to reformat code, where possible.</li> <li>Testing - to ensure all of the codebase is performing as intended.</li> <li>Building the Documentation locally - to test any changes to the docs site render correctly.</li> <li>Releasing a new package version - to walk-through the release process for new versions of Splink. This generally happens every 2 weeks, or in the case of an urgent bug fix.</li> <li>Contributing to the Splink Blog - to walk through the process of adding a post to the Splink blog.</li> </ul>"},{"location":"dev_guides/index.html#how-splink-works","title":"How Splink works","text":"<p>Splink is quite a large, complex codebase. The guides in this section lay out some of the key structures and key areas within the Splink codebase. These include:</p> <ul> <li>Understanding and Debugging Splink - demonstrates several ways of understanding how Splink code is running under the hood. This includes Splink's debug mode and logging.</li> <li>Transpilation using sqlglot - demonstrates how Splink translates SQL in order to be compatible with multiple SQL engines using the sqlglot package.</li> <li>Performance and caching - demonstrates how pipelining and caching is used to make Splink run more efficiently.</li> <li>Comparison and Comparison Level Libraries - demonstrates how <code>Comparison</code> Library and <code>ComparisonLevel</code> Library functions are structured within Splink, including how to add new functions and edit existing functions.</li> <li>Charts - demonstrates how charts are built in Splink, including how to add new charts and edit existing charts.</li> <li>User-Defined Functions - demonstrates how User Defined Functions (UDFs) are used to provide functionality within Splink that is not native to a given SQL backend.</li> <li>Settings Validation - summarises how to use and expand the existing settings schema and validation functions.</li> <li>Managing Splink's Dependencies - this section provides guidelines for managing our core dependencies and our strategy for phasing out Python versions that have reached their end-of-life.</li> </ul>"},{"location":"dev_guides/caching.html","title":"Caching and pipelining","text":""},{"location":"dev_guides/caching.html#caching-and-pipelining","title":"Caching and pipelining","text":"<p>Splink is able to run against multiple SQL backends because all of the core data linking calculations are implemented in SQL. This SQL can therefore be submitted to a chosen SQL backend for execution.</p> <p>Computations in Splink often take the form of a number of <code>select</code> statements run in sequence.</p> <p>For example, the <code>predict()</code> step:</p> <ul> <li>Inputs <code>__splink__df_concat_with_tf</code> and outputs <code>__splink__df_blocked</code></li> <li>Inputs <code>__splink__df_blocked</code> and outputs <code>__splink__df_comparison_vectors</code></li> <li>Inputs <code>__splink__df_comparison_vectors</code> and outputs <code>__splink__df_match_weight_parts</code></li> <li>Inputs <code>__splink__df_match_weight_parts</code> and outputs <code>__splink__df_predict</code></li> </ul> <p>To make this run faster, two key optimisations are implmented:</p> <ul> <li>Pipelining - combining multiple <code>select</code> statements into a single statemenet using <code>WITH</code>(CTE) queries</li> <li>Caching: saving the results of calculations so they don't need recalculating. This is especially useful because some intermediate calculations are reused multiple times during a typical Splink session</li> </ul> <p>This article discusses the general implementation of caching and pipelining. The implementation needs some alterations for certain backends like Spark, which lazily evaluate SQL by default.</p>"},{"location":"dev_guides/caching.html#implementation-pipelining","title":"Implementation: Pipelining","text":"<p>A <code>SQLPipeline</code> class manages SQL pipelining.</p> <p>A <code>SQLPipeline</code> is composed of a number of <code>SQLTask</code> objects, each of which represents a select statement.</p> <p>The code is fairly straightforward: Given a sequence of <code>select</code> statements, <code>[a,b,c]</code> they are combined into a single query as follows:</p> <pre><code>with\na as (a_sql),\nb as (b_sql),\nc_sql\n</code></pre> <p>To make this work, each statement (a,b,c) in the pipeline must refer to the previous step by name. For example, <code>b_sql</code> probably selects from the <code>a_sql</code> table, which has been aliased <code>a</code>. So <code>b_sql</code> must use the table name <code>a</code> to refer to the result of <code>a_sql</code>.</p> <p>To make this tractable, each <code>SQLTask</code> has an <code>output_table_name</code>. For example, the <code>output_table_name</code> for <code>a_sql</code> in the above example is <code>a</code>.</p> <p>For instance, in the <code>predict()</code> pipeline above, the first <code>output_table_name</code> is <code>__splink__df_blocked</code>. By giving each task a meaningful <code>output_table_name</code>, subsequent tasks can reference previous outputs in a way which is semantically clear.</p>"},{"location":"dev_guides/caching.html#implementation-caching","title":"Implementation: Caching","text":"<p>When a SQL pipeline is executed, it has two output names:</p> <ul> <li>A <code>physical_name</code>, which is the name of the materialised table in the output database e.g. <code>__splink__df_predict_cbc9833</code></li> <li>A <code>templated_name</code>, which is a descriptive name of what the table represents e.g. <code>__splink__df_predict</code></li> </ul> <p>Each time Splink runs a SQL pipeline, the SQL string is hashed. This creates a unique identifier for that particular SQL string, which serves to identify the output.</p> <p>When Splink is asked to execute a SQL string, before execution, it checks whether the resultant table already exists. If it does, it returns the table rather than recomputing it.</p> <p>For example, when we run <code>linker.predict()</code>, Splink:</p> <ul> <li>Generates the SQL tasks</li> <li>Pipelines them into a single SQL statement</li> <li>Hashes the statement to create a physical name for the outputs <code>__splink__df_predict_cbc9833</code></li> <li>Checks whether a table with physical name <code>__splink__df_predict_cbc9833</code> alredy exists in the database</li> <li>If not, executes the SQL statement, creating table <code>__splink__df_predict_cbc9833</code> in the database.</li> </ul> <p>In terms of implementation, the following happens:</p> <ul> <li>SQL statements are generated an put in the queue - see here</li> <li>Once all the tasks have been added to the queue, we call <code>_execute_sql_pipeline()</code> see here</li> <li>The SQL is combined into a single pipelined statement here</li> <li>We call <code>_sql_to_splink_dataframe()</code> which returns the table (from the cache if it already exists, or it executes the sql)</li> <li>The table is returned as a <code>SplinkDataframe</code>, an abstraction over a table in a database. See here.</li> </ul>"},{"location":"dev_guides/caching.html#some-cached-tables-do-not-need-a-hash","title":"Some cached tables do not need a hash","text":"<p>A hash is required to uniquely identify some outputs. For example, blocking is used in several places in Splink, with different results. For example, the <code>__splink__df_blocked</code> needed to estimate parameters is different to the <code>__splink__df_blocked</code> needed in the <code>predict()</code> step.</p> <p>As a result, we cannot materialise a single table called <code>__splink__df_blocked</code> in the database and reues it multiple times. This is why we append the hash of the SQL, so that we can uniquely identify the different versions of <code>__splink__df_blocked</code> which are needed in different contexts.</p> <p>There are, however, some tables which are globally unique. They only take a single form, and if they exist in the cache they never need recomputing.</p> <p>An example of this is <code>__splink__df_concat_with_tf</code>, which represents the concatenation of the input dataframes.</p> <p>To create this table, we can execute <code>_sql_to_splink_dataframe</code> with <code>materialise_as_hash</code> set to <code>False</code>. The resultant materialised table will not have a hash appended, and will simply be called <code>__splink__df_concat_with_tf</code>. This is useful, because when performing calculations Splink can now check the cache for <code>__splink__df_concat_with_tf</code> each time it is needed.</p> <p>In fact, many Splink pipelines begin with the assumption that this table exists in the database, because the first <code>SQLTask</code> in the pipeline refers to a table named <code>__splink__df_concat_with_tf</code>. To ensure this is the case, a function is used to create this table if it doesn't exist.</p>"},{"location":"dev_guides/caching.html#using-pipelining-to-optimise-splink-workloads","title":"Using pipelining to optimise Splink workloads","text":"<p>At what point should a pipeline of <code>SQLTask</code>s be executed (materialised into a physical table)?</p> <p>For any individual output, it will usually be fastest to pipeline the full linage of tasks, right from raw data through to the end result.</p> <p>However, there are many intermediate outputs which are used by many different Splink operations.</p> <p>Performance can therefore be improved by computing and saving these intermediate outputs to a cache, to ensure they don't need to be computed repeatedly.</p> <p>This is achieved by enqueueing SQL to a pipline and strategically calling <code>execute_sql_pipeline</code> to materialise results that need to cached.</p>"},{"location":"dev_guides/debug_modes.html","title":"Understanding and debugging Splink","text":""},{"location":"dev_guides/debug_modes.html#understanding-and-debugging-splinks-computations","title":"Understanding and debugging Splink's computations","text":"<p>Splink contains tooling to help developers understand the underlying computations, how caching and pipelining is working, and debug problems.</p> <p>There are two main mechanisms: <code>debug_mode</code>, and setting different logging levels</p>"},{"location":"dev_guides/debug_modes.html#debug-mode","title":"Debug mode","text":"<p>You can turn on debug mode by setting <code>linker.debug_mode = True</code>.</p> <p>This has the following effects:</p> <ul> <li>Each step of Splink's calculations are executed in turn. That is, pipelining is switched off.</li> <li>The SQL statements being executed by Splink are displayed</li> <li>The results of the SQL statements are displayed in tabular format</li> </ul> <p>This is probably the best way to understand each step of the calculations being performed by Splink - because a lot of the implementation gets 'hidden' within pipelines for performance reasons.</p> <p>Note that enabling debug mode will dramatically reduce Splink's performance!</p>"},{"location":"dev_guides/debug_modes.html#logging","title":"Logging","text":"<p>Splink has a range of logging modes that output information about what Splink is doing at different levels of verbosity.</p> <p>Unlike debug mode, logging doesn't affect the performance of Splink.</p>"},{"location":"dev_guides/debug_modes.html#logging-levels","title":"Logging levels","text":"<p>You can set the logging level with code like <code>logging.getLogger(\"splink\").setLevel(desired_level)</code> although see notes below about gotyas.</p> <p>The logging levels in Splink are:</p> <ul> <li><code>logging.INFO</code> (<code>20</code>): This outputs user facing messages about the training status of Splink models</li> <li><code>15</code>: Outputs additional information about time taken and parameter estimation</li> <li><code>logging.DEBUG</code> (<code>10</code>): Outputs information about the names of the SQL statements executed</li> <li><code>logging.DEBUG</code> (<code>7</code>): Outputs information about the names of the components of the SQL pipelines</li> <li><code>logging.DEBUG</code> (<code>5</code>): Outputs the SQL statements themselves</li> </ul>"},{"location":"dev_guides/debug_modes.html#how-to-control-logging","title":"How to control logging","text":"<p>Note that by default Splink sets the logging level to <code>INFO</code> on initialisation</p>"},{"location":"dev_guides/debug_modes.html#with-basic-logging","title":"With basic logging","text":"<pre><code>import logging\nlinker = DuckDBLinker(df, settings, set_up_basic_logging=False)\n\n# This must come AFTER the linker is intialised, because the logging level\n# will be set to INFO\nlogging.getLogger(\"splink\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"dev_guides/debug_modes.html#without-basic-logging","title":"Without basic logging","text":"<pre><code># This code can be anywhere since set_up_basic_logging is False\nimport logging\nlogging.basicConfig(format=\"%(message)s\")\nsplink_logger = logging.getLogger(\"splink\")\nsplink_logger.setLevel(logging.INFO)\n\nlinker = DuckDBLinker(df, settings, set_up_basic_logging=False)\n</code></pre>"},{"location":"dev_guides/dependency_management.html","title":"Splink Dependency Management","text":""},{"location":"dev_guides/dependency_management.html#managing-splinks-dependencies","title":"Managing Splink's Dependencies","text":"<p>This page highlights the importance of package versioning and proposes that we use a \"sunsetting\" strategy for updating our support python and dependency versions as they reach end-of-life.</p> <p>Additionally, it lays out some rough guidelines for us to follow when addresses future package conflicts and issues arises from antiquated dependency versions.</p>"},{"location":"dev_guides/dependency_management.html#package-versioning-policy","title":"Package Versioning Policy","text":"<p>Monitoring package versioning within Splink is important. It ensures that the project can be used by as wide a group of individuals as possible, without wreaking havoc on our issues log.</p> <p>Below is a rough summary of versioning and some complimentary guidelines detailing how we should look to deal with dependency management going forward.</p>"},{"location":"dev_guides/dependency_management.html#benefits-to-effective-versioning","title":"Benefits to Effective Versioning","text":"<p>Effective versioning is crucial for ensuring Splink's compatibility across diverse techical ecosystems and seamless integration with various Python versions and cloud tools. Key advantages include:</p> <ul> <li>Faster dependency resolution with <code>poetry lock</code>.</li> <li>Reduces dependency conflicts across systems.</li> </ul>"},{"location":"dev_guides/dependency_management.html#versioning-guidance","title":"Versioning Guidance","text":""},{"location":"dev_guides/dependency_management.html#establish-minimum-supported-versions","title":"Establish Minimum Supported Versions","text":"<ul> <li>Align with Python Versions: Select the minimum required versions for dependencies based on the earliest version of Python we plan to support. This approach is aligned with our policy on Sunsetting End-of-Life Python Versions, ensuring Splink remains compatible across a broad spectrum of environments.</li> <li>Document Reasons:  Where appropriate, clearly document why specific versions are chosen as minimums, including any critical features or bug fixes that dictate these choices. We should look to do this in pull requests implementing the change and as comments in <code>pyproject.toml</code>. Doing so allows us to easily track versioning decisions.</li> </ul>"},{"location":"dev_guides/dependency_management.html#prefer-open-version-constraints","title":"Prefer Open Version Constraints","text":"<ul> <li>Use Open Upper Bounds: Wherever feasible, avoid setting an upper version limit for a dependency. This reduces compatibility conflicts with external packages and allows the user to decide their versioning strategy at the application level.</li> <li>Monitor Compatibility: Actively monitor the development of our core dependencies to anticipate significant updates (such as new major versions) that might necessitate code changes. Within Splink, this is particularly relevant for both SQLGlot and  DuckDB, that (semi)frequently release new, breaking changes.</li> </ul>"},{"location":"dev_guides/dependency_management.html#compatibility-checks","title":"Compatibility Checks","text":"<ul> <li>Automated Testing: Use Continuous Integration (CI) to help test the latest python and package versions. This helps identify compatibility issues early.</li> <li>Matrix Testing: Test against a matrix of dependencies or python versions to ensure broad compatibility. pytest_run_tests_with_cache.yml is currently our broad compatability check for supported versions of python.</li> </ul>"},{"location":"dev_guides/dependency_management.html#handling-breaking-changes","title":"Handling Breaking Changes","text":"<ul> <li>Temporary Version Pinning for Major Changes: In cases where a dependency introduces breaking changes that we cannot immediately accommodate, we should look to temporarily pin to a specific version or version range until we have an opportunity to update Splink.</li> <li>Adaptive Code Changes: When feasible, adapt code to be compatible with new major versions of dependencies. This may include conditional logic to handle differences across versions. An example of this can be found within <code>input_column.py</code>, where we adjust how column identifiers are extracted from SQLGlot based on its version.</li> </ul>"},{"location":"dev_guides/dependency_management.html#documentation-and-communication","title":"Documentation and Communication","text":"<ul> <li>Clear Documentation: Clearly log installation instructions within the Getting Started section of our documentation. This should cover not only standard installation procedures but also specialised instructions, for instance, installing a -less version of Splink, for locked down environments.</li> <li>Log Dependency Changes in the Changelog: Where dependencies are adjusted, ensure that changes are logged within <code>CHANGELOG.md</code>. This can help simplify debugging and creates a guide that can be easily referenced.</li> </ul>"},{"location":"dev_guides/dependency_management.html#user-support-and-feedback","title":"User Support and Feedback","text":"<ul> <li>Issue Tracking: Actively track and address issues related to dependency compatibility. Where users are having issues, have them report their package versions through either <code>pip freeze</code> or <code>pip-chill</code>, so we can more easily identify what may have caused the problem.</li> <li>Feedback Loops: Encourage feedback from users regarding compatibility and dependency issues. Streamline the reporting process in our issues log.</li> </ul>"},{"location":"dev_guides/dependency_management.html#sunsetting-end-of-life-python-versions","title":"Sunsetting End-of-Life Python Versions","text":"<p>In alignment with the Python community's practices, we are phasing out support for Python versions that have hit end-of-life and are no longer maintained by the core Python development team. This decision ensures that Splink remains secure, efficient, and up-to-date with the latest Python features and improvements.</p> <p>Our approach mirrors that of key package maintainers, such as the developers behind Numpy. The Numpy developers have kindly pulled together NEP 29, their guidelines for python version support. This outlines a recommended framework for the deprecation of outdated Python versions.</p>"},{"location":"dev_guides/dependency_management.html#benefits-of-discontinuing-support-for-older-python-versions","title":"Benefits of Discontinuing Support for Older Python Versions:","text":"<ul> <li>Enhanced Tooling: Embracing newer versions enables the use of advanced Python features. For python 3.8, these include protocols, walrus operators, and improved type annotations, amongst others.</li> <li>Fewer Dependabot Alerts: Transitioning away from older Python versions reduces the volume of alerts associated with legacy package dependencies.</li> <li>Minimised Package Conflicts: Updating python decreases the necessity for makeshift solutions to resolve dependency issues with our core dependencies, fostering a smoother integration with tools like Poetry.</li> </ul> <p>For a comprehensive rationale behind upgrading, the article \"It's time to stop using python 3.7\" offers an insightful summary.</p>"},{"location":"dev_guides/dependency_management.html#implementation-timeline","title":"Implementation Timeline:","text":"<p>The cessation of support for major Python versions post-end-of-life will not be immediate but will instead be phased in gradually over the months following their official end-of-life designation.</p> <p>Proposed Workflow for Sunsetting Major Python Versions:</p> <ol> <li>Initial Grace Period: We propose a waiting period of approximately six months post-end-of-life before initiating the upgrade process. This interval:<ul> <li>Mitigates potential complications arising from system-wide Python updates across major cloud distributors and network administrators.</li> <li>Provides a window to inform users about the impending deprecation of older versions.</li> </ul> </li> <li>Following the Grace Period:<ul> <li>Ensure the upgrade process is seamless and devoid of critical issues, leveraging the backward compatibility strengths of newer Python versions.</li> <li>Address any bugs discovered during the upgrade process.</li> <li>Update <code>pyproject.toml</code> accordingly. Pull requests updating our supported versions should be clearly marked with the <code>[DEPENDENCIES]</code> tag and <code>python_version_update</code> label for straightforward tracking.</li> </ul> </li> </ol>"},{"location":"dev_guides/dependency_management.html#pythons-development-cycle","title":"Python's Development Cycle:","text":"<p>A comprehensive summary of Python's development cycle is available on the Python Developer's Guide. This includes a chart outlining the full release cycle up to 2029:</p> <p></p> <p>As it stands, support for Python 3.8 will officially end in October of 2024. Following an initial grace period of around six months, we will then look to phase out support.</p> <p>We will look to regularly review this page and update Splink's dependencies accordingly.</p>"},{"location":"dev_guides/spark_pipelining_and_caching.html","title":"Spark caching","text":""},{"location":"dev_guides/spark_pipelining_and_caching.html#caching-and-pipelining-in-spark","title":"Caching and pipelining in Spark","text":"<p>This article assumes you've read the general guide to caching and pipelining.</p> <p>In Spark, some additions have to be made to this general pattern because all transformation in Spark are lazy.</p> <p>That is, when we call <code>df = spark.sql(sql)</code>, the <code>df</code> is not immediately computed.</p> <p>Furthermore, even when an action is called, the results aren't automatically persisted by Spark to disk. This differs from other backends, which execute SQL as a <code>create table</code> statement, meaning that the result is automatically saved.</p> <p>This interferes with caching, because Splink assumes that when the the function <code>_execute_sql_against_backend()</code> is called, this will be evaluted greedily (immediately evaluated) AND the results will be saved to the 'database'.</p> <p>Another quirk of Spark is that it chunks work up into tasks. This is relevant for two reasons:</p> <ul> <li>Tasks can suffer from skew, meaning some take longer than others, which can be bad from a performance point of view.</li> <li>The number of tasks and how data is partitioned controls how many files are output when results are saved. Some Splink operations results in a very large number of small files which can take a long time to read and write, relative to the same data stored in fewer files.</li> </ul> <p>Repartitioning can be used to rebalance workloads (reduce skew) and to avoid the 'many small files' problem.</p>"},{"location":"dev_guides/spark_pipelining_and_caching.html#spark-specific-modifications","title":"Spark-specific modifications","text":"<p>The logic for Spark is captured in the implementation of <code>_execute_sql_against_backend()</code> in the spark_linker.py.</p> <p>This has three roles:</p> <ul> <li>It determines how to save result - using either <code>persist</code>, <code>checkpoint</code> or saving to <code>.parquet</code>, with <code>.parquet</code> being the default.</li> <li>It determines which results to save. Some small results such <code>__splink__m_u_counts</code> are immediately converted using <code>toPandas()</code> rather than being saved. This is because saving to disk and reloading is expensive and unnecessary.</li> <li>It chooses which Spark dataframes to repartition to reduce the number of files which are written/read</li> </ul> <p>Note that repartitioning and saving is independent. Some dataframes are saved without repartitioning. Some dataframes are repartitioned without being saved.</p>"},{"location":"dev_guides/transpilation.html","title":"Transpilation using sqlglot","text":""},{"location":"dev_guides/transpilation.html#sql-transpilation-in-splink-and-how-we-support-multiple-sql-backends","title":"SQL Transpilation in Splink, and how we support multiple SQL backends","text":"<p>In Splink, all the core data linking algorithms are implemented in SQL. This allows computation to be offloaded to a SQL backend of the users choice.</p> <p>One difficulty with this paradigm is that SQL implementations differ - the functions available in (say) the Spark dialect of SQL differ from those available in DuckDB SQL. And to make matters worse, functions with the same name may behave differently (e.g. different arguments, arguments in different orders, etc.).</p> <p>Splink therefore needs a mechanism of writing SQL statements that are able to run against all the target SQL backends (engines).</p> <p>Details are as follows:</p>"},{"location":"dev_guides/transpilation.html#1-core-data-linking-algorithms-are-splink","title":"1. Core data linking algorithms are Splink","text":"<p>Core data linking algorithms are implemented in 'backend agnostic' SQL. So they're written using basic SQL functions that are common across the available in all the target backends, and don't need any translation.</p> <p>It has been possible to write all of the core Splink logic in SQL that is consistent between dialects.</p> <p>However, this is not the case with <code>Comparisons</code>, which tend to use backend specific SQL functions like <code>jaro_winker</code>, whose function names and signatures differ between backends.</p>"},{"location":"dev_guides/transpilation.html#2-user-provided-sql-is-interpolated-into-these-dialect-agnostic-sql-statements","title":"2. User-provided SQL is interpolated into these dialect-agnostic SQL statements","text":"<p>The user provides custom SQL is two places in Splink:</p> <ol> <li>Blocking rules</li> <li>The <code>sql_condition</code> (see here) provided as part of a <code>Comparison</code></li> </ol> <p>The user is free to write this SQL however they want.</p> <p>It's up to the user to ensure the SQL they provide will execute successfully in their chosen backend. So the <code>sql_condition</code> must use functions that exist in the target execution engine</p> <p>The custom SQL is interpolated into the the SQL statements generated by Splink.</p> <p>Users are also able to use the <code>comparison_level_library</code> and <code>comparison_library</code> for their chosen backend.</p> <p>These are backend specific and are imported like <code>from splink.spark.comparison_level_library import jaro_winkler_level</code>. This ensures that the syntax matches the chosen execution backend</p>"},{"location":"dev_guides/transpilation.html#3-backends-can-implement-transpilation-and-or-dialect-steps-to-further-transform-the-sql-if-needed","title":"3. Backends can implement transpilation and or dialect steps to further transform the SQL if needed","text":"<p>Occasionally some modifications are needed to the SQL to ensure it executes against the target backend.</p> <p><code>sqlglot</code> is used for this purpose. For instance, a custom dialect is implemented in the sparklinker.</p> <p>A transformer is implemented in the Athena linker.</p>"},{"location":"dev_guides/udfs.html","title":"User-Defined Functions","text":""},{"location":"dev_guides/udfs.html#user-defined-functions","title":"User Defined Functions","text":"<p>User Defined Functions (UDFs) are functions that can be created to add functionality to a given SQL backend that does not already exist. These are particularly useful within Splink as it supports multiple SQL engines each with different inherent functionalty. UDFs are an important tool for creating consistent functionality across backends.</p> <p>For example, DuckDB has an in-built string comparison function for Jaccard similarity whereas Spark SQL doesn't have an equivalent function. Therefore, a UDF is required to use functions like jaccard_at_thresholds() and jaccard_level() with a Spark backend.</p>"},{"location":"dev_guides/udfs.html#spark","title":"Spark","text":"<p>Spark supports UDFs written in Scala and Java.</p> <p>Splink currently uses UDFs written in Scala and are implemented as follows:</p> <ul> <li>The UDFs are created in a separate repository, splink_scalaudfs, with the Scala functions being defined in Similarity.scala. </li> <li>The functions are then stored in a Java Archive (JAR) file - for more on JAR files, see the java documentation.</li> <li>Once the JAR file containing the UDFs has been created, it is copied across to the spark_jars folder in Splink.</li> <li>Specify the the correct jar location within Splink.</li> <li>UDFS are then registered within the Spark Linker.</li> </ul> <p>Now the Spark UDFs have been successfully registered, they can be used in SparkSQL. For example, </p> <pre><code>jaccard(\"name_column_1\", \"name_column_2\") &gt;= 0.9\n</code></pre> <p>which provides the basis for functions such as jaccard_at_thresholds() and jaccard_level().</p>"},{"location":"dev_guides/udfs.html#duckdb","title":"DuckDB","text":"<p>Python UDFs can be registered to a DuckDB connection from version 0.8.0 onwards.</p> <p>The documentation is here, an examples are here.  Note that these functions should be registered against the duckdb connection provided to the linker using <code>connection.create_function</code>.</p> <p>Note that performance will generally be substantially slower than using native DuckDB functions.  Consider using vectorised UDFs were possible - see here.</p>"},{"location":"dev_guides/udfs.html#athena","title":"Athena","text":"<p>Athena supports UDFs written in Java, however these have not yet been implemented in Splink.</p>"},{"location":"dev_guides/udfs.html#sqlite","title":"SQLite","text":"<p>Python UDFs can be registered to a sqlite connection using the <code>create_function</code> function.  An example is as follows:</p> <pre><code>from rapidfuzz.distance.Levenshtein import distance\nconn = sqlite3.connect(\":memory:\")\nconn.create_function(\"levenshtein\", 2, distance)\n</code></pre> <p>The function <code>levenshtein</code> is now available to use as a Python function</p>"},{"location":"dev_guides/changing_splink/blog_posts.html","title":"Contributing to the Splink Blog","text":""},{"location":"dev_guides/changing_splink/blog_posts.html#contributing-to-the-splink-blog","title":"Contributing to the Splink Blog","text":"<p>Thanks for considering making a contribution to the Splink Blog! We are keen to use this blog as a forum all things data linking and Splink!</p> <p>This blog, and the docs as a whole, are built using the fantastic mkdocs-material, to understand more about how the blog works under the hood checkout out the mkdocs-material blog documentation.</p> <p>For more general guidance for contributing to Splink, check out our Contributor Guide.</p>"},{"location":"dev_guides/changing_splink/blog_posts.html#adding-a-blog-post","title":"Adding a blog post","text":"<p>The easiest way to get started with a blog post is to make a copy of one of the pre-existing blog posts and make edits from there. There is a metadata in the section at the top of each post which should be updated with the post date, authors and the category of the post (this is a tag system to make posts easier to find).</p> <p>Blog posts are ordered by date, so change the name of your post markdown file to be a recent date (YYYY-MM-DD format) to make sure it appears at the top of the blog.</p> <p>Note</p> <p>In this blog we want to make content as easily digestible as possible. We encourage breaking up and big blocks of text into sections and using visuals/emojis/gifs to bring your post to life!</p>"},{"location":"dev_guides/changing_splink/blog_posts.html#adding-a-new-author-to-the-blogs","title":"Adding a new author to the blogs","text":"<p>If you are a new author, you will need to add yourself to the .authors.yml file.</p>"},{"location":"dev_guides/changing_splink/blog_posts.html#testing-your-changes","title":"Testing your changes","text":"<p>Once you have made a first draft, check out how the deployed blog will look by building the docs locally.</p>"},{"location":"dev_guides/changing_splink/build_docs_locally.html","title":"Building Docs","text":""},{"location":"dev_guides/changing_splink/build_docs_locally.html#building-docs-locally","title":"Building docs locally","text":"<p>To rapidly build the documentation and immediately see changes you've made you can use this script:</p> <pre><code>source scripts/make_docs_locally.sh\n</code></pre> <p>This is much faster than waiting for github actions to run if you're trying to make fiddly changes to formatting etc.</p> <p>The Splink repo contains a working <code>requirements.txt</code> for building the docs, or a more complete version:</p> Requirements file <pre><code>attrs==22.2.0\nbeautifulsoup4==4.11.2\nbleach==6.0.0\ncertifi==2022.12.7\ncharset-normalizer==3.0.1\nclick==8.1.3\ncolorama==0.4.6\ndefusedxml==0.7.1\nEditorConfig==0.12.3\nfastjsonschema==2.16.2\nghp-import==2.1.0\ngitdb==4.0.10\nGitPython==3.1.31\ngriffe==0.25.5\nidna==3.4\nimportlib-metadata==6.0.0\nJinja2==3.0.3\njsbeautifier==1.14.7\njsonschema==4.17.3\njsonschema2md==0.4.0\njupyter_client==8.0.3\njupyter_core==5.2.0\njupyterlab-pygments==0.2.2\nMarkdown==3.3.7\nMarkupSafe==2.1.2\nmergedeep==1.3.4\nmistune==2.0.5\nmkdocs==1.4.2\nmkdocs-autorefs==0.4.1\nmkdocs-click==0.8.0\nmkdocs-gen-files==0.4.0\nmkdocs-include-markdown-plugin==4.0.3\nmkdocs-material==8.5.11\nmkdocs-material-extensions==1.1.1\nmkdocs-mermaid2-plugin==0.6.0\nmkdocs-monorepo-plugin==1.0.4\nmkdocs-schema-reader==0.11.1\nmkdocs-semiliterate==0.7.0\nmkdocs-simple-plugin==2.1.2\nmkdocstrings==0.20.0\nmkdocstrings-python==0.8.3\nmkdocstrings-python-legacy==0.2.3\nmknotebooks==0.7.1\nnbclient==0.7.2\nnbconvert==7.2.9\nnbformat==5.7.3\npackaging==23.0\npandocfilters==1.5.0\nplatformdirs==3.0.0\nPygments==2.14.0\npymdown-extensions==9.9.2\npyrsistent==0.19.3\npython-dateutil==2.8.2\npython-slugify==8.0.0\npytkdocs==0.16.1\nPyYAML==6.0\npyyaml_env_tag==0.1\npyzmq==25.0.0\nrequests==2.28.2\nsix==1.16.0\nsmmap==5.0.0\nsoupsieve==2.4\ntext-unidecode==1.3\ntinycss2==1.2.1\ntornado==6.2\ntraitlets==5.9.0\nurllib3==1.26.14\nwatchdog==2.2.1\nwebencodings==0.5.1\nzipp==3.14.0\n</code></pre>"},{"location":"dev_guides/changing_splink/building_env_locally.html","title":"Building a Virtual Environment","text":""},{"location":"dev_guides/changing_splink/building_env_locally.html#creating-a-virtual-environment-for-splink","title":"Creating a Virtual Environment for Splink","text":""},{"location":"dev_guides/changing_splink/building_env_locally.html#managing-dependencies-with-poetry","title":"Managing Dependencies with Poetry","text":"<p>Splink utilises <code>poetry</code> for managing its core dependencies, offering a clean and effective solution for tracking and resolving any ensuing package and version conflicts.</p> <p>You can find a list of Splink's core dependencies within the pyproject.toml file.</p>"},{"location":"dev_guides/changing_splink/building_env_locally.html#fundamental-commands-in-poetry","title":"Fundamental Commands in Poetry","text":"<p>Below are some useful commands to help in the maintenance and upkeep of the pyproject.toml file.</p> <p>Adding Packages - To incorporate a new package into Splink:   </p><pre><code>poetry add &lt;package-name&gt;\n</code></pre> - To specify a version when adding a new package:   <pre><code>poetry add &lt;package-name&gt;==&lt;version&gt;\n# Add quotes if you want to use other equality calls\npoetry add \"&lt;package-name&gt; &gt;= &lt;version&gt;\"\n</code></pre> <p>Modifying Packages - To remove a package from the project:   </p><pre><code>poetry remove &lt;package-name&gt;\n</code></pre> - Updating an existing package to a specific version:   <pre><code>poetry add &lt;package-name&gt;==&lt;version&gt;\npoetry add \"&lt;package-name&gt; &gt;= &lt;version&gt;\"\n</code></pre> - To update an existing package to the latest version:   <pre><code>poetry add &lt;package-name&gt;==&lt;version&gt;\npoetry update &lt;package-name&gt;\n</code></pre>   Note: Direct updates can also be performed within the pyproject.toml file. <p>Locking the Project - To update the existing <code>poetry.lock</code> file, thereby locking the project to ensure consistent dependency installation across different environments:   </p><pre><code>poetry lock\n</code></pre>   Note: This should be used sparingly due to our loose dependency requirements and the resulting time to solve the dependency graph. If you only need to update a single depdendency, update it using <code>poetry add &lt;pkg&gt;==&lt;version&gt;</code> instead. <p>Installing Dependencies - To install project dependencies as per the lock file:   </p><pre><code>poetry install\n</code></pre> - For optional dependencies, additional flags are required. For instance, to install dependencies along with Spark support:   <pre><code>poetry install -E spark\n</code></pre> <p>A comprehensive list of Poetry commands is available in the Poetry documentation.</p>"},{"location":"dev_guides/changing_splink/building_env_locally.html#automating-virtual-environment-creation","title":"Automating Virtual Environment Creation","text":"<p>To streamline the creation of a virtual environment via <code>venv</code>, you may use the create_venv.sh script.</p> <p>This script facilitates the automatic setup of a virtual environment, with the default environment name being venv.</p> <p>Default Environment Creation: </p><pre><code>source scripts/create_venv.sh\n</code></pre> <p>Specifying a Custom Environment Name: </p><pre><code>source scripts/create_venv.sh &lt;name_of_venv&gt;\n</code></pre>"},{"location":"dev_guides/changing_splink/lint_and_format.html","title":"Linting and Formatting","text":""},{"location":"dev_guides/changing_splink/lint_and_format.html#linting-your-code","title":"Linting your code","text":"<p>For linting, we currently make use of ruff to perform checks, and if desired, some fixes for your code.</p> <p>For a standard formatter, we use black against the code base.</p> <p>These are used to ensure we produce readable, maintainable, and more consistent code.</p> <p>To quickly run both the linter and formatter, you can source the linting bash script (shown below). The -f flag can be called to run automatic fixes with ruff. If you simply wish for ruff to print the errors it finds to the console, remove this flag.</p> <p></p><pre><code>source scripts/lint_and_format.sh -f  # with the fix flag\n</code></pre> <pre><code>source scripts/lint_and_format.sh  # without\n</code></pre> You can also run ruff and black separately from a terminal instance. <p>Black can be run using: <code>black .</code> or to direct it at specific folders: <code>black splink/</code></p> <p>and ruff requires: <code>ruff --fix .</code> for automatic fixes and error printouts or <code>ruff --show-source .</code> for error checking.</p>"},{"location":"dev_guides/changing_splink/lint_and_format.html#error-suppression","title":"Error Suppression","text":"<p>Should the linter be screaming at you about an \"error\" that you do not wish to change within the code base, you can ignore a single instance of a violation by using the <code># noqa [ERROR CODE]</code> flag at the end of a code line.</p> <p>For example, let's say we have a <code>print()</code> statement buried somewhere in our code that we wish to keep, as it's useful in detecting whether a function has worked as expected.</p> <p>We can remove the lint detection of this line by adding any of the following flags: </p><pre><code># Ignore T201.\nprint(\"Ignore me!\")  # noqa: T201\n\n# Ignore T201 and T203.\nprint(\"Ignoring multiple rules\")  # noqa: T201, T201\n\n# Ignore _all_ violations.\nprint(\"Is there any point in this linter?\")  # noqa\n</code></pre> <p>Another useful example of this can be found within our backend comparison scripts. As we are merely importing and not subsequently using a series of functions and classes, the linter throw an error - F401.</p> <p>To suppress this, we simply add the <code># noqa: F401</code> flag as a comment next to the code chunk or line we wish to suppress checks for.</p> <p>For a more detailed overview of error suppression, see the ruff documentation.</p>"},{"location":"dev_guides/changing_splink/lint_and_format.html#additional-rules","title":"Additional Rules","text":"<p>ruff contains an extensive arsenal of linting rules and techniques that can be applied.</p> <p>Should there be a formatting concern that you feel hasn't been taken into account, please do check the full set of rules to determine whether this is supported by ruff and subsequently raise a pull request featuring this new rule.</p> <p>Adding Additional Rules</p> <p>To add additional rules to the linter, you simply need to:</p> <ol> <li>Open up pyproject.toml and navigate to <code>[tool.ruff]</code>.</li> <li>In the <code>select</code> block where we currently have <code># pyflakes</code>, <code># Pycodestyle</code>, etc. simply add your additional argument code, using the ruff rules page as a reference.</li> <li>Add in a linting violation and run the <code>lint_and_format.sh</code> script to test your change is working as expected.</li> </ol>"},{"location":"dev_guides/changing_splink/releases.html","title":"Releasing a Package Version","text":""},{"location":"dev_guides/changing_splink/releases.html#releasing-a-new-version-of-splink","title":"Releasing a new version of Splink","text":"<p>Splink is regularly updated with releases to add new features or bug fixes to the package.</p> <p>Below are the steps for releasing a new version of Splink:</p> <ol> <li>On a new branch, update pyproject.toml and init.py with the latest version.</li> <li>Update CHANGELOG.md. This consists of adding a heading for the new release below the 'Unreleased' heading, with the new version and date. Additionally the links at the bottom of the file for 'unreleased' and the new version should be updated.</li> <li>Open a pull request to merge the new branch with the master branch (the base branch).</li> <li> <p>Once the pull request has been approved, merge the changes and generate a new release in the releases section of the repo, including:</p> </li> <li> <p>Choosing a new release tag (which matches your updates to pyproject.toml and init.py). Ensure that your release tag follows semantic versioning. The target branch should be set to master.</p> </li> </ol> <p>)</p> <ul> <li>Generating release notes. This can be done automatically by pressing the   button. </li> </ul> <p>This will give you release notes based off the Pull Requests which have been merged since the last release.</p> <p>For example </p> <ul> <li>Publish as the latest release</li> </ul> <p></p> <p>Now your release should be published to pypi.</p>"},{"location":"dev_guides/changing_splink/testing.html","title":"Testing","text":"","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#testing-in-splink","title":"Testing in Splink","text":"<p>Tests in Splink make use of the pytest framework. You can find the tests themselves in the tests folder.</p> <p>Splink tests can be broadly categorised into three sets:</p> <ul> <li>'Core' tests - these are tests which test some specific bit of functionality which does not depend on any specific SQL dialect. They are usually unit tests - examples are testing <code>InputColumn</code> and testing the latitude-longitude distance calculation.</li> <li>Backend-agnostic tests - these are tests which run against some SQL backend, but which are written in such a way that they can run against many backends by making use of the backend-agnostic testing framework. The majority of tests are of this type.</li> <li>Backend-specific tests - these are tests which run against a specific SQL backend, and test some feature particular to this backend. There are not many of these, as Splink is designed to run very similarly independent of the backend used.</li> </ul> <p>Info</p> <p>We currently do not have support for testing the <code>athena</code> backend, due to the complication of needing a connection to an AWS account. All other backends have testing available.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests","title":"Running tests","text":"","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests-locally","title":"Running tests locally","text":"<p>To run tests locally, simply run: </p><pre><code>python3 -m pytest tests/\n</code></pre> or alternatively <pre><code>pytest tests/\n</code></pre> <p>To run a single test file, append the filename to the <code>tests/</code> folder call, for example: </p><pre><code>pytest tests/test_u_train.py\n</code></pre> or for a single test, additionally append the test name after a pair of colons, as: <pre><code>pytest tests/test_u_train.py::test_u_train_multilink\n</code></pre> Further useful pytest options <p>There may be many warnings emitted, for instance by library dependencies, cluttering your output in which case you can use <code>--disable-pytest-warnings</code> or <code>-W ignore</code> so that these will not be displayed. Some additional command-line options that may be useful:</p> <ul> <li><code>-s</code> to disable output capture, so that test output is displayed in the terminal in all cases</li> <li><code>-v</code> for verbose mode, where each test instance will be displayed on a separate line with status</li> <li><code>-q</code> for quiet mode, where output is extremely minimal</li> <li><code>-x</code> to fail on first error/failure rather than continuing to run all selected tests<ul> <li> </li> <li><code>-m some_mark</code> run only those tests marked with <code>some_mark</code> - see below for useful options here</li> </ul> <p>For instance usage might be: </p><pre><code># ignore warnings, display output\npytest -W ignore -s tests/\n</code></pre> <p>or </p><pre><code># ignore warnings, verbose output, fail on first error/failure\npytest -W ignore -v -x tests/\n</code></pre> <p>You can find a host of other available options using pytest's in-built help: </p><pre><code>pytest -h\n</code></pre>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests-for-specific-backends-or-backend-groups","title":"Running tests for specific backends or backend groups","text":"<p>You may wish to run tests relating to to specific backends, tests which are backend-independent, or any combinations of these. Splink allows for various combinations by making use of <code>pytest</code>'s <code>mark</code> feature.</p> <p>If when you invoke pytest you pass no marks explicitly, there will be an implicit mark of <code>default</code>, as per the pyproject.toml pytest.ini configuration.</p> <p>The available options are:</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-core-tests","title":"Run core tests","text":"<p>Option for running only the backend-independent 'core' tests:</p> <ul> <li><code>pytest tests/ -m core</code> - run only the 'core' tests, meaning those without dialect-dependence. In practice this means any test that hasn't been decorated using <code>mark_with_dialects_excluding</code> or <code>mark_with_dialects_including</code>.</li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-tests-on-a-specific-backend","title":"Run tests on a specific backend","text":"<p>Options for running tests on one backend only - this includes tests written specifically for that backend, as well as backend-agnostic tests supported for that backend.</p> <ul> <li><code>pytest tests/ -m duckdb</code> - run all <code>duckdb</code> tests, and all <code>core</code> tests<ul> <li>&amp; similarly for other dialects</li> </ul> </li> <li><code>pytest tests/ -m duckdb_only</code> - run all <code>duckdb</code> tests only, and not the <code>core</code> tests<ul> <li>&amp; similarly for other dialects</li> </ul> </li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-tests-across-multiple-backends","title":"Run tests across multiple backends","text":"<p>Options for running tests on multiple backends (including all backends) - this includes tests written specifically for those backends, as well as backend-agnostic tests supported for those backends.</p> <ul> <li><code>pytest tests/ -m default</code> or equivalently <code>pytest tests/</code> - run all tests in the <code>default</code> group. The <code>default</code> group consists of the <code>core</code> tests, and those dialects in the <code>default</code> group - currently <code>spark</code> and <code>duckdb</code>.<ul> <li>Other groups of dialects can be added and will similarly run with <code>pytest tests/ -m new_dialect_group</code>. Dialects within the current scope of testing and the groups they belong to are defined in the <code>dialect_groups</code> dictionary in tests/decorator.py</li> </ul> </li> <li><code>pytest tests/ -m all</code> run all tests for all available dialects</li> </ul> <p>These all work alongside all the other pytest options, so for instance to run the tests for training <code>probability_two_random_records_match</code> for only <code>duckdb</code>, ignoring warnings, with quiet output, and exiting on the first failure/error: </p><pre><code>pytest -W ignore -q -x -m duckdb tests/test_estimate_prob_two_rr_match.py\n</code></pre> Running tests with docker \ud83d\udc33 <p>If you want to test Splink against a specific version of python, the easiest method is to utilise docker \ud83d\udc33.</p> <p>Docker allows you to more quickly and easily install a specific version of python and run the existing test library against it.</p> <p>This is particularly useful if you're using py &gt; 3.9.10 (which is currently in use in our tests github action) and need to run a secondary set of tests.</p> <p>A pre-built Dockerfile for running tests against python version 3.9.10 can be located within scripts/run_tests.Dockerfile.</p> <p>To run, simply use the following docker command from within a terminal and the root folder of a splink clone: </p><pre><code>docker build -t run_tests:testing -f scripts/run_tests.Dockerfile . &amp;&amp; docker run --rm --name splink-test run_tests:testing\n</code></pre> <p>This will both build and run the tests library.</p> <p>Feel free to replace <code>run_tests:testing</code> with an image name and tag you're happy with.</p> <p>Reusing the same image and tag will overwrite your existing image.</p> <p>You can also overwrite the default <code>CMD</code> if you want a different set of <code>pytest</code> command-line options, for example </p><pre><code>docker run --rm --name splink-test run_tests:testing pytest -W ignore -m spark tests/test_u_train.py\n</code></pre>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#tests-in-ci","title":"Tests in CI","text":"<p>Splink utilises github actions to run tests for each pull request. This consists of a few independent checks:</p> <ul> <li>The full test suite is run separately against several different python versions</li> <li>The example notebooks are checked to ensure they run without error</li> <li>The tutorial notebooks are checked to ensure they run without error</li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#writing-tests","title":"Writing tests","text":"","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#core-tests","title":"Core tests","text":"<p>Core tests are treated the same way as ordinary pytest tests. Any test is marked as <code>core</code> by default, and will only be excluded from being a core test if it is decorated using either:</p> <ul> <li><code>@mark_with_dialects_excluding</code> for backend-agnostic tests, or</li> <li><code>@mark_with_dialects_including</code> for backend-specific tests</li> </ul> <p>from the test decorator file.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#backend-agnostic-testing","title":"Backend-agnostic testing","text":"<p>The majority of tests should be written using the backend-agnostic testing framework. This just provides some small tools which allow tests to be written in a backend-independent way. This means the tests can then by run against all available SQL backends (or a subset, if some lack necessary features for the test).</p> <p>As an example, let's consider a test that will run on all dialects, and then break down the various parts to see what each is doing.</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\n    settings_dict = {\n        \"link_type\": \"dedupe_only\",\n        \"blocking_rules_to_generate_predictions\": [\"l.city = r.city\", \"l.surname = r.surname\", \"l.dob = r.dob\"],\n        \"comparisons\": [\n            helper.cl.exact_match(\"city\"),\n            helper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\n            helper.cl.levenshtein_at_thresholds(\"surname\"),\n            {\n                \"output_column_name\": \"email\",\n                \"comparison_description\": \"Email\",\n                \"comparison_levels\": [\n                    helper.cll.null_level(\"email\"),\n                    helper.cll.exact_match_level(\"email\"),\n                    helper.cll.levenshtein_level(\"email\", 2),\n                    {\n                        \"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n                        \"label_for_charts\": \"email first character matches\",\n                    },\n                    helper.cll.else_level(),\n                ]\n            }\n        ]\n    }\n\n    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n\n    # and then some actual testing logic\n</code></pre> <p>Firstly you should import the decorator-factory <code>mark_with_dialects_excluding</code>, which will decorate each test function:</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n</code></pre> <p>Then we define the function, and pass parameters:</p> <pre><code>@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n</code></pre> <p>The decorator <code>@mark_with_dialects_excluding()</code> will do two things:</p> <ul> <li>marks the test it decorates with the appropriate custom <code>pytest</code> marks. This ensures that it will be run with tests for each dialect, excluding any that are passed as arguments; in this case it will be run for all dialects, as we have passed no arguments.</li> <li>parameterises the test with a string parameter <code>dialect</code>, which will be used to configure the test for that dialect. The test will run for each value of <code>dialect</code> possible, excluding any passed to the decorator (none in this case).</li> </ul> <p>You should aim to exclude as few dialects as possible - consider if you really need to exclude any. Dialects should only be excluded if the test doesn't make sense for them due to features they lack. The default choice should be the decorator with no arguments <code>@mark_with_dialects_excluding()</code>, meaning the test runs for all dialects.</p> <pre><code>@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n</code></pre> <p>As well as the parameter <code>dialect</code> (which is provided by the decorator), we must also pass the helper-factory fixture <code>test_helpers</code>. We can additionally pass further fixtures if needed - in this case <code>some_other_test_fixture</code>. We could similarly provide an explicit parameterisation to the test, in which case we would also pass these parameters - see the pytest docs on parameterisation for more information.</p> <pre><code>    helper = test_helpers[dialect]\n</code></pre> <p>The fixture <code>test_helpers</code> is simply a dictionary of the specific-dialect test helpers - here we pick the appropriate one for our test.</p> <p>Each helper has the same set of methods and properties, which encapsulate all of the dialect-dependencies. You can find the full set of properties and methods by examining the source for the base class <code>TestHelper</code>.</p> <pre><code>    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n</code></pre> <p>Here we are now actually using a method of the test helper - in this case we are loading a table from a csv to the database and returning it in a form suitable for passing to a Splink linker.</p> <p></p><pre><code>    \"comparisons\": [\n        helper.cl.exact_match(\"city\"),\n        helper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\n        helper.cl.levenshtein_at_thresholds(\"surname\"),\n        {\n            \"output_column_name\": \"email\",\n</code></pre> We reference the dialect-specific comparison library as <code>helper.cl</code>, <p></p><pre><code>    {\n        \"output_column_name\": \"email\",\n        \"comparison_description\": \"Email\",\n        \"comparison_levels\": [\n            helper.cll.null_level(\"email\"),\n            helper.cll.exact_match_level(\"email\"),\n            helper.cll.levenshtein_level(\"email\", 2),\n            {\n                \"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n                \"label_for_charts\": \"email first character matches\",\n            }\n            helper.cll.else_level(),\n        ]\n    }\n</code></pre> and the dialect-specific comparison level library as <code>helper.cll</code>. <p></p><pre><code>    {\n        \"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n        \"label_for_charts\": \"email first character matches\",\n    },\n</code></pre> We can include raw SQL statements, but we must ensure they are valid for all dialects we are considering, so we should avoid any unusual functions that are not likely to be universal. <p></p><pre><code>    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n</code></pre> Finally we instantiate the linker, passing any default set of extra arguments provided by the helper, which some dialects require. <p>From this point onwards we will be working with the instantiated <code>linker</code>, and so will not need to refer to <code>helper</code> any more - the rest of the test can be written as usual.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#excluding-some-backends","title":"Excluding some backends","text":"<p>Now let's have a small look at a similar example - only this time we are going to exclude the <code>sqlite</code> backend, as the test relies on features not directly available for that backend. In this example that will be the SQL function <code>split_part</code> which does not exist in the <code>sqlite</code> dialect.</p> <p>Warning</p> <p>Tests should be made available to the widest range of backends possible. Only exclude backends if features not shared by all backends are crucial to the test-logic - otherwise consider rewriting things so that all backends are covered.</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding(\"sqlite\")\ndef test_feature_that_doesnt_work_with_sqlite(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\n    settings_dict = {\n        \"link_type\": \"dedupe_only\",\n        \"blocking_rules_to_generate_predictions\": [\"l.city = r.city\", \"l.surname = r.surname\", \"l.dob = r.dob\"],\n        \"comparisons\": [\n            helper.cl.exact_match(\"city\"),\n            helper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\n            helper.cl.levenshtein_at_thresholds(\"surname\"),\n            {\n                \"output_column_name\": \"email\",\n                \"comparison_description\": \"Email\",\n                \"comparison_levels\": [\n                    helper.cll.null_level(\"email\"),\n                    helper.cll.exact_match_level(\"email\"),\n                    helper.cll.levenshtein_level(\"email\", 2),\n                    {\n                        \"sql_condition\": \"split_part(email_l, '@', 1) = split_part(email_r, '@', 1)\",\n                        \"label_for_charts\": \"email local-part matches\",\n                    },\n                    helper.cll.else_level(),\n                ]\n            }\n        ]\n    }\n\n    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n\n    # and then some actual testing logic\n</code></pre> <p>The key difference is the argument we pass to the decorator: </p><pre><code>@mark_with_dialects_excluding(\"sqlite\")\ndef test_feature_that_doesnt_work_with_sqlite(test_helpers, dialect, some_other_test_fixture):\n</code></pre> As above this marks the test it decorates with the appropriate custom <code>pytest</code> marks, but in this case it ensures that it will be run with tests for each dialect excluding sqlite. Again <code>dialect</code> is passed as a parameter, and the test will run in turn for each value of <code>dialect</code> except for 'sqlite'. <p></p><pre><code>    {\n        \"sql_condition\": \"split_part(email_l, '@', 1) = split_part(email_r, '@', 1)\",\n        \"label_for_charts\": \"email local-part matches\",\n    }\n</code></pre> This line is why we cannot allow <code>sqlite</code> for this test - we make use of the function <code>split_part</code> which is not available in the <code>sqlite</code> dialect, hence its exclusion. We suppose that this particular comparison level is crucial for the test to make sense, otherwise we would rewrite this line to make it run universally. When you come to run the tests, this test will not run on the <code>sqlite</code> backend. <p>If you need to exclude multiple dialects this is also possible - just pass each as an argument. For example, to decorate a test that is not supported on <code>spark</code> or <code>sqlite</code>, use the decorator <code>@mark_with_dialects_excluding(\"sqlite\", \"spark\")</code>.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#backend-specific-tests","title":"Backend-specific tests","text":"<p>If you intend to write a test for a specific backend, first consider whether it is definitely specific to that backend - if not then a backend-agnostic test would be preferable, as then your test will be run against many backends. If you really do need to test features peculiar to one backend, then you can write it simply as you would an ordinary <code>pytest</code> test. The only difference is that you should decorate it with <code>@mark_with_dialects_including</code> (from tests/decorator.py) - for example:</p>  DuckDB Spark SQLite <pre><code>@mark_with_dialects_including(\"duckdb\")\ndef test_some_specific_duckdb_feature():\n    ...\n</code></pre> <pre><code>@mark_with_dialects_including(\"spark\")\ndef test_some_specific_spark_feature():\n    ...\n</code></pre> <pre><code>@mark_with_dialects_including(\"sqlite\")\ndef test_some_specific_sqlite_feature():\n    ...\n</code></pre> <p>This ensures that the test gets marked appropriately for running when the <code>Spark</code> tests should be run, and excludes it from the set of <code>core</code> tests.</p> <p>Note that unlike the exclusive <code>mark_with_dialects_excluding</code>, this decorator will not paramaterise the test with the <code>dialect</code> argument. This is because usage of the inclusive form is largely designed for single-dialect tests. If you wish to override this behaviour and parameterise the test you can use the argument <code>pass_dialect</code>, for example <code>@mark_with_dialects_including(\"spark\", \"sqlite\", pass_dialect=True)</code>, in which case you would need to write the test in a backend-independent manner.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/charts/building_charts.html","title":"Building new charts","text":"<pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\nimport logging\nlogging.getLogger(\"splink\").setLevel(logging.WARNING)\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", ),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\", 2),\n    ],\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n\nlinker = DuckDBLinker(df, settings, set_up_basic_logging=False)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\nfor rule in [\"l.first_name = r.first_name\", \"l.email = r.email\"]:\n    linker.estimate_parameters_using_expectation_maximisation(rule)\n</code></pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre><code># Take linker object and extract complete settings dict\nrecords = linker._settings_obj._parameters_as_detailed_records\n\ncols_to_keep = [\n    \"comparison_name\",\n    \"sql_condition\",\n    \"label_for_charts\",\n    \"m_probability\",\n    \"u_probability\",\n    \"bayes_factor\",\n    \"log2_bayes_factor\",\n    \"comparison_vector_value\"\n]\n\n# Keep useful information for a match weights chart\nrecords = [{k: r[k] for k in cols_to_keep}\n           for r in records \n           if r[\"comparison_vector_value\"] != -1 and r[\"comparison_sort_order\"] != -1]\n</code></pre> records <pre><code>    [\n        {'comparison_name': 'first_name',\n        'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n        'label_for_charts': 'Exact match first_name',\n        'm_probability': 0.5018941916173814,\n        'u_probability': 0.0057935713975033705,\n        'bayes_factor': 86.62949969575988,\n        'log2_bayes_factor': 6.436786480320881,\n        'comparison_vector_value': 4},\n        {'comparison_name': 'first_name',\n        'sql_condition': 'damerau_levenshtein(\"first_name_l\", \"first_name_r\") &amp;lt;= 1',\n        'label_for_charts': 'Damerau_levenshtein &amp;lt;= 1',\n        'm_probability': 0.19595791797531015,\n        'u_probability': 0.00236614327345483,\n        'bayes_factor': 82.81743551783742,\n        'log2_bayes_factor': 6.371862624533329,\n        'comparison_vector_value': 3},\n        {'comparison_name': 'first_name',\n        'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &amp;gt;= 0.9',\n        'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.9',\n        'm_probability': 0.045985303626033085,\n        'u_probability': 0.001296159366708712,\n        'bayes_factor': 35.47812468678278,\n        'log2_bayes_factor': 5.148857848140163,\n        'comparison_vector_value': 2},\n        {'comparison_name': 'first_name',\n        'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &amp;gt;= 0.8',\n        'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.8',\n        'm_probability': 0.06396730257493154,\n        'u_probability': 0.005677583982137938,\n        'bayes_factor': 11.266641370022352,\n        'log2_bayes_factor': 3.493985601438375,\n        'comparison_vector_value': 1},\n        {'comparison_name': 'first_name',\n        'sql_condition': 'ELSE',\n        'label_for_charts': 'All other comparisons',\n        'm_probability': 0.19219528420634394,\n        'u_probability': 0.9848665419801952,\n        'bayes_factor': 0.19514855669673956,\n        'log2_bayes_factor': -2.357355302129234,\n        'comparison_vector_value': 0},\n        {'comparison_name': 'surname',\n        'sql_condition': '\"surname_l\" = \"surname_r\"',\n        'label_for_charts': 'Exact match surname',\n        'm_probability': 0.5527050424941531,\n        'u_probability': 0.004889975550122249,\n        'bayes_factor': 113.02818119005431,\n        'log2_bayes_factor': 6.820538712806792,\n        'comparison_vector_value': 4},\n        {'comparison_name': 'surname',\n        'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &amp;lt;= 1',\n        'label_for_charts': 'Damerau_levenshtein &amp;lt;= 1',\n        'm_probability': 0.22212752320956386,\n        'u_probability': 0.0027554624131641246,\n        'bayes_factor': 80.61351958508214,\n        'log2_bayes_factor': 6.332949906378981,\n        'comparison_vector_value': 3},\n        {'comparison_name': 'surname',\n        'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.9',\n        'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.9',\n        'm_probability': 0.0490149338194711,\n        'u_probability': 0.0010090425738347498,\n        'bayes_factor': 48.57568460485815,\n        'log2_bayes_factor': 5.602162423566203,\n        'comparison_vector_value': 2},\n        {'comparison_name': 'surname',\n        'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.8',\n        'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.8',\n        'm_probability': 0.05001678986356945,\n        'u_probability': 0.003710768991942586,\n        'bayes_factor': 13.478820689774516,\n        'log2_bayes_factor': 3.752622370380284,\n        'comparison_vector_value': 1},\n        {'comparison_name': 'surname',\n        'sql_condition': 'ELSE',\n        'label_for_charts': 'All other comparisons',\n        'm_probability': 0.1261357106132424,\n        'u_probability': 0.9876347504709363,\n        'bayes_factor': 0.1277149376863226,\n        'log2_bayes_factor': -2.969000820703079,\n        'comparison_vector_value': 0},\n        {'comparison_name': 'dob',\n        'sql_condition': '\"dob_l\" = \"dob_r\"',\n        'label_for_charts': 'Exact match',\n        'm_probability': 0.41383785481447766,\n        'u_probability': 0.0017477477477477479,\n        'bayes_factor': 236.78351486807742,\n        'log2_bayes_factor': 7.887424832202931,\n        'comparison_vector_value': 5},\n        {'comparison_name': 'dob',\n        'sql_condition': 'damerau_levenshtein(\"dob_l\", \"dob_r\") &amp;lt;= 1',\n        'label_for_charts': 'Damerau_levenshtein &amp;lt;= 1',\n        'm_probability': 0.10806341031654734,\n        'u_probability': 0.0016436436436436436,\n        'bayes_factor': 65.74625268345359,\n        'log2_bayes_factor': 6.038836762842662,\n        'comparison_vector_value': 4},\n        {'comparison_name': 'dob',\n        'sql_condition': '\\n            abs(date_diff(\\'month\\',\\n                strptime(\"dob_l\", \\'%Y-%m-%d\\'),\\n                strptime(\"dob_r\", \\'%Y-%m-%d\\'))\\n                ) &amp;lt;= 1\\n        ',\n        'label_for_charts': 'Within 1 month',\n        'm_probability': 0.11300938544779224,\n        'u_probability': 0.003833833833833834,\n        'bayes_factor': 29.476860590690453,\n        'log2_bayes_factor': 4.881510974428093,\n        'comparison_vector_value': 3},\n        {'comparison_name': 'dob',\n        'sql_condition': '\\n            abs(date_diff(\\'year\\',\\n                strptime(\"dob_l\", \\'%Y-%m-%d\\'),\\n                strptime(\"dob_r\", \\'%Y-%m-%d\\'))\\n                ) &amp;lt;= 1\\n        ',\n        'label_for_charts': 'Within 1 year',\n        'm_probability': 0.17200656922328977,\n        'u_probability': 0.05062662662662663,\n        'bayes_factor': 3.397551460259144,\n        'log2_bayes_factor': 1.7644954026183992,\n        'comparison_vector_value': 2},\n        {'comparison_name': 'dob',\n        'sql_condition': '\\n            abs(date_diff(\\'year\\',\\n                strptime(\"dob_l\", \\'%Y-%m-%d\\'),\\n                strptime(\"dob_r\", \\'%Y-%m-%d\\'))\\n                ) &amp;lt;= 10\\n        ',\n        'label_for_charts': 'Within 10 years',\n        'm_probability': 0.19035523041792068,\n        'u_probability': 0.3037037037037037,\n        'bayes_factor': 0.6267794172297388,\n        'log2_bayes_factor': -0.6739702908716182,\n        'comparison_vector_value': 1},\n        {'comparison_name': 'dob',\n        'sql_condition': 'ELSE',\n        'label_for_charts': 'All other comparisons',\n        'm_probability': 0.002727549779972325,\n        'u_probability': 0.6384444444444445,\n        'bayes_factor': 0.004272180302776005,\n        'log2_bayes_factor': -7.870811748958801,\n        'comparison_vector_value': 0},\n        {'comparison_name': 'city',\n        'sql_condition': '\"city_l\" = \"city_r\"',\n        'label_for_charts': 'Exact match',\n        'm_probability': 0.6013808934279701,\n        'u_probability': 0.0551475711801453,\n        'bayes_factor': 10.904938885948333,\n        'log2_bayes_factor': 3.4469097796586596,\n        'comparison_vector_value': 1},\n        {'comparison_name': 'city',\n        'sql_condition': 'ELSE',\n        'label_for_charts': 'All other comparisons',\n        'm_probability': 0.3986191065720299,\n        'u_probability': 0.9448524288198547,\n        'bayes_factor': 0.42188504195296994,\n        'log2_bayes_factor': -1.2450781575619725,\n        'comparison_vector_value': 0},\n        {'comparison_name': 'email',\n        'sql_condition': '\"email_l\" = \"email_r\"',\n        'label_for_charts': 'Exact match',\n        'm_probability': 0.5914840252879943,\n        'u_probability': 0.0021938713143283602,\n        'bayes_factor': 269.6074384240141,\n        'log2_bayes_factor': 8.07471649055784,\n        'comparison_vector_value': 2},\n        {'comparison_name': 'email',\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &amp;lt;= 2',\n        'label_for_charts': 'Levenshtein &amp;lt;= 2',\n        'm_probability': 0.3019669634613132,\n        'u_probability': 0.0013542812658830492,\n        'bayes_factor': 222.9721189153553,\n        'log2_bayes_factor': 7.800719512398763,\n        'comparison_vector_value': 1},\n        {'comparison_name': 'email',\n        'sql_condition': 'ELSE',\n        'label_for_charts': 'All other comparisons',\n        'm_probability': 0.10654901125069259,\n        'u_probability': 0.9964518474197885,\n        'bayes_factor': 0.10692840956298139,\n        'log2_bayes_factor': -3.225282884575804,\n        'comparison_vector_value': 0}\n    ]\n</code></pre> <pre><code>import pandas as pd\nimport altair as alt\n\ndf = pd.DataFrame(records)\n\n# Need a unique name for each comparison level - easier to create in pandas than altair\ndf[\"cl_id\"] = df[\"comparison_name\"] + \"_\" + \\\n    df[\"comparison_vector_value\"].astype(\"str\")\n\n# Simple start - bar chart with x, y and color encodings\nalt.Chart(df).mark_bar().encode(\n    y=\"cl_id\",\n    x=\"log2_bayes_factor\",\n    color=\"comparison_name\"\n)\n</code></pre> <pre><code>alt.Chart(df).mark_bar().encode(\n    y=alt.Y(\"cl_id\", \n        sort=\"-x\", \n        title=\"Comparison level\"\n    ),\n    x=alt.X(\"log2_bayes_factor\", \n        title=\"Comparison level match weight = log2(m/u)\", \n        scale=alt.Scale(domain=[-10,10])\n    ),\n    color=\"comparison_name\"\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n</code></pre> <pre><code>alt.Chart(df).mark_bar().encode(\n    y=alt.Y(\"cl_id\",\n            sort=\"-x\",\n            title=\"Comparison level\"\n            ),\n    x=alt.X(\"log2_bayes_factor\",\n            title=\"Comparison level match weight = log2(m/u)\",\n            scale=alt.Scale(domain=[-10, 10])\n            ),\n    color=\"comparison_name\",\n    tooltip=[\n        \"comparison_name\", \n        \"label_for_charts\", \n        \"sql_condition\",\n        \"m_probability\",\n        \"u_probability\",\n        \"bayes_factor\",\n        \"log2_bayes_factor\"\n        ]\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n</code></pre> <pre><code># Create base chart with shared data and encodings (mark type not specified)\nbase = alt.Chart(df).encode(\n    y=alt.Y(\"cl_id\",\n            sort=\"-x\",\n            title=\"Comparison level\"\n            ),\n    x=alt.X(\"log2_bayes_factor\",\n            title=\"Comparison level match weight = log2(m/u)\",\n            scale=alt.Scale(domain=[-10, 10])\n            ),\n    tooltip=[\n        \"comparison_name\",\n        \"label_for_charts\",\n        \"sql_condition\",\n        \"m_probability\",\n        \"u_probability\",\n        \"bayes_factor\",\n        \"log2_bayes_factor\"\n    ]\n)\n\n# Build bar chart from base (color legend made redundant by text labels)\nbar = base.mark_bar().encode(\n    color=alt.Color(\"comparison_name\", legend=None)\n)\n\n# Build text layer from base\ntext = base.mark_text(dx=0, align=\"right\").encode(\n    text=\"comparison_name\"\n)\n\n# Final layered chart\nchart = bar + text\n\n# Add global config\nchart.resolve_axis(\n    y=\"shared\", \n    x=\"shared\"\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n</code></pre> <p>Sometimes things go wrong in Altair and it's not clear why or how to fix it. If the docs and Stack Overflow don't have a solution, the answer is usually that Altair is making decisions under the hood about the Vega-Lite schema that are out of your control.</p> <p>In this example, the sorting of the y-axis is broken when layering charts. If we show <code>bar</code> and <code>text</code> side-by-side, you can see they work as expected, but the sorting is broken in the layering process.</p> <pre><code>bar | text\n</code></pre> <p>Once we get to this stage (or whenever you're comfortable), we can switch to Vega-Lite by exporting the JSON from our <code>chart</code> object, or opening the chart in the Vega-Lite editor.</p> <pre><code>chart.to_json()\n</code></pre> Chart JSON <pre><code>  {\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\",\n  \"config\": {\n    \"view\": {\n      \"continuousHeight\": 300,\n      \"continuousWidth\": 300\n    }\n  },\n  \"data\": {\n    \"name\": \"data-3901c03d78701611834aa82ab7374cce\"\n  },\n  \"datasets\": {\n    \"data-3901c03d78701611834aa82ab7374cce\": [\n      {\n        \"bayes_factor\": 86.62949969575988,\n        \"cl_id\": \"first_name_4\",\n        \"comparison_name\": \"first_name\",\n        \"comparison_vector_value\": 4,\n        \"label_for_charts\": \"Exact match first_name\",\n        \"log2_bayes_factor\": 6.436786480320881,\n        \"m_probability\": 0.5018941916173814,\n        \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n        \"u_probability\": 0.0057935713975033705\n      },\n      {\n        \"bayes_factor\": 82.81743551783742,\n        \"cl_id\": \"first_name_3\",\n        \"comparison_name\": \"first_name\",\n        \"comparison_vector_value\": 3,\n        \"label_for_charts\": \"Damerau_levenshtein &amp;lt;= 1\",\n        \"log2_bayes_factor\": 6.371862624533329,\n        \"m_probability\": 0.19595791797531015,\n        \"sql_condition\": \"damerau_levenshtein(\\\"first_name_l\\\", \\\"first_name_r\\\") &amp;lt;= 1\",\n        \"u_probability\": 0.00236614327345483\n      },\n      {\n        \"bayes_factor\": 35.47812468678278,\n        \"cl_id\": \"first_name_2\",\n        \"comparison_name\": \"first_name\",\n        \"comparison_vector_value\": 2,\n        \"label_for_charts\": \"Jaro_winkler_similarity &amp;gt;= 0.9\",\n        \"log2_bayes_factor\": 5.148857848140163,\n        \"m_probability\": 0.045985303626033085,\n        \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &amp;gt;= 0.9\",\n        \"u_probability\": 0.001296159366708712\n      },\n      {\n        \"bayes_factor\": 11.266641370022352,\n        \"cl_id\": \"first_name_1\",\n        \"comparison_name\": \"first_name\",\n        \"comparison_vector_value\": 1,\n        \"label_for_charts\": \"Jaro_winkler_similarity &amp;gt;= 0.8\",\n        \"log2_bayes_factor\": 3.493985601438375,\n        \"m_probability\": 0.06396730257493154,\n        \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &amp;gt;= 0.8\",\n        \"u_probability\": 0.005677583982137938\n      },\n      {\n        \"bayes_factor\": 0.19514855669673956,\n        \"cl_id\": \"first_name_0\",\n        \"comparison_name\": \"first_name\",\n        \"comparison_vector_value\": 0,\n        \"label_for_charts\": \"All other comparisons\",\n        \"log2_bayes_factor\": -2.357355302129234,\n        \"m_probability\": 0.19219528420634394,\n        \"sql_condition\": \"ELSE\",\n        \"u_probability\": 0.9848665419801952\n      },\n      {\n        \"bayes_factor\": 113.02818119005431,\n        \"cl_id\": \"surname_4\",\n        \"comparison_name\": \"surname\",\n        \"comparison_vector_value\": 4,\n        \"label_for_charts\": \"Exact match surname\",\n        \"log2_bayes_factor\": 6.820538712806792,\n        \"m_probability\": 0.5527050424941531,\n        \"sql_condition\": \"\\\"surname_l\\\" = \\\"surname_r\\\"\",\n        \"u_probability\": 0.004889975550122249\n      },\n      {\n        \"bayes_factor\": 80.61351958508214,\n        \"cl_id\": \"surname_3\",\n        \"comparison_name\": \"surname\",\n        \"comparison_vector_value\": 3,\n        \"label_for_charts\": \"Damerau_levenshtein &amp;lt;= 1\",\n        \"log2_bayes_factor\": 6.332949906378981,\n        \"m_probability\": 0.22212752320956386,\n        \"sql_condition\": \"damerau_levenshtein(\\\"surname_l\\\", \\\"surname_r\\\") &amp;lt;= 1\",\n        \"u_probability\": 0.0027554624131641246\n      },\n      {\n        \"bayes_factor\": 48.57568460485815,\n        \"cl_id\": \"surname_2\",\n        \"comparison_name\": \"surname\",\n        \"comparison_vector_value\": 2,\n        \"label_for_charts\": \"Jaro_winkler_similarity &amp;gt;= 0.9\",\n        \"log2_bayes_factor\": 5.602162423566203,\n        \"m_probability\": 0.0490149338194711,\n        \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &amp;gt;= 0.9\",\n        \"u_probability\": 0.0010090425738347498\n      },\n      {\n        \"bayes_factor\": 13.478820689774516,\n        \"cl_id\": \"surname_1\",\n        \"comparison_name\": \"surname\",\n        \"comparison_vector_value\": 1,\n        \"label_for_charts\": \"Jaro_winkler_similarity &amp;gt;= 0.8\",\n        \"log2_bayes_factor\": 3.752622370380284,\n        \"m_probability\": 0.05001678986356945,\n        \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &amp;gt;= 0.8\",\n        \"u_probability\": 0.003710768991942586\n      },\n      {\n        \"bayes_factor\": 0.1277149376863226,\n        \"cl_id\": \"surname_0\",\n        \"comparison_name\": \"surname\",\n        \"comparison_vector_value\": 0,\n        \"label_for_charts\": \"All other comparisons\",\n        \"log2_bayes_factor\": -2.969000820703079,\n        \"m_probability\": 0.1261357106132424,\n        \"sql_condition\": \"ELSE\",\n        \"u_probability\": 0.9876347504709363\n      },\n      {\n        \"bayes_factor\": 236.78351486807742,\n        \"cl_id\": \"dob_5\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 5,\n        \"label_for_charts\": \"Exact match\",\n        \"log2_bayes_factor\": 7.887424832202931,\n        \"m_probability\": 0.41383785481447766,\n        \"sql_condition\": \"\\\"dob_l\\\" = \\\"dob_r\\\"\",\n        \"u_probability\": 0.0017477477477477479\n      },\n      {\n        \"bayes_factor\": 65.74625268345359,\n        \"cl_id\": \"dob_4\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 4,\n        \"label_for_charts\": \"Damerau_levenshtein &amp;lt;= 1\",\n        \"log2_bayes_factor\": 6.038836762842662,\n        \"m_probability\": 0.10806341031654734,\n        \"sql_condition\": \"damerau_levenshtein(\\\"dob_l\\\", \\\"dob_r\\\") &amp;lt;= 1\",\n        \"u_probability\": 0.0016436436436436436\n      },\n      {\n        \"bayes_factor\": 29.476860590690453,\n        \"cl_id\": \"dob_3\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 3,\n        \"label_for_charts\": \"Within 1 month\",\n        \"log2_bayes_factor\": 4.881510974428093,\n        \"m_probability\": 0.11300938544779224,\n        \"sql_condition\": \"\\n            abs(date_diff('month',\\n                strptime(\\\"dob_l\\\", '%Y-%m-%d'),\\n                strptime(\\\"dob_r\\\", '%Y-%m-%d'))\\n                ) &amp;lt;= 1\\n        \",\n        \"u_probability\": 0.003833833833833834\n      },\n      {\n        \"bayes_factor\": 3.397551460259144,\n        \"cl_id\": \"dob_2\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 2,\n        \"label_for_charts\": \"Within 1 year\",\n        \"log2_bayes_factor\": 1.7644954026183992,\n        \"m_probability\": 0.17200656922328977,\n        \"sql_condition\": \"\\n            abs(date_diff('year',\\n                strptime(\\\"dob_l\\\", '%Y-%m-%d'),\\n                strptime(\\\"dob_r\\\", '%Y-%m-%d'))\\n                ) &amp;lt;= 1\\n        \",\n        \"u_probability\": 0.05062662662662663\n      },\n      {\n        \"bayes_factor\": 0.6267794172297388,\n        \"cl_id\": \"dob_1\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 1,\n        \"label_for_charts\": \"Within 10 years\",\n        \"log2_bayes_factor\": -0.6739702908716182,\n        \"m_probability\": 0.19035523041792068,\n        \"sql_condition\": \"\\n            abs(date_diff('year',\\n                strptime(\\\"dob_l\\\", '%Y-%m-%d'),\\n                strptime(\\\"dob_r\\\", '%Y-%m-%d'))\\n                ) &amp;lt;= 10\\n        \",\n        \"u_probability\": 0.3037037037037037\n      },\n      {\n        \"bayes_factor\": 0.004272180302776005,\n        \"cl_id\": \"dob_0\",\n        \"comparison_name\": \"dob\",\n        \"comparison_vector_value\": 0,\n        \"label_for_charts\": \"All other comparisons\",\n        \"log2_bayes_factor\": -7.870811748958801,\n        \"m_probability\": 0.002727549779972325,\n        \"sql_condition\": \"ELSE\",\n        \"u_probability\": 0.6384444444444445\n      },\n      {\n        \"bayes_factor\": 10.904938885948333,\n        \"cl_id\": \"city_1\",\n        \"comparison_name\": \"city\",\n        \"comparison_vector_value\": 1,\n        \"label_for_charts\": \"Exact match\",\n        \"log2_bayes_factor\": 3.4469097796586596,\n        \"m_probability\": 0.6013808934279701,\n        \"sql_condition\": \"\\\"city_l\\\" = \\\"city_r\\\"\",\n        \"u_probability\": 0.0551475711801453\n      },\n      {\n        \"bayes_factor\": 0.42188504195296994,\n        \"cl_id\": \"city_0\",\n        \"comparison_name\": \"city\",\n        \"comparison_vector_value\": 0,\n        \"label_for_charts\": \"All other comparisons\",\n        \"log2_bayes_factor\": -1.2450781575619725,\n        \"m_probability\": 0.3986191065720299,\n        \"sql_condition\": \"ELSE\",\n        \"u_probability\": 0.9448524288198547\n      },\n      {\n        \"bayes_factor\": 269.6074384240141,\n        \"cl_id\": \"email_2\",\n        \"comparison_name\": \"email\",\n        \"comparison_vector_value\": 2,\n        \"label_for_charts\": \"Exact match\",\n        \"log2_bayes_factor\": 8.07471649055784,\n        \"m_probability\": 0.5914840252879943,\n        \"sql_condition\": \"\\\"email_l\\\" = \\\"email_r\\\"\",\n        \"u_probability\": 0.0021938713143283602\n      },\n      {\n        \"bayes_factor\": 222.9721189153553,\n        \"cl_id\": \"email_1\",\n        \"comparison_name\": \"email\",\n        \"comparison_vector_value\": 1,\n        \"label_for_charts\": \"Levenshtein &amp;lt;= 2\",\n        \"log2_bayes_factor\": 7.800719512398763,\n        \"m_probability\": 0.3019669634613132,\n        \"sql_condition\": \"levenshtein(\\\"email_l\\\", \\\"email_r\\\") &amp;lt;= 2\",\n        \"u_probability\": 0.0013542812658830492\n      },\n      {\n        \"bayes_factor\": 0.10692840956298139,\n        \"cl_id\": \"email_0\",\n        \"comparison_name\": \"email\",\n        \"comparison_vector_value\": 0,\n        \"label_for_charts\": \"All other comparisons\",\n        \"log2_bayes_factor\": -3.225282884575804,\n        \"m_probability\": 0.10654901125069259,\n        \"sql_condition\": \"ELSE\",\n        \"u_probability\": 0.9964518474197885\n      }\n    ]\n  },\n  \"layer\": [\n    {\n      \"encoding\": {\n        \"color\": {\n          \"field\": \"comparison_name\",\n          \"legend\": null,\n          \"type\": \"nominal\"\n        },\n        \"tooltip\": [\n          {\n            \"field\": \"comparison_name\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"label_for_charts\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"sql_condition\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"m_probability\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"u_probability\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"bayes_factor\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"log2_bayes_factor\",\n            \"type\": \"quantitative\"\n          }\n        ],\n        \"x\": {\n          \"field\": \"log2_bayes_factor\",\n          \"scale\": {\n            \"domain\": [\n              -10,\n              10\n            ]\n          },\n          \"title\": \"Comparison level match weight = log2(m/u)\",\n          \"type\": \"quantitative\"\n        },\n        \"y\": {\n          \"field\": \"cl_id\",\n          \"sort\": \"-x\",\n          \"title\": \"Comparison level\",\n          \"type\": \"nominal\"\n        }\n      },\n      \"mark\": {\n        \"type\": \"bar\"\n      }\n    },\n    {\n      \"encoding\": {\n        \"text\": {\n          \"field\": \"comparison_name\",\n          \"type\": \"nominal\"\n        },\n        \"tooltip\": [\n          {\n            \"field\": \"comparison_name\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"label_for_charts\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"sql_condition\",\n            \"type\": \"nominal\"\n          },\n          {\n            \"field\": \"m_probability\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"u_probability\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"bayes_factor\",\n            \"type\": \"quantitative\"\n          },\n          {\n            \"field\": \"log2_bayes_factor\",\n            \"type\": \"quantitative\"\n          }\n        ],\n        \"x\": {\n          \"field\": \"log2_bayes_factor\",\n          \"scale\": {\n            \"domain\": [\n              -10,\n              10\n            ]\n          },\n          \"title\": \"Comparison level match weight = log2(m/u)\",\n          \"type\": \"quantitative\"\n        },\n        \"y\": {\n          \"field\": \"cl_id\",\n          \"sort\": \"-x\",\n          \"title\": \"Comparison level\",\n          \"type\": \"nominal\"\n        }\n      },\n      \"mark\": {\n        \"align\": \"right\",\n        \"dx\": 0,\n        \"type\": \"text\"\n      }\n    }\n  ]\n  }\n</code></pre>"},{"location":"dev_guides/charts/building_charts.html#building-a-new-chart-in-splink","title":"Building a new chart in Splink","text":"<p>As mentioned in the Understanding Splink Charts topic guide, splink charts are made up of three distinct parts:</p> <ol> <li>A function to create the dataset for the chart </li> <li>A template chart definition (in a json file)</li> <li>A function to read the chart definition, add the data to it, and return the chart itself </li> </ol>"},{"location":"dev_guides/charts/building_charts.html#worked-example","title":"Worked Example","text":"<p>Below is a worked example of how to create a new chart that shows all comparisons levels ordered by match weight:</p>"},{"location":"dev_guides/charts/building_charts.html#generate-data-for-chart","title":"Generate data for chart","text":""},{"location":"dev_guides/charts/building_charts.html#create-a-chart-template","title":"Create a chart template","text":""},{"location":"dev_guides/charts/building_charts.html#build-prototype-chart-in-altair","title":"Build prototype chart in Altair","text":""},{"location":"dev_guides/charts/building_charts.html#sort-bars-edit-axestitles","title":"Sort bars, edit axes/titles","text":""},{"location":"dev_guides/charts/building_charts.html#add-tooltip","title":"Add tooltip","text":""},{"location":"dev_guides/charts/building_charts.html#add-text-layer","title":"Add text layer","text":""},{"location":"dev_guides/charts/building_charts.html#edit-in-vega-lite","title":"Edit in Vega-Lite","text":"<p>Opening the JSON from the above chart in Vega-Lite editor, it is now behaving as intended, with both bar and text layers sorted by match weight.</p> <p>If the chart is working as intended, there is only one step required before saving the JSON file - removing data from the template schema.</p> <p>The data appears as follows with a dictionary of all included <code>datasets</code> by name, and then each chart referencing the <code>data</code> it uses by name:</p> <pre><code>\"data\": {\"name\": \"data-a6c84a9cf1a0c7a2cd30cc1a0e2c1185\"},\n\"datasets\": {\n  \"data-a6c84a9cf1a0c7a2cd30cc1a0e2c1185\": [\n\n    ...\n\n  ]\n},\n</code></pre> <p>Where only one dataset is required, this is equivalent to: </p><pre><code>\"data\": {\"values\": [...]}\n</code></pre> <p>After removing the data references, the template can be saved in Splink as <code>splink/files/chart_defs/my_new_chart.json</code></p>"},{"location":"dev_guides/charts/building_charts.html#combine-the-chart-dataset-and-template","title":"Combine the chart dataset and template","text":"<p>Putting all of the above together, Splink needs definitions for the methods that generate the chart and the data behind it (these can be separate or performed by the same function if relatively simple).</p>"},{"location":"dev_guides/charts/building_charts.html#chart-definition","title":"Chart definition","text":"<p>In <code>splink/charts.py</code> we can add a new function to populate the chart definition with the provided data:</p> <pre><code>def my_new_chart(records, as_dict=False):\n    chart_path = \"my_new_chart.json\"\n    chart = load_chart_definition(chart_path)\n\n    chart[\"data\"][\"values\"] = records\n    return altair_or_json(chart, as_dict=as_dict)\n</code></pre> <p>&gt;Note - only the data is being added to a fixed chart definition here. Other elements of the chart spec can be changed by editing the <code>chart</code> dictionary in the same way.  &gt; &gt; For example, if you wanted to add a <code>color_scheme</code> argument to replace the default scheme (\"tableau10\"), this function could include the line: <code>chart[\"layer\"][0][\"encoding\"][\"color\"][\"scale\"][\"scheme\"] = color_scheme</code></p>"},{"location":"dev_guides/charts/building_charts.html#chart-method","title":"Chart method","text":"<p>Then we can add a method to the linker in <code>splink/linker.py</code> so the chart can be generated by <code>linker.my_new_chart()</code>:</p> <pre><code>from .charts import my_new_chart\n\n...\n\nclass Linker:\n\n    ...\n\n    def my_new_chart(self):\n\n        # Take linker object and extract complete settings dict\n        records = self._settings_obj._parameters_as_detailed_records\n\n        cols_to_keep = [\n            \"comparison_name\",\n            \"sql_condition\",\n            \"label_for_charts\",\n            \"m_probability\",\n            \"u_probability\",\n            \"bayes_factor\",\n            \"log2_bayes_factor\",\n            \"comparison_vector_value\"\n        ]\n\n        # Keep useful information for a match weights chart\n        records = [{k: r[k] for k in cols_to_keep}\n                   for r in records \n                   if r[\"comparison_vector_value\"] != -1 and r[\"comparison_sort_order\"] != -1]\n\n        return my_new_chart(records)\n</code></pre>"},{"location":"dev_guides/charts/building_charts.html#previous-new-chart-prs","title":"Previous new chart PRs","text":"<p>Real-life Splink chart additions, for reference:</p> <ul> <li>Term frequency adjustment chart</li> <li>Completeness (multi-dataset) chart</li> <li>Cumulative blocking rule chart</li> <li>Unlinkables chart</li> <li>Missingness chart</li> <li>Waterfall chart</li> </ul>"},{"location":"dev_guides/charts/understanding_and_editing_charts.html","title":"Understanding and editing charts","text":""},{"location":"dev_guides/charts/understanding_and_editing_charts.html#charts-in-splink","title":"Charts in Splink","text":"<p>Interactive charts are a key tool when linking data with Splink. To see all of the charts available, check out the Splink Charts Gallery.</p>"},{"location":"dev_guides/charts/understanding_and_editing_charts.html#how-do-charts-work-in-splink","title":"How do charts work in Splink?","text":"<p>Charts in Splink are built with Altair. </p> <p>For a given chart, there is usually:</p> <ul> <li>A template chart definition (e.g. <code>match_weights_waterfall.json</code>)</li> <li>A function to create the dataset for the chart (e.g. <code>records_to_waterfall_data</code>)</li> <li>A function to read the chart definition, add the data to it, and return the chart itself (e.g. <code>waterfall_chart</code>)</li> </ul> The Vega-Lite Editor <p>By far the best feature of Vega-Lite is the online editor where the JSON schema and the chart are shown side-by-side, showing changes in real time as the editor helps you to navigate the API.</p> <p></p>"},{"location":"dev_guides/charts/understanding_and_editing_charts.html#editing-existing-charts","title":"Editing existing charts","text":"<p>If you take any Altair chart in HTML format, you should be able to make changes pretty easily with the Vega-Lite Editor.</p> <p>For example, consider the <code>comparator_score_chart</code> from the <code>comparison_helpers library</code>:</p> Before After <p>Desired changes</p> <ul> <li>Titles (shared title)</li> <li>Axis titles</li> <li>Shared y-axis</li> <li>Color scales!! \ud83e\udd2e (see the Vega color schemes docs)</li> <li>red-green is an accessibility no-no</li> <li>shared colour scheme for different metrics</li> <li>unpleasant and unclear to look at</li> <li>legends not necessary (especially when using text labels)</li> <li>Text size encoding (larger text for similar strings)</li> <li>Remove \"_similarity\" and \"_distance\" from column labels</li> <li>Fixed column width (rather than chart width)</li> <li>Row highlighting (on click/hover)</li> </ul> <p>The old spec can be pasted into the Vega Lite editor and edited as shown in the video below:</p> <p></p> <p>Check out the final, improved version chart specification.</p>  Before-After diff <pre><code>@@ -1,9 +1,8 @@\n {\n-  \"config\": {\n-    \"view\": {\n-      \"continuousWidth\": 400,\n-      \"continuousHeight\": 300\n-    }\n+  \"title\": {\n+    \"text\": \"Heatmaps of string comparison metrics\",\n+    \"anchor\": \"middle\",\n+    \"fontSize\": 16\n   },\n   \"hconcat\": [\n     {\n@@ -18,25 +17,32 @@\n                   0,\n                   1\n                 ],\n-                \"range\": [\n-                  \"red\",\n-                  \"green\"\n-                ]\n+                \"scheme\": \"greenblue\"\n               },\n-              \"type\": \"quantitative\"\n+              \"type\": \"quantitative\",\n+              \"legend\": null\n             },\n             \"x\": {\n               \"field\": \"comparator\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"title\": null\n             },\n             \"y\": {\n               \"field\": \"strings_to_compare\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"title\": \"String comparison\",\n+              \"axis\": {\n+                \"titleFontSize\": 14\n+              }\n             }\n           },\n-          \"height\": 300,\n-          \"title\": \"Heatmap of Similarity Scores\",\n-          \"width\": 300\n+          \"title\": \"Similarity\",\n+          \"width\": {\n+            \"step\": 40\n+          },\n+          \"height\": {\n+            \"step\": 30\n+          }\n         },\n         {\n           \"mark\": {\n@@ -44,6 +50,16 @@\n             \"baseline\": \"middle\"\n           },\n           \"encoding\": {\n+            \"size\": {\n+              \"field\": \"score\",\n+              \"scale\": {\n+                \"range\": [\n+                  8,\n+                  14\n+                ]\n+              },\n+              \"legend\": null\n+            },\n             \"text\": {\n               \"field\": \"score\",\n               \"format\": \".2f\",\n@@ -51,7 +67,10 @@\n             },\n             \"x\": {\n               \"field\": \"comparator\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"axis\": {\n+                \"labelFontSize\": 12\n+              }\n             },\n             \"y\": {\n               \"field\": \"strings_to_compare\",\n@@ -72,29 +91,33 @@\n             \"color\": {\n               \"field\": \"score\",\n               \"scale\": {\n-                \"domain\": [\n-                  0,\n-                  5\n-                ],\n-                \"range\": [\n-                  \"green\",\n-                  \"red\"\n-                ]\n+                \"scheme\": \"yelloworangered\",\n+                \"reverse\": true\n               },\n-              \"type\": \"quantitative\"\n+              \"type\": \"quantitative\",\n+              \"legend\": null\n             },\n             \"x\": {\n               \"field\": \"comparator\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"title\": null,\n+              \"axis\": {\n+                \"labelFontSize\": 12\n+              }\n             },\n             \"y\": {\n               \"field\": \"strings_to_compare\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"axis\": null\n             }\n           },\n-          \"height\": 300,\n-          \"title\": \"Heatmap of Distance Scores\",\n-          \"width\": 200\n+          \"title\": \"Distance\",\n+          \"width\": {\n+            \"step\": 40\n+          },\n+          \"height\": {\n+            \"step\": 30\n+          }\n         },\n         {\n           \"mark\": {\n@@ -102,6 +125,17 @@\n             \"baseline\": \"middle\"\n           },\n           \"encoding\": {\n+            \"size\": {\n+              \"field\": \"score\",\n+              \"scale\": {\n+                \"range\": [\n+                  8,\n+                  14\n+                ],\n+                \"reverse\": true\n+              },\n+              \"legend\": null\n+            },\n             \"text\": {\n               \"field\": \"score\",\n               \"type\": \"quantitative\"\n@@ -124,7 +158,9 @@\n   ],\n   \"resolve\": {\n     \"scale\": {\n-      \"color\": \"independent\"\n+      \"color\": \"independent\",\n+      \"y\": \"shared\",\n+      \"size\": \"independent\"\n     }\n   },\n   \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\",\n</code></pre>"},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html","title":"Extending existing comparisons and comparison levels","text":"","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#extending-existing-comparisons-and-comparison-levels","title":"Extending existing comparisons and comparison levels","text":"<p>Creating a linkage (or deduplication) model necessitates making various comparisons between (or within) your data sets. There is some choice available in what kind of comparisons you will wish to do for the linkage problem you are dealing with. Splink comes with several comparisons ready to use directly, as well as several comparison levels that you can use to construct your own comparison. You may find that within these you find yourself using a specialised version repeatedly, and would like to make a shorthand for this and contribute it to the Splink library for other users to benefit from - this page will aid you in this process.</p> <p>This guide supplements the guide for adding entirely new comparisons and comparison levels to show how things work when you are extending existing entries.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparison-levels","title":"Subclassing existing library comparison levels","text":"<p>For this example, let's consider a comparison level that returns a match on two strings within a fixed Hamming distance. This is a specific example of the generic string distance function comparison level.</p> <p>In this case, working in <code>splink/comparison_level_library.py</code>, we simply subclass the appropriate level, and call its constructor, fixing whatever properties we need (using dialect-specific properties where appropriate - in this case the name of the function which calculates Hamming distance, which will be stored in the property <code>self._hamming_name</code>):</p> <pre><code>class HammingLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: int,\n        m_probability=None,\n    ):\n        \"\"\"Represents a comparison using a Hamming distance function,\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                Hamming similarity\n        \"\"\"\n        super().__init__(\n            col_name,\n            self._hamming_name,\n            distance_threshold,\n            higher_is_more_similar=False,\n            m_probability=m_probability,\n        )\n</code></pre> <p>The rest of the process is identical to that described for creating brand-new comparison levels.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparisons","title":"Subclassing existing library comparisons","text":"<p>As in our <code>hamming_level</code> example above, you can similarly subclass existing library <code>Comparisons</code> to create e.g. <code>hamming_at_thresholds</code> from the more generic <code>distance_function_at_thresholds</code>, similarly to how we create new comparisons. The main difficulty here is that the subclassed comparison levels will have different function arguments, so we need to check within the constructor if we are in the generic version (<code>distance_function_at_thresholds</code>) or a specific version (<code>hamming_level</code>). See the source code for <code>DistanceFunctionAtThreholdsComparisonBase</code> for an example.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html","title":"Creating new comparisons and comparison levels","text":"","tags":["API","comparisons"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparisons-and-comparison-levels-for-libraries","title":"Creating new comparisons and comparison levels for libraries","text":"<p>The Fellegi-Sunter model that Splink implements depends on having several comparisons, which are each composed of two or more comparison levels. Splink provides several ready-made comparisons and comparison levels to use out-of-the-box, but you may find in your particular application that you have to create your own custom versions if there is not a suitable comparison/level for the SQL dialect you are working with (or for any available dialects).</p> <p>Having created a custom comparison you may decide that your use case is common enough that you want to contribute it to Splink for other users to benefit from. This guide will take you through the process of doing so. Looking at existing examples should also prove to be useful for further guidance, and to perhaps serve as a starting template.</p> <p>After creating your new levels/comparisons, be sure to add some tests to help ensure that the code runs without error and behaves as expected.</p> <p>This guide is for adding new comparisons and comparison levels from scratch. If you instead want to create specialised versions of existing levels, be sure to also have a look at the guide for extending existing library entries.</p>","tags":["API","comparisons"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparison-levels","title":"Creating new comparison levels","text":"<p>For this example, let's consider a comparison level that compares if the length of two arrays are within <code>n</code> of one another (without reference to the contents of these arrays) for some non-negative integer <code>n</code>. An example of this might be if we were linking people with partial address information in two tables --- in one table we have an array of postcodes, and the other table we have an array of road-names. We don't expect them to match, but we are probably interested if the count of the number of objects within each array are similar - each corresponding to the number of addresses per person.</p> <p>To create a new comparison level, you must create a new subclass of <code>ComparisonLevel</code> which will serve as the base comparison level for any SQL dialects that will allow this level, in the file <code>splink/comparison_level_library.py</code>. It will contain the full logic for creating the comparison level - any dialect dependencies will be implemented as properties on the specific dialect-dependent object. In this case we will need to refer to a property <code>_array_length_function_name</code>, as this can vary by dialect. We will not define the property directly on this object, as our dialect-dependent versions will inherit this property from elsewhere. We also include any customisable parameters for our level - in this case we will allow options for the maximum number of elements the array may differ by.</p> <pre><code>class ArrayLengthLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name: str,\n        length_difference: int,\n        m_probability=None,\n    ):\n        \"\"\"Compares two arrays whose sizes are within a fixed distance\n        | length(arr_1) - length(arr_2) | &lt;= (length_difference)\n\n        Arguments:\n            col_name (str): Input column name\n            length_difference (int): Maximum difference in array lengths\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the size difference\n                between two arrays\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n        col_l, col_r = col.names_l_r()\n\n        sql_exp = (\n            f\"abs(\"\n            f\"{self._array_length_function_name}({col_l}) - \"\n            f\"{self._array_length_function_name}({col_r})\"\n            f\") &lt;= {length_difference}\"\n        )\n        level_dict = {\n            \"sql_condition\": sql_exp,\n            \"label_for_charts\": f\"Array sizes differ by at most {length_difference)\",\n        }\n\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>If you are using a new dialect-dependent property (as we are in this case), then it should be added as a property on <code>DialectBase</code> in <code>splink.dialect_base.py</code>, with either a sensible default value, or raising a <code>NotImplementedError</code>:</p> <pre><code>class DialectBase():\n    ...\n    @property\n    def _array_length_function_name():\n        raise NotImplementedError(\n            \"`array_length_function_name` not implemented on base class\"\n        )\n</code></pre> <p>Then any dialects that use a different value can override this (e.g in <code>splink.spark.spark_helpers.spark_base</code>):</p> <pre><code>class SparkBase(DialectBase):\n    ...\n    @property\n    def _array_length_function_name():\n        return \"array_size\"\n</code></pre> <p>Then any dialects where this comparison level can be used can simply inherit from this dialect-specific base, along with the comparison level base <code>ArrayLengthLevelBase</code> - here in <code>splink.spark.spark_helpers.spark_comparison_imports</code>:</p> <pre><code>class array_length_level(SparkBase, ArrayLengthLevelBase):\n    pass\n</code></pre> <p>Similarly for DuckDB define the appropriate function name in the base <code>splink.duckdb.duckdb_helpers.duckdb_base</code></p> <pre><code>class DuckDBBase(DialectBase):\n    ...\n    @property\n    def _array_length_function_name():\n        return \"array_length\"\n</code></pre> <p>and then simply create the level in the corresponding library <code>splink.duckdb.duckdb_helpers.duckdb_comparison_imports</code>:</p> <pre><code>class array_length_level(DuckDBBase, ArrayLengthLevelBase):\n    pass\n</code></pre> <p>The names of these should be the same for all dialects (and written in snake-case), with them being distinguished solely by path.</p>","tags":["API","comparisons"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparisons","title":"Creating new comparisons","text":"<p>The process for creating new library <code>Comparison</code>s is similar to the <code>ComparisonLevel</code> case, but slightly more involved. This is due to the fact that dialect-specific <code>Comparison</code>s need to 'know' about the dialect-specific <code>ComparisonLevel</code>s that they employ.</p> <p>As an example, we will consider a new <code>Comparison</code> that makes use of our new <code>array_length_level</code> above. Specifically, it will have the following levels:</p> <ul> <li>an optional <code>exact_match_level</code></li> <li>one or more <code>array_length_level</code>s with different values of <code>length_difference</code>, as specified</li> <li>an <code>else_level</code></li> </ul> <pre><code>class ArrayLengthAtThresholdsBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        length_thresholds: Union[int, list] = [0],\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_sizes: Union[float, list] = None,\n        m_probability_else=None,\n    ):\n        \"\"\"A comparison of the data in the array column `col_name` with various\n        size difference thresholds to assess similarity levels.\n\n        An example of the output with default arguments and settings\n        `length_thresholds = [0]` would be\n        - An exact match\n        - The two arrays are the same length\n        - Anything else (i.e. the arrays are difference lengths)\n\n        Args:\n            col_name (str): The name of the array column to compare.\n            length_thresholds (Union[int, list], optional): The difference(s) between\n                array sizes of thresholds, to assess whether two arrays are within a\n                given length difference.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_sizes (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the sizes specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Returns:\n            Comparison: A comparison that can be inclued in the Splink settings\n                dictionary.\n        \"\"\"\n\n        thresholds = ensure_is_iterable(length_thresholds)\n\n        if m_probability_or_probabilities_sizes is None:\n            m_probability_or_probabilities_sizes = [None] * len(thresholds)\n        m_probabilities = ensure_is_iterable(m_probability_or_probabilities_sizes)\n\n        comparison_levels = []\n        comparison_levels.append(self._null_level(col_name))\n        if include_exact_match_level:\n            level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match,\n            )\n            comparison_levels.append(level)\n\n        for length_thres, m_prob in zip(thresholds, m_probabilities):\n            level = self._array_length_level(\n                col_name,\n                length_difference=length_thres,\n                m_probability=m_prob,\n            )\n            comparison_levels.append(level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        thres_desc = \", \".join(thresholds)\n        plural = \"\" if len(thresholds) == 1 else \"s\"\n        comparison_desc += (\n            f\"Array length differences with threshold{plural} {thres_desc} vs. \"\n        )\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n</code></pre> <p>Crucially we needed to use <code>self._null_level</code>, <code>self._exact_match_level</code>, <code>self._else_level</code> which already exist, but also the new <code>self._array_length_level</code> which relates to our new comparison level. We will need to make sure that the dialect-specific comparisons which will actually be used will have this property.</p> <p>Each dialect has a comparison properties base, which stores information about all of the dialect-specific comparison levels used by all comparisons. We will need to add our new level to this, which we referred to above in <code>ArrayLengthAtThresholdsBase</code> - for this example in <code>splink.spark.spark_helpers.spark_comparison_imports</code>: </p><pre><code>from splink.spark.comparison_level_library import (\n    exact_match_level,\n    ...\n    array_length_level,\n)\n...\n\nclass SparkComparisonProperties(SparkBase):\n    @property\n    def _exact_match_level(self):\n        return exact_match_level\n    ...\n    @property\n    def _array_length_level(self):\n        return array_length_level\n</code></pre> <p>Any dialect-specific version of comparisons will inherit from this (where it learns about the dialect-specific comparison levels), and the comparison itself; in our case, in the same file:</p> <pre><code>...\nclass array_length_at_thresholds(\n    SparkComparisonProperties, ArrayLengthAtThresholds\n):\n    pass\n</code></pre> <p>This is now ready to import and be used, just as any other pre-existing comparisons.</p>","tags":["API","comparisons"]},{"location":"dev_guides/settings_validation/extending_settings_validator.html","title":"Extending the Settings Validator","text":""},{"location":"dev_guides/settings_validation/extending_settings_validator.html#expanding-the-settings-validator","title":"Expanding the Settings Validator","text":"<p>If a validation check is currently missing, you might want to expand the existing validation codebase.</p> <p>Before adding any code, it's essential to determine whether the checks you want to include fit into any of the general validation categories already in place.</p> <p>In summary, the following validation checks are currently carried out:</p> <ul> <li>Verifying that the user's blocking rules and comparison levels have been imported from the correct library and contain sufficient information for Splink model usage.</li> <li>Performing column lookups to ensure that columns specified in the user's settings dictionary exist within all of the user's input dataframes.</li> <li>Various miscellaneous checks designed to generate more informative error messages for the user if they happen to employ Splink in an unintended manner.</li> </ul> <p>If you plan to introduce checks that differ from those currently in place, it's advisable to create a new script within <code>splink/settings_validation</code>.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#splink-exceptions-and-warnings","title":"Splink Exceptions and Warnings","text":"<p>While working on extending the settings validation tools suite, it's important to consider how we notify users when they've included invalid settings or features.</p> <p>Exception handling and warnings should be integrated into your validation functions to either halt the program or inform the user when errors occur, raising informative error messages as needed.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#warnings-in-splink","title":"Warnings in Splink","text":"<p>Warnings should be employed when you want to alert the user that an included setting might lead to unintended consequences, allowing the user to decide if it warrants further action.</p> <p>This could be applicable in scenarios such as:</p> <ul> <li>Parsing SQL where the potential for failure or incorrect column parsing exists.</li> <li>Situations where the user is better positioned to determine whether the issue should be treated as an error, like when dealing with exceptionally high values for probability_two_random_records_match.</li> </ul> <p>Implementing warnings is straightforward and involves creating a logger instance within your script, followed by a warning call.</p> Warnings in practice: <pre><code>import logging\nlogger = logging.getLogger(__name__)\n\nlogger.warning(\"My warning message\")\n</code></pre> <p>Which will print:</p> <p><code>My warning message</code></p> <p>to both the console and your log file.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#splink-exceptions","title":"Splink Exceptions","text":"<p>Exceptions should be raised when you want the program to halt due to an unequivocal error.</p> <p>In addition to the built-in exception types, such as SyntaxError, we have several Splink-specific exceptions available for use.</p> <p>These exceptions serve to raise issues specific to Splink or to customize exception behavior. For instance, you can specify a message prefix by modifying the constructor of an exception, as exemplified in the ComparisonSettingsException.</p> <p>It's crucial to also consider how to inform the user that such behavior is not permitted. For guidelines on crafting effective error messages, refer to How to Write Good Error Messages.</p> <p>For a comprehensive list of exceptions native to Splink, visit the exceptions.py script.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#raising-multiple-exceptions","title":"Raising Multiple Exceptions","text":"<p>Raising multiple errors sequentially without disrupting the program, is a feature we commonly wish to implement across the validation steps.</p> <p>In numerous instances, it makes sense to wait until all checks have been performed before raising exceptions captured to the user in one go.</p> <p>To enable the logging of multiple errors in a singular check, or across multiple checks, an <code>ErrorLogger</code> class is available for use.</p> <p>The <code>ErrorLogger</code> operates in a similar way to working with a list, allowing you to add additional errors using the <code>append</code> method. Once you've logged all of your errors, you can raise them with the <code>raise_and_log_all_errors</code> method.</p> <code>ErrorLogger</code> in practice <pre><code>from splink.exceptions import ErrorLogger\n\n# Create an error logger instance\ne = ErrorLogger()\n\n# Log your errors\ne.append(SyntaxError(\"The syntax is wrong\"))\ne.append(NameError(\"Invalid name entered\"))\n\n# Raise your errors\ne.raise_and_log_all_errors()\n</code></pre> <p></p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#expanding-our-miscellaneous-checks","title":"Expanding our Miscellaneous Checks","text":"<p>Miscellaneous checks should typically be added as standalone functions. These functions can then be integrated into the linker's startup process for validation.</p> <p>In most cases, you have more flexibility in how you structure your solutions. You can place the checks in a script that corresponds to the specific checks being performed, or, if one doesn't already exist, create a new script with a descriptive name.</p> <p>A prime example of a miscellaneous check is <code>validate_dialect</code>, which assesses whether the settings dialect aligns with the linker's dialect.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#additional-comparison-and-blocking-rule-checks","title":"Additional Comparison and Blocking Rule Checks","text":"<p>If your checks pertain to comparisons or blocking rules, most of these checks are currently implemented within the valid_types.py script.</p> <p>Currently, comparison and blocking rule checks are organised in a modular format.</p> <p>To expand the current suite of tests, you should:</p> <ol> <li>Create a function to inspect the presence of the error you're evaluating.</li> <li>Define an error message that you intend to add to the <code>ErrorLogger</code> class.</li> <li>Integrate these elements into either the <code>validate_comparison_levels</code> function (or something similar), which appends any detected errors to an <code>ErrorLogger</code>.</li> <li>Finally, work out where this function should live in the setup process of the linker object. Typically, you should look to add these checks before any processing of the settings dictionary is performed.</li> </ol> <p>The above steps are set to change as we are looking to refactor our settings object.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#checking-that-columns-exist","title":"Checking that columns exist","text":"<p>Should you need to include extra checks to assess the validity of columns supplied by a user, your primary focus should be on the column_lookups.py script.</p> <p>There are currently three classes employed to construct the current log strings. These can be extended to perform additional column checks.</p> <code>InvalidCols</code> <p><code>InvalidCols</code> is a NamedTuple, used to construct the bulk of our log strings. This accepts a list of columns and the type of error, producing a complete log string when requested.</p> <p>In practice, this is used as follows: </p><pre><code># Store the invalid columns and why they're invalid\nmy_invalid_cols = InvalidCols(\"invalid_cols\", [\"first_col\", \"second_col\"])\n# Construct the corresponding log string\nmy_invalid_cols.construct_log_string()\n</code></pre> <code>InvalidColValidator</code> <p><code>InvalidColValidator</code> houses a series of validation checks to evaluate whether the column(s) contained within either a SQL string or a user's raw input string, are present within the underlying dataframes.</p> <p>To achieve this, it employs a range of cleaning functions to standardise our column inputs and conducts a series of checks on these cleaned columns. It utilises <code>InvalidCols</code> tuples to log any identified invalid columns.</p> <p>It inherits from our the <code>SettingsValidator</code> class.</p> <code>InvalidColumnsLogger</code> <p>The principal logging class for our invalid column checks.</p> <p>This class primarily calls our builder functions outlined in <code>InvalidColValidator</code>, constructing a series of log strings for output to both the console and the user's log file (if it exists).</p> <p>To extend the column checks, you simply need to add an additional validation method to the <code>InvalidColValidator</code> class, followed by an extension of the <code>InvalidColumnsLogger</code>.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#a-practical-example-of-a-column-check","title":"A Practical Example of a Column Check","text":"<p>For an example of column checks in practice, see <code>validate_uid</code>.</p> <p>Here, we call <code>validate_settings_column</code>, checking whether the unique ID column submitted by the user is valid. The output of this call yields either an <code>InvalidCols</code> tuple, or <code>None</code>.</p> <p>From there, we can use the built-in log constructor <code>construct_generic_settings_log_string</code> to construct and print the required logs. Where the output above was <code>None</code>, nothing is logged.</p> <p>If your checks aren't part of the initial settings check (say you want to assess additional columns found in blocking rules supplied at a later stage by the user), you should add a new method to <code>InvalidColumnsLogger</code>, similar in functionality to <code>construct_output_logs</code>.</p> <p>However, it is worth noting that not all checks are performed on a simple string columns. Where you require checks to be performed on SQL strings, there's an additional step required, outlined below.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#single-column-checks","title":"Single Column Checks","text":"<p>To review single columns, <code>validate_settings_column</code> should be used. This takes in a <code>setting_id</code> (analogous to the title you want to give your log string) and a list of columns to be checked.</p> <p>A working example of this in practice can be found in the section above.</p>"},{"location":"dev_guides/settings_validation/extending_settings_validator.html#checking-columns-in-sql-statements","title":"Checking Columns in SQL statements","text":"<p>For raw SQL statements, you should make use of the <code>validate_columns_in_sql_strings</code> method.</p> <p>This takes in a list of SQL strings and spits out a list of <code>InvalidCols</code> tuples, depending on the checks you ask it to perform.</p> <p>Should you need more control, the process is similar to that of the single column case, just with an additional parsing step.</p> <p>Parsing is handled by <code>parse_columns_in_sql</code>. This will spit out a list of column names that were identified by sqlglot.</p> <p>Note that as this is handled by SQLglot, it's not always 100% accurate. For our purposes though, its flexibility is unparalleled and allows us to more easily and efficiently extract column names.</p> <p>Once your columns have been parsed, you can again run a series of lookups against your input dataframe(s). This is identical to the steps outlined in the Single Column Checks section.</p> <p>You may also wish to perform additional checks on the columns, to assess whether they contain valid prefixes, suffixes or some other quality of the column.</p> <p>Additional checks can be passed to <code>validate_columns_in_sql_strings</code> and should be specified as methods in the <code>InvalidColValidator</code> class.</p> <p>See validate_blocking_rules for a practical example where we loop through each blocking rule, parse it and then assess whether it:</p> <ol> <li>Contains a valid list of columns</li> <li>Each column contains a valid table prefix.</li> </ol>"},{"location":"dev_guides/settings_validation/settings_validation_overview.html","title":"Settings Validation Overview","text":""},{"location":"dev_guides/settings_validation/settings_validation_overview.html#settings-validation","title":"Settings Validation","text":"<p>A common issue within Splink is users providing invalid settings dictionaries. To prevent this, the settings validator scans through a settings dictionary and provides user-friendly feedback on what needs to be fixed.</p> <p>At a high level, this includes:</p> <ol> <li>Assessing the structure of the settings dictionary. See the Settings Schema Validation section.</li> <li>The contents of the settings dictionary. See the Settings Vaildator section.</li> </ol>"},{"location":"dev_guides/settings_validation/settings_validation_overview.html#settings-schema-validation","title":"Settings Schema Validation","text":"<p>Our custom settings schema can be found within settings_jsonschema.json.</p> <p>This is a json file, outlining the required data type, key and value(s) to be specified by the user while constructing their settings. Where values devivate from this specified schema, an error will be thrown.</p> <p>Schema validation is currently performed inside the settings.py script.</p> <p>You can modify the schema by manually editing the json schema.</p> <p>Modifications can be used to (amongst other uses):</p> <ul> <li>Set or remove default values for schema keys.</li> <li>Set the required data type for a given key.</li> <li>Expand or refine previous titles and descriptions to help with clarity.</li> </ul> <p>Any updates you wish to make to the schema should be discussed with the wider team, to ensure it won't break backwards compatability and makes sense as a design decision.</p> <p>Detailed information on the arguments that can be suppled to the json schema can be found within the json schema documentation.</p>"},{"location":"dev_guides/settings_validation/settings_validation_overview.html#settings-validator","title":"Settings Validator","text":"<p>The settings validation code currently resides in the settings validation directory of Splink. This code is responsible for executing a secondary series of tests to determine whether all values within the settings dictionary will generate valid SQL.</p> <p>Numerous inputs pass our initial schema checks before breaking other parts of the codebase. These breaks are typically due to the construction of invalid SQL, that is then passed to the database engine, commonly resulting in uninformative errors.</p> <p>Frequently encountered problems include:</p> <ul> <li>Usage of invalid column names. For example, specifying a <code>unique_id_column_name</code> that doesn't exist in the underlying dataframe(s). Such names satisfy the schema requirements as long as they are strings.</li> <li>Users not updating default values in the settings schema, even when these values are inappropriate for their provided input dataframes.</li> <li>Importing comparisons and blocking rules from incorrect sections of the codebase, or using an inappropriate data type (comparison level vs. comparison).</li> <li>Using Splink for an invalid form of linkage. See the following dicsussion.</li> </ul> <p>Currently, the settings validation scripts are setup in a modular fashion, to allow each to inherit the checks it needs.</p> <p>The folder is comprised of three scripts, each of which inspects the settings dictionary at different stages of its journey:</p> <ul> <li>valid_types.py - This script includes various miscellaneous checks for comparison levels, blocking rules, and linker objects. These checks are primarily performed within settings.py.</li> <li>settings_validator.py - This script includes the core <code>SettingsValidator</code> class and contains a series of methods that retrieve information on fields within the user's settings dictionary that contain information on columns to be used in training and prediction. Additionally, it provides supplementary cleaning functions to assist in the removal of quotes, prefixes, and suffixes that may be present in a given column name.</li> <li>column_lookups.py - This script contains helper functions that generate a series of log strings outlining invalid columns identified within your settings dictionary. It primarily consists of methods that run validation checks on either raw SQL or input columns and assesses their presence in all dataframes supplied by the user.</li> </ul> <p>For information on expanding the range of checks available to the validator, see Extending the Settings Validator.</p>"},{"location":"includes/tags.html","title":"Tags","text":""},{"location":"includes/tags.html#tags","title":"Tags","text":"<p>Following is a list of relevant tags:</p> <p>[TAGS]</p>"},{"location":"includes/generated_files/comparison_composition_library_dialect_table.html","title":"Comparison composition library dialect table","text":"DuckDB  Spark  Athena  SQLite  PostgreSql and_ \u2713 \u2713 \u2713 \u2713 \u2713 not_ \u2713 \u2713 \u2713 \u2713 \u2713 or_ \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"includes/generated_files/comparison_level_library_dialect_table.html","title":"Comparison level library dialect table","text":"DuckDB  Spark  Athena  SQLite  PostgreSql array_intersect_level \u2713 \u2713 \u2713 \u2713 columns_reversed_level \u2713 \u2713 \u2713 \u2713 \u2713 damerau_levenshtein_level \u2713 \u2713 \u2713 datediff_level \u2713 \u2713 \u2713 \u2713 distance_function_level \u2713 \u2713 \u2713 \u2713 \u2713 distance_in_km_level \u2713 \u2713 \u2713 \u2713 else_level \u2713 \u2713 \u2713 \u2713 \u2713 exact_match_level \u2713 \u2713 \u2713 \u2713 \u2713 jaccard_level \u2713 \u2713 jaro_level \u2713 \u2713 \u2713 jaro_winkler_level \u2713 \u2713 \u2713 levenshtein_level \u2713 \u2713 \u2713 \u2713 \u2713 null_level \u2713 \u2713 \u2713 \u2713 \u2713 percentage_difference_level \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"includes/generated_files/comparison_library_dialect_table.html","title":"Comparison library dialect table","text":"DuckDB  Spark  Athena  SQLite  PostgreSql array_intersect_at_sizes \u2713 \u2713 \u2713 \u2713 damerau_levenshtein_at_thresholds \u2713 \u2713 \u2713 datediff_at_thresholds \u2713 \u2713 \u2713 \u2713 distance_function_at_thresholds \u2713 \u2713 \u2713 \u2713 \u2713 distance_in_km_at_thresholds \u2713 \u2713 \u2713 \u2713 exact_match \u2713 \u2713 \u2713 \u2713 \u2713 jaccard_at_thresholds \u2713 \u2713 jaro_at_thresholds \u2713 \u2713 \u2713 jaro_winkler_at_thresholds \u2713 \u2713 \u2713 levenshtein_at_thresholds \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"includes/generated_files/comparison_template_library_dialect_table.html","title":"Comparison template library dialect table","text":"DuckDB  Spark  Athena  SQLite  PostgreSql date_comparison \u2713 \u2713 email_comparison \u2713 \u2713 forename_surname_comparison \u2713 \u2713 \u2713 name_comparison \u2713 \u2713 \u2713 postcode_comparison \u2713 \u2713 \u2713"},{"location":"includes/generated_files/dataset_labels_table.html","title":"Dataset labels table","text":"dataset name description rows unique entities link to source <code>fake_1000_labels</code> Clerical labels for fake_1000 3,176 NA source"},{"location":"includes/generated_files/datasets_table.html","title":"Datasets table","text":"dataset name description rows unique entities link to source <code>fake_1000</code> Fake 1000 from splink demos.  Records are 250 simulated people, with different numbers of duplicates, labelled. 1,000 250 source <code>historical_50k</code> The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors. 50,000 5,156 source <code>febrl3</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL3 data set contains 5000 records (2000 originals and 3000 duplicates), with a maximum of 5 duplicates based on one original record. 5,000 2,000 source <code>febrl4a</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4a contains 5000 original records. 5,000 5,000 source <code>febrl4b</code> The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4b contains 5000 duplicate records, one for each record in FEBRL4a. 5,000 5,000 source <code>transactions_origin</code> This data has been generated to resemble bank transactions leaving an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart arriving in 'transactions_destination'. Memo is sometimes truncated or missing. 45,326 45,326 source <code>transactions_destination</code> This data has been generated to resemble bank transactions arriving in an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart sent from 'transactions_origin'. There may be a delay between the source and destination account, and the amount may vary due to hidden fees and foreign exchange rates. Memo is sometimes truncated or missing. 45,326 45,326 source"},{"location":"settingseditor/editor.html","title":"Interactive Settings Editor","text":"","tags":["settings"]},{"location":"topic_guides/topic_guides_index.html","title":"Introduction","text":""},{"location":"topic_guides/topic_guides_index.html#topic-guides","title":"Topic Guides","text":"<p>This section contains in-depth guides on a variety of topics and concepts within Splink, as well as data linking more generally. These are intended to provide an extra layer of detail ontop of the Splink tutorial and examples.</p> <p>The topic guides are broken up into the following categories:</p> <ol> <li>Record Linkage Theory - for an introduction to data linkage from a theoretical perspective, and to help build some intuition around the parameters being estimated in Splink models.  </li> <li>Linkage Models in Splink - for an introduction to the building blocks of a Splink model. Including the supported SQL Backends and how to define a model with a Splink Settings dictionary.</li> <li>Data Preparation - for guidance on preparing your data for linkage. Including guidance on feature engineering to help improve Splink models. </li> <li>Blocking - for an introduction to Blocking Rules and their purpose within record linkage. Including how blocking rules are used in different contexts within Splink.</li> <li>Comparing Records - for guidance on defining <code>Comparison</code>s withing a Splink model. Including how comparing records are structured within <code>Comparison</code>s, how to utilise string comparators for fuzzy matching and how deal with skewed data with Term Frequency Adjustments.</li> <li>Model Training - for guidance on the methods for training a Splink model, and how to choose them for specific use cases. (Coming soon)</li> <li>Clustering - for guidance on how records are clustered together. (Coming Soon)</li> <li>Evaluation - for guidance on how to evaluate Splink models, links and clusters (including Clerical Labelling). </li> <li>Performance - for guidance on how to make Splink models run more efficiently.</li> </ol>"},{"location":"topic_guides/blocking/blocking_rules.html","title":"What are Blocking Rules?","text":"","tags":["Blocking","Performance"]},{"location":"topic_guides/blocking/blocking_rules.html#the-challenges-of-record-linkage","title":"The Challenges of Record Linkage","text":"<p>One of the main challenges to overcome in record linkage is the scale of the problem.</p> <p>The number of pairs of records to compare grows using the formula \\(\\frac{n\\left(n-1\\right)}2\\), i.e. with (approximately) the square of the number of records, as shown in the following chart:</p> <p></p> <p>For example, a dataset of 1 million input records would generate around 500 billion pairwise record comparisons.</p> <p>So, when datasets get bigger the amount of computational resource gets extremely large (and costly). In reality, we try and reduce the amount of computation required using blocking.</p>","tags":["Blocking","Performance"]},{"location":"topic_guides/blocking/blocking_rules.html#blocking","title":"Blocking","text":"<p>Blocking is a technique for reducing the number of record pairs that are considered by a model.</p> <p>Considering a dataset of 1 million records, comparing each record against all of the other records in the dataset generates ~500 billion pairwise comparisons. However, we know the vast majority of these record comparisons won't be matches, so processing the full ~500 billion comparisons would be largely pointless (as well as costly and time-consuming).</p> <p>Instead, we can define a subset of potential comparisons using Blocking Rules. These are rules that define \"blocks\" of comparisons that should be considered. For example, the blocking rule:</p> <p><code>\"l.first_name = r.first_name and l.surname = r.surname\"</code> </p> <p>will generate only those pairwise record comparisons where first name and surname match.</p> <p>Within a Splink model, you can specify multiple Blocking Rules to ensure all potential matches are considered.  These are provided as a list.  Splink will then produce all record comparisons that satisfy at least one of your blocking rules.</p> Further Reading <p>For more information on blocking, please refer to this article</p>","tags":["Blocking","Performance"]},{"location":"topic_guides/blocking/blocking_rules.html#choosing-blocking-rules","title":"Choosing Blocking Rules","text":"<p>The aim of blocking rules is to recover all matching record pairs (i.e to have high recall).</p> <p>It is less important if the blocking rules select some (or even many) record pairs which are not matches (i.e. high precision). Record comparisons that 'pass' the blocking rules are then put forward to the scoring/prediction step. The more pairs let through, the more computation is required at the prediction step.</p> <p>Ultimately, the blocking process is a compromise between the amount of computational resource used when comparing records and capturing all true matches. </p> <p>Even after blocking, the number of comparisons generated is usually much higher than the number of input records - often between 10 and 1,000 times higher. As a result, the performance of Splink is heavily influenced by the number of comparisons generated by the blocking rules, rather than the number of input records.</p> <p>Getting the balance right between computational resource and capturing matches can be tricky, and is largely dependent on the specific datasets and use case of the linkage. In general, we recommend a strategy of starting with strict blocking rules, and gradually loosening them. Sticking to less than 10 million comparisons is a good place to start, before scaling jobs up to 100s of millions ( DuckDB on a laptop), or sometimes billions ( Spark or  Athena). </p> <p>Guidance for choosing Blocking Rules can be found in the two Blocking in Splink topic guides.</p> <p>Taking blocking to the extremes</p> <p>If you have a large dataset to deduplicate, let's consider the implications of two cases of taking blocking to the extremes:</p> <p>Not enough blocking (ensuring all matches are captured) There will be too many record pairs to consider, which will take an extremely long time to run (hours/days) or the process will be so large that it crashes.</p> <p>Too much blocking (minimising computational resource) There won't be enough records pairs to consider, so the model won't perform well (or will struggle to be trained at all). </p>","tags":["Blocking","Performance"]},{"location":"topic_guides/blocking/blocking_rules.html#blocking-in-splink","title":"Blocking in Splink","text":"<p>There are two areas in Splink where blocking is used:</p> <ul> <li> <p>The first is to generate pairwise comparisons when finding links (running <code>predict()</code>). This is the sense in which 'blocking' is usually understood in the context of record linkage</p> </li> <li> <p>The second is a less familiar application of blocking: using it for model training.</p> </li> </ul> <p>each of which is described in their own, dedicated topic guide.</p>","tags":["Blocking","Performance"]},{"location":"topic_guides/blocking/model_training.html","title":"Model Training Blocking Rules","text":""},{"location":"topic_guides/blocking/model_training.html#blocking-for-model-training","title":"Blocking for Model Training","text":"<p>Model Training Blocking Rules choose which record pairs from a dataset get considered when training a Splink model. These are used during Expectation Maximisation (EM), where we estimate the m probability (in most cases).</p> <p>The aim of Model Training Blocking Rules is to reduce the number of record pairs considered when training a Splink model in order to reduce the computational resource required. Each Training Blocking Rule define a training \"block\" of records which have a combination of matches and non-matches that are considered by Splink's Expectation Maximisation algorithm.</p> <p>The Expectation Maximisation algorithm seems to work best when the pairwise record comparisons are a mix of anywhere between around 0.1% and 99.9% true matches. It works less efficiently if there is a huge imbalance between the two (e.g. a billion non matches and only a hundred matches).</p> <p>Note</p> <p>Unlike Prediction Rules, it does not matter if Training Rules excludes some true matches - it just needs to generate examples of matches and non-matches.</p>"},{"location":"topic_guides/blocking/model_training.html#using-training-rules-in-splink","title":"Using Training Rules in Splink","text":"<p>Blocking Rules for Model Training are used as a parameter in the <code>estimate_parameters_using_expectation_maximisation</code> function. After a <code>linker</code> object has been instantiated, you can estimate <code>m probability</code> with training sessions such as:</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\n\nblocking_rule_for_training = block_on(\"first_name\")\nlinker.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n</code></pre> <p>Here, we have defined a \"block\" of records where <code>first_name</code> are the same. As names are not unique, we can be pretty sure that there will be a combination of matches and non-matches in this \"block\" which is what is required for the EM algorithm.</p> <p>Matching only on <code>first_name</code> will likely generate a large \"block\" of pairwise comparisons which will take longer to run. In this case it may be worthwhile applying a stricter blocking rule to reduce runtime. For example, a match on <code>first_name</code> and <code>surname</code>:</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n    )\n</code></pre> <p>which will still have a combination of matches and non-matches, but fewer record pairs to consider.</p>"},{"location":"topic_guides/blocking/model_training.html#choosing-training-rules","title":"Choosing Training Rules","text":"<p>The idea behind Training Rules is to consider \"blocks\" of record pairs with a mixture of matches and non-matches. In practice, most blocking rules have a mixture of matches and non-matches so the primary consideration should be to reduce the runtime of model training by choosing Training Rules that reduce the number of record pairs in the training set.</p> <p>There are some tools within Splink to help choosing these rules. For example, the <code>count_num_comparisons_from_blocking_rule</code> gives the number of records pairs generated by a blocking rule:</p> <pre><code>from splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.count_num_comparisons_from_blocking_rule(blocking_rule)\n</code></pre> <p>1056</p> <p>It is recommended that you run this function to check how many comparisons are generated before training a model so that you do not needlessly run a training session on billions of comparisons.</p> <p>Note</p> <p>Unlike Prediction Rules, Training Rules are treated separately for each EM training session therefore the tota number of comparisons for Model Training is simply the sum of <code>count_num_comparisons_from_blocking_rule</code> across all Blocking Rules (as opposed to the result of <code>cumulative_comparisons_from_blocking_rules_records</code>).</p>"},{"location":"topic_guides/blocking/performance.html","title":"Computational Performance","text":""},{"location":"topic_guides/blocking/performance.html#blocking-rule-performance","title":"Blocking Rule Performance","text":"<p>When considering computational performance of blocking rules, there are two main drivers to address:</p> <ul> <li>How may pairwise comparisons are generated</li> <li>How quickly each pairwise comparison takes to run</li> </ul> <p>Below we run through an example of how to address each of these drivers.</p>"},{"location":"topic_guides/blocking/performance.html#strict-vs-lenient-blocking-rules","title":"Strict vs lenient Blocking Rules","text":"<p>One way to reduce the number of comparisons being considered within a model is to apply strict blocking rules. However, this can have a significant impact on the how well the Splink model works.</p> <p>In reality, we recommend getting a model up and running with strict Blocking Rules and incrementally loosening them to see the impact on the runtime and quality of the results. By starting with strict blocking rules, the linking process will run faster which means you can iterate through model versions more quickly.</p> Example - Incrementally loosening Prediction Blocking Rules <p>When choosing Prediction Blocking Rules, consider how <code>blocking_rules_to_generate_predictions</code> may be made incrementally less strict. We may start with the following rule:</p> <p><code>l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob</code>.</p> <p>This is a very strict rule, and will only create comparisons where full name and date of birth match. This has the advantage of creating few record comparisons, but the disadvantage that the rule will miss true matches where there are typos or nulls in any of these three fields.</p> <p>This blocking rule could be loosened to:</p> <p><code>substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname and l.year_of_birth = r.year_of_birth</code></p> <p>Now it allows for typos or aliases in the first name, so long as the first letter is the same, and errors in month or day of birth.</p> <p>Depending on the side of your input data, the rule could be further loosened to</p> <p><code>substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname</code></p> <p>or even</p> <p><code>l.surname = r.surname</code></p> <p>The user could use the <code>linker.count_num_comparisons_from_blocking_rule()</code> function to select which rule is appropriate for their data.</p>"},{"location":"topic_guides/blocking/performance.html#efficient-blocking-rules","title":"Efficient Blocking Rules","text":"<p>While the number of pairwise comparisons is important for reducing the computation, it is also helpful to consider the efficiency of the Blocking Rules. There are a number of ways to define subsets of records (i.e. \"blocks\"), but they are not all computationally efficient.</p> <p>From a performance prespective, here we consider two classes of blocking rule:</p> <ul> <li>Equi-join conditions</li> <li>Filter conditions</li> </ul>"},{"location":"topic_guides/blocking/performance.html#equi-join-conditions","title":"Equi-join Conditions","text":"<p>Equi-joins are simply equality conditions between records, e.g.</p> <p><code>l.first_name = r.first_name</code></p> <p>These equality-based blocking rules are extremely efficient and can be executed quickly, even on very large datasets.</p> <p>Equality-based blocking rules should be considered the default method for defining blocking rules and form the basis of the Blocking Rules Library. For example, the above example can be written as:</p> <p><code>brl.block_on(\"first_name\")</code></p>"},{"location":"topic_guides/blocking/performance.html#filter-conditions","title":"Filter Conditions","text":"<p>Filter conditions refer to any Blocking Rule that isn't a simple equality between columns. E.g.</p> <p><code>levenshtein(l.surname, r.surname) &lt; 3</code></p> <p>Similarity based blocking rules, such as the example above, are inefficient as the <code>levenshtein</code> function needs to be evaluated for all possible record comparisons before filtering out the pairs that do not satisfy the filter condition.</p>"},{"location":"topic_guides/blocking/performance.html#combining-blocking-rules-efficiently","title":"Combining Blocking Rules Efficiently","text":"<p>Just as how Blocking Rules can impact on performance, so can how they are combined. The most efficient Blocking Rules combinations are \"AND\" statements. E.g.</p> <p><code>l.first_name = r.first_name AND l.surname = r.surname</code></p> <p>\"OR\" statements are not as efficient and should be used sparingly. E.g.</p> <p><code>l.first_name = r.first_name OR l.surname = r.surname</code></p> <p>In most SQL engines, an <code>OR</code> condition within a blocking rule will result in all possible record comparisons being generated.  That is, the whole blocking rule becomes a filter condition rather than an equi-join condition, so these should be avoided.  For further information, see here.</p> Spark-specific Further Reading <p>Given the ability to parallelise operations in Spark, there are some additional configuration options which can improve performance of blocking. Please refer to the Spark Performance Topic Guides for more information.</p> <p>Note: In Spark Equi-joins are implemented using hash partitioning, which facilitates splitting the workload across multiple machines.</p>"},{"location":"topic_guides/blocking/predictions.html","title":"Prediction Blocking Rules","text":""},{"location":"topic_guides/blocking/predictions.html#blocking-rules-for-splink-predictions","title":"Blocking Rules for Splink Predictions","text":"<p>Prediction Blocking Rules choose which record pairs from a dataset get considered and scored by the Splink model.</p> <p>The aim of Prediction Blocking Rules are to:</p> <ul> <li>Capture as many true matches as possible</li> <li>Reduce the total number of comparisons being generated</li> </ul>"},{"location":"topic_guides/blocking/predictions.html#using-prediction-rules-in-splink","title":"Using Prediction Rules in Splink","text":"<p>Blocking Rules for Prediction are defined through <code>blocking_rules_to_generate_predictions</code> in the Settings dictionary of a model. For example:</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n       brl.block_on([\"first_name\", \"surname\"]),\n       brl.block_on(\"dob\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\"),\n    ],\n}\n</code></pre> <p>will generate comparisons for all true matches where names match. But it would miss a true match where there was a typo in (say) the first name.</p> <p>In general, it is usually impossible to find a single rule which both:</p> <ul> <li> <p>Reduces the number of comparisons generated to a computationally tractable number</p> </li> <li> <p>Ensures comparisons are generated for all true matches</p> </li> </ul> <p>This is why <code>blocking_rules_to_generate_predictions</code> is a list. Suppose we also block on <code>postcode</code>:</p> <pre><code>settings_example = {\n    \"blocking_rules_to_generate_predictions\" [\n        brl.block_on([\"first_name\", \"surname\"]),\n        brl.block_on(\"postcode\")\n    ]\n}\n</code></pre> <p>This generates all pairwise comparisons that satisfy at least one of the rules.</p> <p>We will now generate a pairwise comparison for the record where there was a typo in the first name, so long as there isn't also a difference in the postcode.</p> <p>By specifying a variety of <code>blocking_rules_to_generate_predictions</code>, it becomes unlikely that a truly matching record would not be captured by at least one of the rules.</p> <p>Note</p> <p>Unlike Training Rules, Prediction Rules are considered collectively, and are order-dependent. So, in the example above, the <code>l.postcode = r.postcode</code> blocking rule only generates record comparisons that are a match on <code>postcode</code> were not already captured by the <code>first_name</code> and <code>surname</code> rule.</p>"},{"location":"topic_guides/blocking/predictions.html#choosing-prediction-rules","title":"Choosing Prediction Rules","text":"<p>When defining blocking rules it is important to consider the number of pairwise comparisons being generated your the blocking rules. There are a number of useful functions in Splink which can help with this.</p> <p>Once a linker has been instated, we can use the <code>cumulative_num_comparisons_from_blocking_rules_chart</code> function to look at the cumulative number of comparisons generated by <code>blocking_rules_to_generate_predictions</code>. For example, a setting dictionary like this:</p> <pre><code>settings = {\n    \"blocking_rules_to_generate_predictions\": [\n        brl.block_on(\"first_name\"),\n        brl.block_on(\"surname\")\n    ],\n}\n</code></pre> <p>will generate the something like:</p> <pre><code>linker = DuckDBLinker(df, settings)\nlinker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <p></p> <p>Where, similar to the note above, the <code>l.surname = r.surname</code> bar in light blue is a count of all record comparisons that match on <code>surname</code> that have not already been captured by the <code>first_name</code> rule.</p> <p>You can also return the underlying data for this chart using the <code>cumulative_comparisons_from_blocking_rules_records</code> function:</p> <pre><code>linker.cumulative_comparisons_from_blocking_rules_records()\n</code></pre> <p>[{'row_count': 2253, 'rule': 'l.first_name = r.first_name'}, {'row_count': 2568, 'rule': 'l.surname = r.surname'}]</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html","title":"Choosing comparators and thresholds","text":"<pre><code>import splink.comparison_helpers as ch\n\nch.comparator_score(\"Richard\", \"iRchard\")\n</code></pre> string1 string2 levenshtein_distance damerau_levenshtein_distance jaro_similarity jaro_winkler_similarity jaccard_similarity 0 Richard iRchard 2 1 0.95 0.95 1.0 <p>Now consider a collection of common variations of the name \"Richard\" - which comparators will consider these variations as sufficiently similar to \"Richard\"?</p> <pre><code>import pandas as pd\n\ndata = {\n    \"string1\": [\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n    ],\n    \"string2\": [\n        \"Richard\",\n        \"ichard\",\n        \"Richar\",\n        \"iRchard\",\n        \"Richadr\",\n        \"Rich\",\n        \"Rick\",\n        \"Ricky\",\n        \"Dick\",\n        \"Rico\",\n        \"Rachael\",\n        \"Stephen\",\n    ],\n    \"error_type\": [\n        \"None\",\n        \"Deletion\",\n        \"Deletion\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Shortening\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Different Name\",\n        \"Different Name\",\n    ],\n}\ndf = pd.DataFrame(data)\ndf\n</code></pre> string1 string2 error_type 0 Richard Richard None 1 Richard ichard Deletion 2 Richard Richar Deletion 3 Richard iRchard Transposition 4 Richard Richadr Transposition 5 Richard Rich Shortening 6 Richard Rick Nickname/Alias 7 Richard Ricky Nickname/Alias 8 Richard Dick Nickname/Alias 9 Richard Rico Nickname/Alias 10 Richard Rachael Different Name 11 Richard Stephen Different Name <p>The <code>comparator_score_chart</code> function allows you to compare two lists of strings and how similar the elements are according to the available string similarity and distance metrics.</p> <pre><code>ch.comparator_score_chart(data, \"string1\", \"string2\")\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink/splink/comparison_helpers.py:121: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  similarity_df[\"comparator\"] = similarity_df[\"comparator\"].str.replace(\n/Users/rosskennedy/splink/splink/comparison_helpers.py:126: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  distance_df[\"comparator\"] = distance_df[\"comparator\"].str.replace(\"_distance\", \"\")\n</code>\n</pre> <p>Here we can see that all of the metrics are fairly sensitive to transcriptions errors (\"Richadr\", \"Richar\", \"iRchard\"). However, considering nicknames/aliases (\"Rick\", \"Ricky\", \"Rico\"), simple metrics such as Jaccard, Levenshtein and Damerau-Levenshtein tend to be less useful. The same can be said for name shortenings (\"Rich\"), but to a lesser extent than more complex nicknames. However, even more subtle metrics like Jaro and Jaro-Winkler still struggle to identify less obvious nicknames/aliases such as \"Dick\". </p> <p>If you would prefer the underlying dataframe instead of the chart, there is the <code>comparator_score_df</code> function.</p> <pre><code>ch.comparator_score_df(data, \"string1\", \"string2\")\n</code></pre> string1 string2 levenshtein_distance damerau_levenshtein_distance jaro_similarity jaro_winkler_similarity jaccard_similarity 0 Richard Richard 0 0 1.00 1.00 1.00 1 Richard ichard 1 1 0.95 0.95 0.86 2 Richard Richar 1 1 0.95 0.97 0.86 3 Richard iRchard 2 1 0.95 0.95 1.00 4 Richard Richadr 2 1 0.95 0.97 1.00 5 Richard Rich 3 3 0.86 0.91 0.57 6 Richard Rick 4 4 0.73 0.81 0.38 7 Richard Ricky 4 4 0.68 0.68 0.33 8 Richard Dick 5 5 0.60 0.60 0.22 9 Richard Rico 4 4 0.73 0.81 0.38 10 Richard Rachael 3 3 0.71 0.74 0.44 11 Richard Stephen 7 7 0.43 0.43 0.08 <pre><code>ch.comparator_score_threshold_chart(\n    data, \"string1\", \"string2\", distance_threshold=2, similarity_threshold=0.8\n)\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink/splink/comparison_helpers.py:172: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  similarity_df[\"comparator\"] = similarity_df[\"comparator\"].str.replace(\n/Users/rosskennedy/splink/splink/comparison_helpers.py:177: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  distance_df[\"comparator\"] = distance_df[\"comparator\"].str.replace(\"_distance\", \"\")\n</code>\n</pre> <p>To class our variations on \"Richard\" in the same <code>Comparison Level</code>, a good choice of metric could be Jaro-Winkler with a threshold of 0.8. Lowering the threshold any more could increase the chances for false positives. </p> <p>For example, consider a single Jaro-Winkler <code>Comparison Level</code> threshold of 0.7 would lead to \"Rachael\" being considered as providing the same amount evidence for a record matching as \"iRchard\".</p> <p>An alternative way around this is to construct a <code>Comparison</code> with multiple levels, each corresponding to a different threshold of Jaro-Winkler similarity. For example, below we construct a <code>Comparison</code> using the <code>Comparison Library</code> function jaro_winkler_at_thresholds with multiple levels for different match thresholds.:</p> <pre><code>import splink.duckdb.comparison_library as cl\n\nfirst_name_comparison = cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.8, 0.7])\n</code></pre> <p>If we print this comparison as a dictionary we can see the underlying SQL.</p> <pre><code>first_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'first_name',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.9'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.7',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.7'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. First_Name within jaro_winkler_similarity thresholds 0.9, 0.8, 0.7 vs. anything else'}</code>\n</pre> <p>Where:  </p> <ul> <li>Exact Match level will catch perfect matches (\"Richard\").  </li> <li>The 0.9 threshold will catch Shortenings and Typos (\"ichard\", \"Richar\", \"iRchard\", \"Richadr\",  \"Rich\").  </li> <li>The 0.8 threshold will catch simple Nicknames/Aliases (\"Rick\", \"Rico\").  </li> <li>The 0.7 threshold will catch more complex Nicknames/Aliases (\"Ricky\"), but will also include less relevant names (e.g. \"Rachael\"). However, this should not be a concern as the model should give less predictive power (i.e. Match Weight) to this level of evidence.  </li> <li>All other comparisons will end up in the \"Else\" level  </li> </ul> <pre><code>import splink.comparison_helpers\n\nch.phonetic_transform(\"Richard\")\n</code></pre> <pre>\n<code>{'soundex': 'R02063', 'metaphone': 'RXRT', 'dmetaphone': ('RXRT', 'RKRT')}</code>\n</pre> <pre><code>ch.phonetic_transform(\"Steven\")\n</code></pre> <pre>\n<code>{'soundex': 'S30105', 'metaphone': 'STFN', 'dmetaphone': ('STFN', '')}</code>\n</pre> <p>Now consider a collection of common variations of the name \"Stephen\". Which phonetic transforms will consider these as sufficiently similar to \"Stephen\"?</p> <pre><code>data = {\n    \"string1\": [\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n    ],\n    \"string2\": [\n        \"Stephen\",\n        \"Steven\",\n        \"Stephan\",\n        \"Steve\",\n        \"Stehpen\",\n        \"tSephen\",\n        \"Stephne\",\n        \"Stphen\",\n        \"Stepheb\",\n        \"Stephanie\",\n        \"Richard\",\n    ],\n    \"error_type\": [\n        \"None\",\n        \"Spelling Variation\",\n        \"Spelling Variation/Similar Name\",\n        \"Nickname/Alias\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Deletion\",\n        \"Replacement\",\n        \"Different Name\",\n        \"Different Name\",\n    ],\n}\n\ndf = pd.DataFrame(data)\ndf\n</code></pre> string1 string2 error_type 0 Stephen Stephen None 1 Stephen Steven Spelling Variation 2 Stephen Stephan Spelling Variation/Similar Name 3 Stephen Steve Nickname/Alias 4 Stephen Stehpen Transposition 5 Stephen tSephen Transposition 6 Stephen Stephne Transposition 7 Stephen Stphen Deletion 8 Stephen Stepheb Replacement 9 Stephen Stephanie Different Name 10 Stephen Richard Different Name <p>The <code>phonetic_match_chart</code> function allows you to compare two lists of strings and how similar the elements are according to the available string similarity and distance metrics.</p> <pre><code>ch.phonetic_match_chart(data, \"string1\", \"string2\")\n</code></pre> <p>Here we can see that all of the algorithms recognise simple phonetically similar names (\"Stephen\", \"Steven\"). However, there is some variation when it comes to transposition errors (\"Stehpen\", \"Stephne\") with soundex and metaphone-esque giving different results. There is also different behaviour considering different names (\"Stephanie\").</p> <p>Given there is no clear winner that captures all of the similar names, it is recommended that phonetic matches are used as a single <code>Comparison Level</code> within in a <code>Comparison</code> which also includes string comparators in the other levels. To see an example of this, see the Combining String scores and Phonetic matching section of this topic guide.</p> <p>If you would prefer the underlying dataframe instead of the chart, there is the <code>phonetic_transform_df</code> function.</p> <pre><code>ch.phonetic_transform_df(data, \"string1\", \"string2\")\n</code></pre> string1 string2 soundex metaphone dmetaphone 0 Stephen Stephen [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 1 Stephen Steven [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 2 Stephen Stephan [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 3 Stephen Steve [S30105, S3010] [STFN, STF] [(STFN, ), (STF, )] 4 Stephen Stehpen [S30105, S30105] [STFN, STPN] [(STFN, ), (STPN, )] 5 Stephen tSephen [S30105, t50105] [STFN, TSFN] [(STFN, ), (TSFN, )] 6 Stephen Stephne [S30105, S301050] [STFN, STFN] [(STFN, ), (STFN, )] 7 Stephen Stphen [S30105, S3105] [STFN, STFN] [(STFN, ), (STFN, )] 8 Stephen Stepheb [S30105, S30101] [STFN, STFP] [(STFN, ), (STFP, )] 9 Stephen Stephanie [S30105, S301050] [STFN, STFN] [(STFN, ), (STFN, )] 10 Stephen Richard [S30105, R02063] [STFN, RXRT] [(STFN, ), (RXRT, RKRT)] <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n    \"first_name\",\n    phonetic_col_name=\"first_name_dm\",\n    damerau_levenshtein_thresholds=[],\n    levenshtein_thresholds=[2],\n    jaro_winkler_thresholds=[0.8],\n)\n\nfirst_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_first_name_first_name_dm',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match first_name'},\n  {'sql_condition': '\"first_name_dm_l\" = \"first_name_dm_r\"',\n   'label_for_charts': 'Exact match first_name_dm'},\n  {'sql_condition': 'levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. First_Name within levenshtein threshold 2 vs. First_Name within jaro_winkler threshold 0.8 vs. anything else'}</code>\n</pre> <p>where <code>first_name_dm</code> refers to a column in the dataset which has been created during the feature engineering step to give the <code>Dmetaphone</code> transform of <code>first_name</code>.</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#choosing-string-comparators","title":"Choosing String Comparators","text":"<p>When building a Splink model, one of the most important aspects is defining the <code>Comparisons</code> and <code>Comparison Levels</code> that the model will train on. Each <code>Comparison Level</code> within a <code>Comparison</code> should contain a different amount of evidence that two records are a match, which the model can assign a Match Weight to. When considering different amounts of evidence for the model, it is helpful to explore fuzzy matching as a way of distinguishing strings that are similar, but not the same, as one another.</p> <p>This guide is intended to show how Splink's string comparators perform in different situations in order to help choosing the most appropriate comparator for a given column as well as the most appropriate threshold (or thresholds). For descriptions and examples of each string comparators available in Splink, see the dedicated topic guide.</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#what-options-are-available-when-comparing-strings","title":"What options are available when comparing strings?","text":"<p>There are three main classes of string comparator that are considered within Splink:</p> <ol> <li>String Similarity Scores </li> <li>String Distance Scores </li> <li>Phonetic Matching </li> </ol> <p>where  </p> <p>String Similarity Scores are scores between 0 and 1 indicating how similar two strings are. 0 represents two completely dissimilar strings and 1 represents identical strings. E.g. Jaro-Winkler Similarity.  </p> <p>String Distance Scores are integer distances, counting the number of operations to convert one string into another. A lower string distance indicates more similar strings. E.g. Levenshtein Distance.  </p> <p>Phonetic Matching is whether two strings are phonetically similar. The two strings are passed through a phonetic transformation algorithm and then the resulting phonetic codes are matched. E.g. Double Metaphone.</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#comparing-string-similarity-and-distance-scores","title":"Comparing String Similarity and Distance Scores","text":"<p>Splink contains a <code>comparison_helpers</code> module which includes some helper functions for comparing the string similarity and distance scores that can help when choosing the most appropriate fuzzy matching function.</p> <p>For comparing two strings the <code>comparator_score</code> function returns the scores for all of the available comparators. E.g. consider a simple inversion \"Richard\" vs \"iRchard\":</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#choosing-thresholds","title":"Choosing thresholds","text":"<p>We can add distance and similarity thresholds to the comparators to see what strings would be included in a given comparison level:</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#phonetic-matching","title":"Phonetic Matching","text":"<p>There are similar functions available within splink to help users get familiar with phonetic transformations. You can create similar visualisations to string comparators.</p> <p>To see the phonetic transformations for a single string, there is the <code>phonetic_transform</code> function:</p>"},{"location":"topic_guides/comparisons/choosing_comparators.html#combining-string-scores-and-phonetic-matching","title":"Combining String scores and Phonetic matching","text":"<p>Once you have considered all of the string comparators and phonetic transforms for a given column, you may decide that you would like to have multiple comparison levels including a combination of options.</p> <p>For this you can construct a custom comparison to catch all of the edge cases you want. For example, if you decide that the comparison for <code>first_name</code> in the model should consider:</p> <ol> <li>A <code>Dmetaphone</code> level for phonetic similarity</li> <li>A <code>Levenshtein</code> level with distance of 2 for typos</li> <li>A <code>Jaro-Winkler</code> level with similarity 0.8 for fuzzy matching</li> </ol> <p>The name_comparison function from the <code>Comparison Template Library</code> can be configured as follows:</p>"},{"location":"topic_guides/comparisons/comparators.html","title":"String comparators","text":"","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#string-comparators","title":"String Comparators","text":"<p>A Splink model contains a collection of <code>Comparisons</code> and <code>ComparisonLevels</code> organised in a hierarchy.  For example:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: Up to one character difference\n\u2502    \u251c\u2500-- ComparisonLevel: Up to three character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> <p>For more detail on how comparisons are constructed, see the dedicated topic guide as well as fuller descriptions of <code>Comparisons</code> and <code>Comparison Levels</code>. </p> <p>Within <code>Comparisons</code> it is useful for different <code>Comparison Levels</code> to allow for different styles (and levels) fuzzy match. Each of these <code>Comparison Levels</code> indicates a different class of match between two records and therefore a different type (and amount) of evidence for or against the two records being a match. Once these <code>Comparison Levels</code> have been defined, the Splink model is trained to estimate the Match Weight to assign to each <code>Comparison Level</code>.</p> <p>There are a number of string comparator functions available in Splink that allow fuzzy matching for strings within <code>Comparisons</code> and <code>Comparison Levels</code>. For each of these fuzzy matching functions, below you will find explanations of how they work, worked examples and recommendations for the types of data they are useful for.</p> <p>For guidance on how to choose the most suitable string comparator, and associated threshold, see the dedicated topic guide.</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#levenshtein-distance","title":"Levenshtein Distance","text":"<p>At a glance</p> <p>Useful for: Data entry errors e.g. character miskeys. Splink comparison functions: levenshtein_level() and levenshtein_at_thresholds() Returns: An integer (lower is more similar).</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#description","title":"Description","text":"<p>Levenshtein distance, also known as edit distance, is a measure of the difference between two strings. It represents the minimum number of insertions, deletions, or substitutions of characters required to transform one string into the other.</p> <p>Or, as a formula,</p> \\[\\textsf{Levenshtein}(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion , } \\text{deletion , } \\text{substitution}  \\end{array} \\rbrace \\]","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#examples","title":"Examples","text":"\"KITTEN\" vs \"SITTING\" <p>The minimum number of operations to convert \"KITTEN\" into \"SITTING\" are:</p> <ul> <li>Substitute \"K\" in \"KITTEN\" with \"S\" to get \"SITTEN.\"</li> <li>Substitute \"E\" in \"SITTEN\" with \"I\" to get \"SITTIN.\"</li> <li>Insert \"G\" after \"N\" in \"SITTIN\" to get \"SITTING.\"</li> </ul> <p>Therefore, </p> \\[\\textsf{Levenshtein}(\\texttt{KITTEN}, \\texttt{SITTING}) = 3\\] \"CAKE\" vs \"ACKE\" <p>The minimum number of operations to convert \"CAKE\" into \"ACKE\" are:</p> <ul> <li>Substitute \"C\" in \"CAKE\" with \"A\" to get \"AAKE.\"</li> <li>substitute the second \"A\" in \"AAKE\" with \"C\" to get \"ACKE.\"</li> </ul> <p>Therefore, </p> \\[\\textsf{Levenshtein}(\\texttt{CAKE}, \\texttt{ACKE}) = 2\\]","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#sample-code","title":"Sample code","text":"<p>You can test out the Levenshtein distance between two strings through the jellyfish package.</p> <pre><code>import jellyfish\nlevenshtein_distance(\"CAKE\", \"ACKE\")\n</code></pre> <p>2</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#damerau-levenshtein-distance","title":"Damerau-Levenshtein Distance","text":"<p>At a glance</p> <p>Useful for: Data entry errors e.g. character transpositions and miskeys Splink comparison functions: damerau_levenshtein_level() and damerau_levenshtein_at_thresholds() Returns: An integer (lower is more similar).</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#description_1","title":"Description","text":"<p>Damerau-Levenshtein distance is a variation of Levenshtein distance that also includes transposition operations, which are the interchange of adjacent characters. This distance measures the minimum number of operations required to transform one string into another by allowing insertions, deletions, substitutions, and transpositions of characters.</p> <p>Or, as a formula,</p> \\[\\textsf{DamerauLevenshtein}(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion , } \\text{deletion , }  \\text{substitution , } \\text{transposition} \\end{array} \\rbrace \\]","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#examples_1","title":"Examples","text":"\"KITTEN\" vs \"SITTING\" <p>The minimum number of operations to convert \"KITTEN\" into \"SITTING\" are:</p> <ul> <li>Substitute \"K\" in \"KITTEN\" with \"S\" to get \"SITTEN\".</li> <li>Substitute \"E\" in \"SITTEN\" with \"I\" to get \"SITTIN\".</li> <li>Insert \"G\" after \"T\" in \"SITTIN\" to get \"SITTING\".</li> </ul> <p>Therefore, </p> \\[\\textsf{DamerauLevenshtein}(\\texttt{KITTEN}, \\texttt{SITTING}) = 3\\] \"CAKE\" vs \"ACKE\" <p>The minimum number of operations to convert \"CAKE\" into \"ACKE\" are:</p> <ul> <li>Transpose \"C\" and \"A\" in \"CAKE\" with \"A\" to get \"ACKE.\"</li> </ul> <p>Therefore, </p> \\[\\textsf{DamerauLevenshtein}(\\texttt{CAKE}, \\texttt{ACKE}) = 1\\]","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#sample-code_1","title":"Sample code","text":"<p>You can test out the Damerau-Levenshtein distance between two strings through the jellyfish package.</p> <pre><code>import jellyfish\ndamerau_levenshtein_distance(\"CAKE\", \"ACKE\")\n</code></pre> <p>1</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#jaro-similarity","title":"Jaro Similarity","text":"<p>At a glance</p> <p>Useful for:  Strings where all characters are considered equally important, regardless of order e.g. ID numbers Splink comparison functions: jaro_level() and jaro_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#description_2","title":"Description","text":"<p>Jaro similarity is a measure of similarity between two strings. It takes into account the number and order of matching characters, as well as the number of transpositions needed to make the strings identical.</p> <p>Jaro similarity considers:</p> <ul> <li>The number of matching characters (characters in the same position in both strings).</li> <li>The number of transpositions (pairs of characters that are not in the same position in both strings).</li> </ul> <p>Or, as a formula:</p> \\[\\textsf{Jaro}(s_1, s_2) = \\frac{1}{3} \\left[ \\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m} \\right]\\] <p>where:</p> <ul> <li>\\(s_1\\) and \\(s_2\\) are the two strings being compared</li> <li>\\(m\\) is the number of common characters (which are considered matching only if they are the same and not farther than \\(\\left\\lfloor \\frac{\\min(|s_1|,|s_2|)}{2} \\right\\rfloor - 1\\) characters apart)</li> <li>\\(t\\) is the number of transpositions (which is calculated as the number of matching characters that are not in the right order divided by two).</li> </ul>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#examples_2","title":"Examples","text":"\"MARTHA\" vs \"MARHTA\": <ul> <li>There are four matching characters: \"M\", \"A\", \"R\", and \"T\".</li> <li>There is one transposition: the fifth character in \"MARTHA\" (\"H\") is not in the same position as the fifth character in \"MARHTA\" (\"T\").</li> <li>We calculate the Jaro similarity using the formula:</li> </ul> \\[\\textsf{Jaro}(\\texttt{MARTHA}, \\texttt{MARHTA}) = \\frac{1}{3} \\left[ \\frac{4}{6} + \\frac{4}{6} + \\frac{4-1}{4} \\right] = 0.944\\] \"MARTHA\" vs \"AMRTHA\": <ul> <li>There are four matching characters: \"M\", \"A\", \"R\", and \"T\".</li> <li>There is one transposition: the first character in \"MARTHA\" (\"M\") is not in the same position as the first character in \"AMRTHA\" (\"T\").</li> <li>We calculate the Jaro similarity using the formula:</li> </ul> \\[\\textsf{Jaro}(\\texttt{MARTHA}, \\texttt{AMRTHA}) = \\frac{1}{3} \\left[ \\frac{4}{6} + \\frac{4}{6} + \\frac{4-1}{4} \\right] = 0.944\\] <p>Noting that transpositions of strings gives the same Jaro similarity regardless of order.</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#sample-code_2","title":"Sample code","text":"<p>You can test out the Jaro similarity between two strings through the jellyfish package.</p> <pre><code>import jellyfish\njellyfish.jaro_similarity(\"MARTHA\", \"AMRTHA\")\n</code></pre> <p>0.944</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#jaro-winkler-similarity","title":"Jaro-Winkler Similarity","text":"<p>At a glance</p> <p>Useful for: Strings where importance is weighted towards the first 4 characters e.g. Names Splink comparison functions: jaro_winkler_level() and jaro_winkler_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#description_3","title":"Description","text":"<p>Jaro-Winkler similarity is a variation of Jaro similarity that gives extra weight to matching prefixes of the strings. It is particularly useful for names</p> <p>The Jaro-Winkler similarity is calculated as follows:</p> \\[\\textsf{JaroWinkler}(s_1, s_2) = \\textsf{Jaro}(s_1, s_2) + p \\cdot l \\cdot (1 - \\textsf{Jaro}(s_1, s_2))\\] <p>where: - \\(\\textsf{Jaro}(s_1, s_2)\\) is the Jaro similarity between the two strings - \\(l\\) is the length of the common prefix between the two strings, up to a maximum of four characters - \\(p\\) is a prefix scale factor, commonly set to 0.1.</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#examples_3","title":"Examples","text":"\"MARTHA\" vs \"MARHTA\" <p>The common prefix between the two strings is \"MAR\", which has a length of 3. We calculate the Jaro-Winkler similarity using the formula:</p> \\[\\textsf{Jaro-Winkler}(\\texttt{MARTHA}, \\texttt{MARHTA}) = 0.944 + 0.1 \\cdot 3 \\cdot (1 - 0.944) = 0.9612\\] <p>The Jaro-Winkler similarity is slightly higher than the Jaro similarity, due to the matching prefix. </p> \"MARTHA\" vs \"AMRTHA\": <p>There is no common prefix, so the Jaro-Winkler similarity formula gives:</p> \\[\\textsf{Jaro-Winkler}(\\texttt{MARTHA}, \\texttt{MARHTA}) = 0.944 + 0.1 \\cdot 0 \\cdot (1 - 0.944) = 0.944\\] <p>Which is the same as the Jaro score.</p> <p>Note that the Jaro-Winkler similarity should be used with caution, as it may not always provide better results than the standard Jaro similarity, especially when dealing with short strings or strings that have no common prefix.</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#sample-code_3","title":"Sample code","text":"<p>You can test out the Jaro similarity between two strings through the jellyfish package.</p> <pre><code>import jellyfish\njellyfish.jaro_winkler_similarity(\"MARTHA\", \"MARHTA\")\n</code></pre> <p>0.9612</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#jaccard-similarity","title":"Jaccard Similarity","text":"<p>At a glance</p> <p>Useful for: Splink comparison functions: jaccard_level() and jaccard_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#description_4","title":"Description","text":"<p>Jaccard similarity is a measure of similarity between two sets of items, based on the size of their intersection (elements in common) and union (total elements across both sets). For strings, it considers the overlap of characters within each string. Mathematically, it can be represented as:</p> \\[\\textsf{Jaccard}=\\frac{|A \\cap B|}{|A \\cup B|}\\] <p>where A and B are two strings, and |A| and |B| represent their cardinalities (i.e., the number of elements in each set).</p> <p>In practice, Jaccard is more useful with strings that can be split up into multiple words as opposed to characters within a single word or string. E.g. tokens within addresses:</p> <p>Address 1: {\"flat\", \"2\", \"123\", \"high\", \"street\", \"london\", \"sw1\", \"1ab\"}</p> <p>Address 2: {\"2\", \"high\", \"street\", \"london\", \"sw1a\", \"1ab\"},</p> <p>where: </p> <ul> <li>there are 9 unique tokens across the addresses: \"flat\", \"2\", \"123\", \"high\", \"street\", \"london\", \"sw1\", \"sw1a\", \"1ab\"  </li> <li>there are 5 tokens found in both addresses: \"2\", \"high\", \"street\", \"london\", \"1ab\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[\\textsf{Jaccard}(\\textrm{Address1}, \\textrm{Address2})=\\frac{5}{9}=0.5556\\] <p>However, this functionality is not currently implemented within Splink</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#examples_4","title":"Examples","text":"\"DUCK\" vs \"LUCK\" <ul> <li>There are five unique characters across the strings: \"D\", \"U\", \"C\", \"K\", \"L\"</li> <li>Three are found in both strings: \"U\", \"C\", \"K\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[\\textsf{Jaccard}(\\texttt{DUCK}, \\texttt{LUCK})=\\frac{3}{5}=0.6\\] \"MARTHA\" vs \"MARHTA\" <ul> <li>There are five unique characters across the strings: \"M\", \"A\", \"R\", \"T\", \"H\"</li> <li>Five are found in both strings: \"M\", \"A\", \"R\", \"T\", \"H\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[\\textsf{Jaccard}(\\texttt{MARTHA}, \\texttt{MARHTA})=\\frac{5}{5}=1\\]","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparators.html#sample-code_4","title":"Sample code","text":"<p>You can test out the Jaccard similarity between two strings with the function below:</p> <pre><code>def jaccard_similarity(str1, str2):\n        set1 = set(str1)\n        set2 = set(str2)\n        return len(set1 &amp; set2) / len(set1 | set2)\n\njaccard_similarity(\"DUCK\", \"LUCK\")\n</code></pre> <p>0.6</p>","tags":["API","comparisons","Levenshtein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparisons/comparison_templates.html","title":"Out-of-the-box comparisons","text":"<pre><code>from splink.duckdb.comparison_template_library import date_comparison\n\ndate_of_birth_comparison = date_comparison(\"date_of_birth\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: Date of birth\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Up to one character difference\n\u251c\u2500-- ComparisonLevel: Dates within 1 month of each other\n\u251c\u2500-- ComparisonLevel: Dates within 1 year of each other\n\u251c\u2500-- ComparisonLevel: Dates within 10 years of each other\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>date_of_birth_comparison</code>:</p> <pre><code>print(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 10 years' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 10\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The date_comparison function also allows the user flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>date_of_birth_comparison = date_comparison(\n    \"date_of_birth\",\n    levenshtein_thresholds=[2],\n    damerau_levenshtein_thresholds=[],\n    datediff_thresholds=[7, 1, 1],\n    datediff_metrics=[\"day\", \"month\", \"year\"],\n)\nprint(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within levenshtein threshold 2 vs. Dates within the following thresholds Day(s): 7, Month(s): 1, Year(s): 1 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2\n    - 'Within 7 days' with SQL rule: \n            abs(date_diff('day', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 7\n\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>To see this as a specifications dictionary you can call</p> <pre><code>date_of_birth_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'date_of_birth',\n 'comparison_levels': [{'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': '\\n            abs(date_diff(\\'day\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 7\\n        ',\n   'label_for_charts': 'Within 7 days'},\n  {'sql_condition': '\\n            abs(date_diff(\\'month\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 1\\n        ',\n   'label_for_charts': 'Within 1 month'},\n  {'sql_condition': '\\n            abs(date_diff(\\'year\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 1\\n        ',\n   'label_for_charts': 'Within 1 year'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein threshold 2 vs. Dates within the following thresholds Day(s): 7, Month(s): 1, Year(s): 1 vs. anything else'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.comparison_template_library import name_comparison\n\nfirst_name_comparison = name_comparison(\"first_name\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: First Name\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Up to one character difference\n\u251c\u2500-- ComparisonLevel: First Names with Jaro-Winkler similarity of 0.9 or greater \n\u251c\u2500-- ComparisonLevel: First Names with Jaro-Winkler similarity of 0.8 or greater\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>first_name_comparison</code>:</p> <pre><code>print(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 1\n    - 'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9\n    - 'Jaro_winkler_similarity &gt;= 0.8' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The name_comparison function also allows flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>surname_comparison = name_comparison(\n    \"surname\",\n    phonetic_col_name=\"surname_dm\",\n    term_frequency_adjustments=True,\n    levenshtein_thresholds=[2],\n    damerau_levenshtein_thresholds=[],\n    jaro_winkler_thresholds=[],\n    jaccard_thresholds=[1],\n)\nprint(surname_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Names with phonetic exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. anything else' of \"surname\" and \"surname_dm\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"surname_l\" IS NULL OR \"surname_r\" IS NULL\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match surname_dm' with SQL rule: \"surname_dm_l\" = \"surname_dm_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") &lt;= 2\n    - 'Jaccard &gt;= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") &gt;= 1\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Where <code>surname_dm</code> refers to a column which has used the DoubleMetaphone algorithm on <code>surname</code> to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide.</p> <p>To see this as a specifications dictionary you can call</p> <pre><code>surname_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_surname_surname_dm',\n 'comparison_levels': [{'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Exact match surname',\n   'tf_adjustment_column': 'surname',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"surname_dm_l\" = \"surname_dm_r\"',\n   'label_for_charts': 'Exact match surname_dm',\n   'tf_adjustment_column': 'surname_dm',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard &gt;= 1'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. anything else'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.comparison_template_library import forename_surname_comparison\n\nname_comparison = forename_surname_comparison(\"forename\", \"surname\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: First Name\n\u251c\u2500-- ComparisonLevel: Exact match Forename and Surname\n\u251c\u2500-- ComparisonLevel: Exact match Forename and Surname swapped\n\u251c\u2500-- ComparisonLevel: Exact match Surname\n\u251c\u2500-- ComparisonLevel: Exact match Forename\n\u251c\u2500-- ComparisonLevel: Surnames with Jaro-Winkler similarity greater than 0.88\n\u251c\u2500-- ComparisonLevel: Forenames with Jaro-Winkler similarity greater than 0.88\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>first_name_comparison</code>:</p> <pre><code>print(name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within jaro-winkler threshold 0.88 vs. Forename within jaro-winkler threshold 0.88 vs. anything else' of \"surname\" and \"forename\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: (\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)\n    - 'Full name exact match' with SQL rule: forename_l = forename_r AND surname_l = surname_r\n    - 'Exact match on reversed cols' with SQL rule: \"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match forename' with SQL rule: \"forename_l\" = \"forename_r\"\n    - 'Jaro_winkler_similarity surname &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.88\n    - 'Jaro_winkler_similarity forename &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"forename_l\", \"forename_r\") &gt;= 0.88\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The forename_surname_comparison function also allows flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>full_name_comparison = forename_surname_comparison(\n    \"forename\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n    phonetic_forename_col_name=\"forename_dm\",\n    phonetic_surname_col_name=\"surname_dm\",\n    levenshtein_thresholds=[2],\n    jaro_winkler_thresholds=[],\n    jaccard_thresholds=[1],\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Phonetic match forename and surname vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. Forename within levenshtein threshold 2 vs. Forename within jaccard threshold 1 vs. anything else' of \"surname\", \"forename\", \"surname_dm\" and \"forename_dm\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: (\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)\n    - 'Full name exact match' with SQL rule: forename_l = forename_r AND surname_l = surname_r\n    - 'Full name phonetic match' with SQL rule: forename_dm_l = forename_dm_r AND surname_dm_l = surname_dm_r\n    - 'Exact match on reversed cols' with SQL rule: \"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match forename' with SQL rule: \"forename_l\" = \"forename_r\"\n    - 'Levenshtein surname &lt;= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") &lt;= 2\n    - 'Jaccard surname &gt;= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") &gt;= 1\n    - 'Levenshtein forename &lt;= 2' with SQL rule: levenshtein(\"forename_l\", \"forename_r\") &lt;= 2\n    - 'Jaccard forename &gt;= 1' with SQL rule: jaccard(\"forename_l\", \"forename_r\") &gt;= 1\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Where:</p> <ul> <li> <p><code>forename_dm</code> and <code>surname_dm</code> refer to columns which have used the DoubleMetaphone algorithm on <code>forename</code> and <code>surname</code> to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide. These columns will have to already exist in the dataset, or be created in the feature engineering stage when preparing datasets for linking.</p> </li> <li> <p><code>full_name</code> is a column containing <code>forename</code> and <code>surname</code> so that the model can consider the term-frequency of the full name, as well as <code>forename</code> and <code>surname</code> individually. These columns will have to already exist in the dataset, or be created in the feature engineering stage when preparing datasets for linking.</p> </li> </ul> <p>To see this as a specifications dictionary you can call</p> <pre><code>full_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_surname_forename_surname_dm_forename_dm',\n 'comparison_levels': [{'sql_condition': '(\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': 'forename_l = forename_r AND surname_l = surname_r',\n   'label_for_charts': 'Full name exact match',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': 'forename_dm_l = forename_dm_r AND surname_dm_l = surname_dm_r',\n   'label_for_charts': 'Full name phonetic match',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"',\n   'label_for_charts': 'Exact match on reversed cols',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Exact match surname',\n   'tf_adjustment_column': 'surname',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"forename_l\" = \"forename_r\"',\n   'label_for_charts': 'Exact match forename',\n   'tf_adjustment_column': 'forename',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein surname &lt;= 2'},\n  {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard surname &gt;= 1'},\n  {'sql_condition': 'levenshtein(\"forename_l\", \"forename_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein forename &lt;= 2'},\n  {'sql_condition': 'jaccard(\"forename_l\", \"forename_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard forename &gt;= 1'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Phonetic match forename and surname vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. Forename within levenshtein threshold 2 vs. Forename within jaccard threshold 1 vs. anything else'}</code>\n</pre> <p>Which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.comparison_template_library import postcode_comparison\n\npc_comparison = postcode_comparison(\"postcode\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: Postcode\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Exact match on sector\n\u251c\u2500-- ComparisonLevel: Exact match on district\n\u251c\u2500-- ComparisonLevel: Exact match on area\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>pc_comparison</code>:</p> <pre><code>print(pc_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. all other comparisons' of \"postcode\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"postcode_l\" IS NULL OR \"postcode_r\" IS NULL\n    - 'Exact match postcode' with SQL rule: lower(\"postcode_l\") = lower(\"postcode_r\")\n    - 'Exact match Postcode Sector' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]')\n\n    - 'Exact match Postcode District' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?')\n\n    - 'Exact match Postcode Area' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}')\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>where individual postcode components are extracted under-the-hood using the <code>regex_extract</code> argument.</p> <p>Note that the 'Exact match Postcode District' level also captures matches on subdistricts where they exist in the data.</p> <p>Performing comparisons based on substrings alone doesn't always give the best sense of whether two postcodes are close together since locations which are geographically close can be in different postcode regions e.g. London postcodes starting 'N' vs 'SW'. Given this, the postcode_comparison function also allows the user flexibility to include cll.distance_in_km_level() by supplying <code>lat_col</code>, <code>long_col</code> and <code>km_thresholds</code> arguments. This can help to improve results. (See Feature Enginnering for more details.)</p> <p>Users also have the option to set <code>invalid_postcodes_as_null</code> to <code>True</code>. If <code>True</code>, postcodes that do not adhere to a valid postcode format as determined by <code>valid_postcode_regex</code> will be included in the null level. <code>valid_postcode_regex</code> defaults to <code>\"^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\"</code>.</p> <p>For example:</p> <pre><code>pc_comparison = postcode_comparison(\n    \"postcode\",\n    invalid_postcodes_as_null=True,\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons' of \"postcode\", \"long\" and \"lat\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$')\n     IS NULL OR \n        regexp_extract(\"postcode_r\", '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$')\n     IS NULL OR\n\n        regexp_extract(\"postcode_l\", '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$')\n    =='' OR \n        regexp_extract(\"postcode_r\", '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$')\n     ==''\n    - 'Exact match postcode' with SQL rule: lower(\"postcode_l\") = lower(\"postcode_r\")\n    - 'Exact match Postcode Sector' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]')\n\n    - 'Exact match Postcode District' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?')\n\n    - 'Exact match Postcode Area' with SQL rule: \n        regexp_extract(lower(\"postcode_l\"), '^[A-Za-z]{1,2}')\n     = \n        regexp_extract(lower(\"postcode_r\"), '^[A-Za-z]{1,2}')\n\n    - 'Distance less than 1km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 1\n\n    - 'Distance less than 10km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 10\n\n    - 'Distance less than 50km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 50\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>To see this as a specifications dictionary you can call</p> <pre><code>pc_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'postcode',\n 'comparison_levels': [{'sql_condition': '\\n        regexp_extract(\"postcode_l\", \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\\')\\n     IS NULL OR \\n        regexp_extract(\"postcode_r\", \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\\')\\n     IS NULL OR\\n                      \\n        regexp_extract(\"postcode_l\", \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\\')\\n    ==\\'\\' OR \\n        regexp_extract(\"postcode_r\", \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$\\')\\n     ==\\'\\'',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': 'lower(\"postcode_l\") = lower(\"postcode_r\")',\n   'label_for_charts': 'Exact match postcode'},\n  {'sql_condition': '\\n        regexp_extract(lower(\"postcode_l\"), \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]\\')\\n     = \\n        regexp_extract(lower(\"postcode_r\"), \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9]\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode Sector'},\n  {'sql_condition': '\\n        regexp_extract(lower(\"postcode_l\"), \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?\\')\\n     = \\n        regexp_extract(lower(\"postcode_r\"), \\'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]?\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode District'},\n  {'sql_condition': '\\n        regexp_extract(lower(\"postcode_l\"), \\'^[A-Za-z]{1,2}\\')\\n     = \\n        regexp_extract(lower(\"postcode_r\"), \\'^[A-Za-z]{1,2}\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode Area'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 1\\n        ',\n   'label_for_charts': 'Distance less than 1km'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 10\\n        ',\n   'label_for_charts': 'Distance less than 10km'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 50\\n        ',\n   'label_for_charts': 'Distance less than 50km'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.comparison_template_library import email_comparison\n\nstandard_email_comparison = email_comparison(\"email\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: Email\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Exact match on username with different domain\n\u251c\u2500-- ComparisonLevel: Fuzzy match on email using Jaro-Winkler\n\u251c\u2500-- ComparisonLevel: Fuzzy match on username using Jaro-Winkler\n\u251c\u2500-- ComparisonLevel: All other comparisons\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>email_comparison</code>:</p> <pre><code>print(standard_email_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Exact username match different domain vs. Fuzzy Email within jaro_winkler threshold 0.88 vs. Fuzzy Username within jaro_winkler threshold 0.88 vs. anything else' of \"email\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"email_l\" IS NULL OR \"email_r\" IS NULL\n    - 'Exact match email' with SQL rule: \"email_l\" = \"email_r\"\n    - 'Exact match email' with SQL rule: \n        regexp_extract(\"email_l\", '^[^@]+')\n     = \n        regexp_extract(\"email_r\", '^[^@]+')\n\n    - 'Jaro_winkler_similarity email &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"email_l\", \"email_r\") &gt;= 0.88\n    - 'Jaro_winkler_similarity Username &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\n        regexp_extract(\"email_l\", '^[^@]+')\n    , \n        regexp_extract(\"email_r\", '^[^@]+')\n    ) &gt;= 0.88\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>where individual email components are extracted under-the-hood using the <code>regex_extract</code> argument.</p> <p>By default, the fuzzy matching is done using Jaro-Winkler thresholds. This will bias the start of a string, specifically the first four characters, which may not be appropriate for all emails. The <code>email_comparison</code> function is flexible and allows a number of other string fuzzy matching functions.</p> <p>Users also have the option to set <code>invalid_emails_as_null</code> to <code>True</code>. If <code>True</code>, postcodes that do not adhere to a valid email format as determined by <code>valid_email_regex</code> will be included in the null level. <code>valid_email_regex</code> defaults to <code>\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\"</code>.</p> <p>For example:</p> <pre><code>bespoke_email_comparison = email_comparison(\n    \"email\",\n    jaro_winkler_thresholds=[],\n    jaro_thresholds=[0.8],\n    include_username_match_level=False,\n    include_domain_match_level=True,\n    invalid_emails_as_null=True,\n)\nprint(bespoke_email_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Domain-only match vs.anything else' of \"email\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \n        regexp_extract(\"email_l\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n     IS NULL OR \n        regexp_extract(\"email_r\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n     IS NULL OR\n\n        regexp_extract(\"email_l\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n    =='' OR \n        regexp_extract(\"email_r\", '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$')\n     ==''\n    - 'Exact match email' with SQL rule: \"email_l\" = \"email_r\"\n    - 'Jaro_similarity email &gt;= 0.8' with SQL rule: jaro_similarity(\"email_l\", \"email_r\") &gt;= 0.8\n    - 'Jaro_similarity email &gt;= 0.8' with SQL rule: jaro_similarity(\"email_l\", \"email_r\") &gt;= 0.8\n    - 'Exact match Email Domain' with SQL rule: \n        regexp_extract(\"email_l\", '@([^@]+)$')\n     = \n        regexp_extract(\"email_r\", '@([^@]+)$')\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>To see this as a specifications dictionary you can call</p> <pre><code>bespoke_email_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'email',\n 'comparison_levels': [{'sql_condition': '\\n        regexp_extract(\"email_l\", \\'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\\')\\n     IS NULL OR \\n        regexp_extract(\"email_r\", \\'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\\')\\n     IS NULL OR\\n                      \\n        regexp_extract(\"email_l\", \\'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\\')\\n    ==\\'\\' OR \\n        regexp_extract(\"email_r\", \\'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}$\\')\\n     ==\\'\\'',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"email_l\" = \"email_r\"',\n   'label_for_charts': 'Exact match email'},\n  {'sql_condition': 'jaro_similarity(\"email_l\", \"email_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_similarity email &gt;= 0.8'},\n  {'sql_condition': 'jaro_similarity(\"email_l\", \"email_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_similarity email &gt;= 0.8'},\n  {'sql_condition': '\\n        regexp_extract(\"email_l\", \\'@([^@]+)$\\')\\n     = \\n        regexp_extract(\"email_r\", \\'@([^@]+)$\\')\\n    ',\n   'label_for_charts': 'Exact match Email Domain'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Domain-only match vs.anything else'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#out-of-the-box-comparisons-for-specific-data-types","title":"Out-of-the-box Comparisons for specific data types","text":"<p>Similarity is defined differently for types of data (e.g. names, dates of birth, postcodes, addresses, ids). The Comparison Template Library contains functions to generate ready-made comparisons for a variety of data types.</p> <p>Below are examples of how to structure comparisons for a variety of data types.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#date-comparisons","title":"Date Comparisons","text":"<p>Date comparisons are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match  </li> <li>Fuzzy match (using metric of choice)  </li> <li>Interval match (within X days/months/years)  </li> <li>Else level</li> </ul> <p>The comparison_template_library contains the date_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#name-comparisons","title":"Name Comparisons","text":"<p>Name comparisons for an individual name column (e.g. forename, surname) are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match  </li> <li>Fuzzy match (using metric of choice)  </li> <li>Else level</li> </ul> <p>The comparison_template_library contains the name_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#forename-and-surname-comparisons","title":"Forename and Surname Comparisons","text":"<p>It can be helpful to construct a single comparison for for comparing the forename and surname of two records as:</p> <ol> <li> <p>The Fellegi-Sunter model assumes that comparisons are independent. We know that forename and surname are usually correlated given the regional variation of names etc, so considering then in a single comparison can help to create better models.</p> </li> <li> <p>Term-frequencies of individual forename and surname individually does not necessarily reflect how common the combination of forename and surname are.  For example, in the UK population \u201cMohammed Khan\u201d is a relatively common full name despite neither name occurring frequently. For more information on term-frequencies, see the dedicated topic guide. Addressing forename and surname in a single comparison can allows the model to consider the joint term-frequency as well as individual.</p> </li> <li> <p>It is common for some records to have swapped forename and surname by mistake. Addressing forename and surname in a single comparison can allows the model to consider these name inversions.</p> </li> </ol> <p>Forename and Surname comparisons for an individual name column (e.g. forename, surname) are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match Forename and Surname</li> <li>Exact match Forename and Surname swapped</li> <li>Exact match Surname</li> <li>Exact match Forename</li> <li>Fuzzy match Surname (using metric of choice)</li> <li>Fuzzy match Forename (using metric of choice)</li> <li>Else level</li> </ul> <p>The comparison_template_library contains the forename_surname_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#postcode-comparisons","title":"Postcode Comparisons","text":"<p>The comparison_template_library contains the postcode_comparison function which provides a sensible approach to comparing postcodes in terms of their constituent components, out-of-the-box. See Feature Engineering for more details.</p>"},{"location":"topic_guides/comparisons/comparison_templates.html#email-comparisons","title":"Email Comparisons","text":"<p>Email comparisons are generally structured as:</p> <ul> <li>Null Level</li> <li>Exact match on email address</li> <li>Exact match on username</li> <li>Fuzzy match on email address</li> <li>Fuzzy match on username</li> <li>All other comparisons</li> </ul> <p>The comparison_template_library contains the email_comparison function which provides a sensible approach to comparing emails out-of-the-box.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html","title":"Defining and customising comparisons","text":"<pre><code>import splink.duckdb.comparison_library as cl\n\nfirst_name_comparison = cl.exact_match(\"first_name\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Note that, under the hood, these functions generate a Python dictionary, which conforms to the underlying <code>.json</code> specification of a model:</p> <pre><code>first_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'first_name',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. anything else'}</code>\n</pre> <p>We can now generate a second, more complex comparison:</p> <pre><code>import splink.duckdb.comparison_library as cl\n\ndob_comparison = cl.levenshtein_at_thresholds(\"dob\", [1, 2])\nprint(dob_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Dob within levenshtein thresholds 1, 2 vs. anything else' of \"dob\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"dob_l\" IS NULL OR \"dob_r\" IS NULL\n    - 'Exact match' with SQL rule: \"dob_l\" = \"dob_r\"\n    - 'Levenshtein &lt;= 1' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") &lt;= 1\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") &lt;= 2\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>These <code>Comparisons</code> can be specified in a data linking model as follows:</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        exact_match(\"first_name\"),\n        levenshtein_at_thresholds(\"dob\", [1, 2]),\n    ],\n}\n</code></pre> <pre><code>import splink.duckdb.comparison_template_library as ctl\n\ndate_of_birth_comparison = ctl.date_comparison(\"date_of_birth\")\nprint(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 10 years' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 10\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>These <code>Comparisons</code> can be specified in a data linking model as follows:</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        exact_match(\"first_name\"),\n        date_comparison(\"dob\"),\n    ],\n}\n</code></pre> <p>You can customise a <code>ComparisonTemplate</code> by choosing your own values for the function parameters, but for anything more bespoke you will want to construct a <code>Comparison</code> with <code>ComparisonLevels</code> or provide the spec as a dictionary.</p> <p>For a deep dive on Comparison Templates, see the dedicated topic guide.</p> <pre><code>import splink.spark.comparison_level_library as cll\n\ncomparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        cll.null_level(\"first_name\"),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments=True),\n        cll.exact_match_level(\"dmeta_first_name\", term_frequency_adjustments=True),\n        cll.else_level(),\n    ],\n}\n\n\nfrom splink.comparison import Comparison\n\nprint(Comparison(comparison_first_name).human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'First name jaro dmeta' of `first_name` and `dmeta_first_name`.\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: `first_name_l` IS NULL OR `first_name_r` IS NULL\n    - 'Exact match' with SQL rule: `first_name_l` = `first_name_r`\n    - 'Exact match' with SQL rule: `dmeta_first_name_l` = `dmeta_first_name_r`\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>This can now be specified in the settings dictionary as follows:</p> <pre><code>import splink.spark.comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        comparison_first_name,  # The comparison specified above using ComparisonLevels\n        cl.levenshtein_at_thresholds(\n            \"dob\", [1, 2], term_frequency_adjustments=True\n        ),  # From comparison_library\n    ],\n}\n</code></pre> <pre><code>comparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"dmeta_first_name_l = dmeta_first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"dmeta_first_name\",\n            \"tf_adjustment_weight\": 1.0,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &amp;gt; 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        comparison_first_name,  # The comparison specified above using the dict\n        cl.levenshtein_at_thresholds(\n            \"dob\", [1, 2], term_frequency_adjustments=True\n        ),  # From comparison_library\n    ],\n}\n</code></pre>"},{"location":"topic_guides/comparisons/customising_comparisons.html#defining-and-customising-how-record-comparisons-are-made","title":"Defining and customising how record comparisons are made","text":"<p>A key feature of Splink is the ability to customise how record comparisons are made - that is, how similarity is defined for different data types.  For example, the definition of similarity that is appropriate for a date of birth field is different than for a first name field.</p> <p>By tailoring the definitions of similarity, linking models are more effectively able to distinguish between different gradations of similarity, leading to more accurate data linking models.</p> <p>Note that for performance reasons, Splink requires the user to define <code>n</code> discrete levels (gradations) of similarity.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#comparing-information","title":"Comparing information","text":"<p>Comparisons are defined on pairwise record comparisons.  Suppose for instance your data contains <code>first_name</code> and <code>surname</code> and <code>dob</code>:</p> id first_name surname dob 1 john smith 1991-04-11 2 jon smith 1991-04-17 3 john smyth 1991-04-11 <p>To compare these records, at the blocking stage, Splink will set these records against each other in a table of pairwise record comparisons:</p> id_l id_r first_name_l first_name_r surname_l surname_r dob_l dob_r 1 2 john jon smith smith 1991-04-11 1991-04-17 1 3 john john smith smyth 1991-04-11 1991-04-11 2 3 jon john smith smyth 1991-04-17 1991-04-11 <p>When defining comparisons, we are defining rules that operate on each row of this latter table of pairwise comparisons</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#comparisons-comparisontemplates-and-comparisonlevels","title":"<code>Comparisons</code>, <code>ComparisonTemplates</code> and <code>ComparisonLevels</code>","text":"<p>A Splink model contains a collection of <code>Comparisons</code> and <code>ComparisonLevels</code> organised in a hierarchy.  An example is as follows:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: Up to one character difference\n\u2502    \u251c\u2500-- ComparisonLevel: Up to three character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> <p>A fuller description of <code>Comaprison</code>s and <code>ComparisonLevel</code>s can be found here and here respectively.</p> <p>How are these comparisons specified?</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#three-ways-of-specifying-comparisons","title":"Three ways of specifying Comparisons","text":"<p>In Splink, there are three ways of specifying <code>Comparisons</code>:</p> <ul> <li>Using pre-baked comparisons from a backend's <code>ComparisonLibrary</code> or <code>ComparisonTemplateLibrary</code>.   (Most simple/succinct)</li> <li>Composing pre-defined <code>ComparisonLevels</code> from a backend's <code>ComparisonLevelLibrary</code></li> <li>Writing a full spec of a <code>Comparison</code> by hand (most verbose/flexible)</li> </ul>"},{"location":"topic_guides/comparisons/customising_comparisons.html#method-1-using-the-comparisonlibrary","title":"Method 1: Using the <code>ComparisonLibrary</code>","text":"<p>The <code>ComparisonLibrary</code> for a each backend (<code>DuckDB</code>, <code>Spark</code>, etc.) contains pre-baked similarity functions that cover many common use cases.</p> <p>These functions generate an entire <code>Comparison</code>, composed of several <code>ComparisonLevels</code></p> <p>The following provides an example of using the <code>ComparisonLibrary</code> for DuckDB.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#method-2-using-the-comparisontemplatelibrary","title":"Method 2: Using the <code>ComparisonTemplateLibrary</code>","text":"<p>The <code>ComparisonTemplateLibrary</code> is very similar to <code>ComparisonLibrary</code> in that it contains pre-baked similarity functions for each backend (DuckDB, Spark, etc.) to cover common use cases.</p> <p>The key difference is that <code>ComparisonTemplateLibrary</code> contains functions to generate a 'best practice' <code>Comparison</code> based on the type of data in a given column. This includes: </p> <ul> <li>How comparison is structured (what comparison levels are included, and in what order) </li> <li>Default parameters (e.g. <code>damerau_levenshtein_thresholds = [1]</code>)</li> </ul> <p>The following provides an example of using the ComparisonTemplateLibrary for DuckDB.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#method-3-comparisonlevels","title":"Method 3: <code>ComparisonLevels</code>","text":"<p>The <code>ComparisonLevels</code> API provides a lower-level API that gives the user greater control over their comparisons.</p> <p>For example, the user may wish to specify a comparison that has levels for a match on dmetaphone and jaro_winkler of the <code>first_name</code> field.  </p> <p>The below example assumes the user has derived a column <code>dmeta_first_name</code> which contains the dmetaphone of the first name.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#method-4-providing-the-spec-as-a-dictionary","title":"Method 4: Providing the spec as a dictionary","text":"<p>Ultimately, comparisons are specified as a dictionary which conforms to the formal <code>jsonschema</code> specification of the settings dictionary and here.</p> <p>The library functions described above are convenience functions that provide a shorthand way to produce valid dictionaries.</p> <p>For maximium control over your settings, you can specify your comparisons as a dictionary.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#examples","title":"Examples","text":"<p>Below are some examples of how you can define the same comparison, but through different methods.</p> <p>Note: the following examples show working code for duckdb. In order to change to Where functions exist</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#exact-match-comparison-with-term-frequency-adjustments","title":"Exact match Comparison with Term-Frequency Adjustments","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.comparison_library as cl\n\nfirst_name_comparison = cl.exact_match(\"first_name\", term_frequency_adjustments=True)\n</code></pre> <pre><code>import splink.duckdb.comparison_level_library as cll\n\nfirst_name_comparison = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"Exact match vs. anything else\",\n    \"comparison_levels\": [\n        cll.null_level(\"first_name\"),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments=True),\n        cll.else_level(),\n    ],\n}\n</code></pre> <pre><code>first_name_comparison = {\n    'output_column_name': 'first_name',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n            'label_for_charts': 'Exact match',\n            'tf_adjustment_column': 'first_name',\n            'tf_adjustment_weight': 1.0\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <p></p><pre><code>{\n    'output_column_name': 'first_name',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n            'label_for_charts': 'Exact match',\n            'tf_adjustment_column': 'first_name',\n            'tf_adjustment_weight': 1.0\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. anything else'\n}\n</code></pre> in your settings dictionary."},{"location":"topic_guides/comparisons/customising_comparisons.html#name-comparison","title":"Name Comparison","text":"Example Comparison Template LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nsurname_comparison = ctl.name_comparison(\"surname\")\n</code></pre> <pre><code>import splink.duckdb.comparison_level_library as cll\n\n    surname_comparison = {\n        \"output_column_name\": \"surname\",\n        \"comparison_description\": \"Exact match vs. Surname within jaro_winkler thresholds 0.95, 0.88 vs. anything else\",\n        \"comparison_levels\": [\n            cll.null_level(\"surname\"),\n            cll.exact_match_level(\"surname\"),\n            cll.damerau_levenshtein_level(\"surname\", 1)\n            cll.jaro_winkler_level(\"surname\", 0.9),\n            cll.jaro_winkler_level(\"surname\", 0.8),\n            cll.else_level(),\n        ],\n    }\n</code></pre> <pre><code>surname_comparison = {\n    'output_column_name': 'surname',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"surname_l\" = \"surname_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &amp;lt;= 1',\n            'label_for_charts': 'Damerau_levenshtein &amp;lt;= 1'\n        },\n        {\n            'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.9',\n            'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.9'\n        },\n        {\n            'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.8',\n            'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.8'\n        },\n        {\n            'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n        }],\n         'comparison_description': 'Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else'\n        }\n</code></pre> <p>Each of which gives</p> <p></p><pre><code>{\n        'output_column_name': 'surname',\n        'comparison_levels': [\n            {\n                'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n                'label_for_charts': 'Null',\n                'is_null_level': True\n            },\n            {\n                'sql_condition': '\"surname_l\" = \"surname_r\"',\n                'label_for_charts': 'Exact match'\n            },\n            {\n                'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &amp;lt;= 1',\n                'label_for_charts': 'Damerau_levenshtein &amp;lt;= 1'\n            },\n            {\n                'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.9',\n                'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.9'\n            },\n            {\n                'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &amp;gt;= 0.8',\n                'label_for_charts': 'Jaro_winkler_similarity &amp;gt;= 0.8'\n            },\n            {\n                'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n            }],\n             'comparison_description': 'Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else'\n}\n</code></pre> in your settings dictionary."},{"location":"topic_guides/comparisons/customising_comparisons.html#levenshtein-comparison","title":"Levenshtein Comparison","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.comparison_library as cl\n\nemail_comparison = cl.levenshtein_at_thresholds(\"email\", [2, 4])\n</code></pre> <pre><code>import splink.duckdb.comparison_level_library as cll\n\nemail_comparison = {\n    \"output_column_name\": \"email\",\n    \"comparison_description\": \"Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else\",\n    \"comparison_levels\": [\n        cll.null_level(\"email\"),\n        cll.exact_match_level(\"surname\"),\n        cll.levenshtein_level(\"surname\", 2),\n        cll.levenshtein_level(\"surname\", 4),\n        cll.else_level(),\n    ],\n}\n</code></pre> <pre><code>email_comparison = {\n    'output_column_name': 'email',\n    'comparison_levels': [{'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n    'label_for_charts': 'Null',\n    'is_null_level': True},\n    {\n        'sql_condition': '\"email_l\" = \"email_r\"',\n        'label_for_charts': 'Exact match'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &amp;lt;= 2',\n        'label_for_charts': 'Levenshtein &amp;lt;= 2'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &amp;lt;= 4',\n        'label_for_charts': 'Levenshtein &amp;lt;= 4'\n    },\n    {\n        'sql_condition': 'ELSE', \n        'label_for_charts': 'All other comparisons'\n    }],\n    'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'}\n</code></pre> <p>Each of which gives</p> <pre><code>{\n    'output_column_name': 'email',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True},\n        {\n            'sql_condition': '\"email_l\" = \"email_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &amp;lt;= 2',\n            'label_for_charts': 'Levenshtein &amp;lt;= 2'\n        },\n        {\n            'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &amp;lt;= 4',\n            'label_for_charts': 'Levenshtein &amp;lt;= 4'\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'\n}\n</code></pre> <p>in your settings dictionary.</p>"},{"location":"topic_guides/comparisons/customising_comparisons.html#date-comparison","title":"Date Comparison","text":"Example Comparison Template LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.comparison_template_library as ctl\n\ndob_comparison = ctl.date_comparison(\"date_of_birth\")\n</code></pre> <pre><code>import splink.duckdb.comparison_level_library as cll\n\ndob_comparison = {\n            \"output_column_name\": \"date_of_birth\",\n            \"comparison_description\": \"Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else\",\n            \"comparison_levels\": [\n                cll.null_level(\"date_of_birth\"),\n                cll.exact_match_level(\"date_of_birth\"),\n                cll.levenshtein_level(\"date_of_birth\", 1),\n                cll.levenshtein_level(\"date_of_birth\", 2),\n                cll.datediff_level(\"date_of_birth\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"),\n                cll.datediff_level(\"date_of_birth\",\n                                    date_threshold=10,\n                                    date_metric=\"year\"),\n                cll.else_level(),\n            ],\n        }\n</code></pre> <pre><code>dob_comparison = {\n    'output_column_name': 'date_of_birth',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &amp;lt;= 1',\n        'label_for_charts': 'Levenshtein &amp;lt;= 1'\n        },\n        {\n            'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &amp;lt;= 2',\n        'label_for_charts': 'Levenshtein &amp;lt;= 2'\n        },\n        {\n            'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &amp;lt;= 1',\n            'label_for_charts': 'Within 1 year'\n        },\n        {\n            'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &amp;lt;= 10',\n            'label_for_charts': 'Within 10 years'},\n        {\n            'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <p></p><pre><code>{\n        'output_column_name': 'date_of_birth',\n        'comparison_levels': [\n            {\n                'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n                'label_for_charts': 'Null',\n                'is_null_level': True\n            },\n            {\n                'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n                'label_for_charts': 'Exact match'\n            },\n            {\n                'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &amp;lt;= 1',\n            'label_for_charts': 'Levenshtein &amp;lt;= 1'\n            },\n            {\n                'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &amp;lt;= 2',\n            'label_for_charts': 'Levenshtein &amp;lt;= 2'\n            },\n            {\n                'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &amp;lt;= 1',\n                'label_for_charts': 'Within 1 year'\n            },\n            {\n                'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &amp;lt;= 10',\n                'label_for_charts': 'Within 10 years'},\n            {\n                'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n            }],\n        'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else'\n    }\n</code></pre> in your settings dictionary."},{"location":"topic_guides/comparisons/customising_comparisons.html#km-distance-between-coordinates","title":"KM Distance between coordinates","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.comparison_library as cl\n\ndistance_comparison = cl.distance_in_km_at_thresholds(\"lat_col\",\n                        \"long_col\",\n                        km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>distance_comparison = {\n                \"output_column_name\": \"custom_lat_col_long_col\",\n                \"comparison_description\": \"Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else\",\n                \"comparison_levels\": [\n                    cll.or_(\n                        cll.null_level(\"lat_col\"),\n                        cll.null_level(\"long_col\"),\n                        ),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=0.1),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=1),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=10),\n                    cll.else_level(),\n                ],\n            }\n</code></pre> <pre><code>distance_comparison = {\n    'output_column_name': 'custom_lat_col_long_col',\n    'comparison_levels': [\n        {\n            'sql_condition': '(lat_col_l IS NULL OR lat_col_r IS NULL) \\nOR (long_col_l IS NULL OR long_col_r IS NULL)',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 0.1',\n            'label_for_charts': 'Distance less than 0.1km'\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 1',\n            'label_for_charts': 'Distance less than 1km'\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when ( sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 10',\n            'label_for_charts': 'Distance less than 10km'\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <pre><code>    {\n        'output_column_name': 'custom_lat_col_long_col',\n        'comparison_levels': [\n            {\n                'sql_condition': '(lat_col_l IS NULL OR lat_col_r IS NULL) \\nOR (long_col_l IS NULL OR long_col_r IS NULL)',\n                'label_for_charts': 'Null',\n                'is_null_level': True\n            },\n            {\n                'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 0.1',\n                'label_for_charts': 'Distance less than 0.1km'\n            },\n            {\n                'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 1',\n                'label_for_charts': 'Distance less than 1km'\n            },\n            {\n                'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;gt; 1 then 1 when ( sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &amp;lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&amp;lt;= 10',\n                'label_for_charts': 'Distance less than 10km'\n            },\n            {\n                'sql_condition': 'ELSE', \n                'label_for_charts': 'All other comparisons'\n            }],\n        'comparison_description': 'Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else'\n    }\n</code></pre>"},{"location":"topic_guides/comparisons/phonetic.html","title":"Phonetic transformations","text":"","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/phonetic.html#phonetic-transformation-algorithms","title":"Phonetic transformation algorithms","text":"<p>Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently (e.g. \"Stephen\" vs \"Steven\"). These algorithms to give another type of fuzzy match and are often generated in the Feature Engineering step of record linkage.</p> <p>Once generated, phonetic matches can be used within comparisons &amp; comparison levels and blocking rules.</p> <p>E.g. For a comparison including a Double Metaphone phonetic match using the name_comparison function from the comparison template library:</p>  DuckDB Spark <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <p>Comparison 'Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"first_name\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Exact match first_name_dm' with SQL rule: \"first_name_dm_l\" = \"first_name_dm_r\"</li> <li>'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 1</li> <li>'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9</li> <li>'Jaro_winkler_similarity &gt;= 0.8' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/phonetic.html#algorithms","title":"Algorithms","text":"<p>Below are some examples of well known phonetic transformation algorithms.</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/phonetic.html#soundex","title":"Soundex","text":"<p>Soundex is a phonetic algorithm that assigns a code to words based on their sound. The Soundex algorithm works by converting a word into a four-character code, where the first character is the first letter of the word, and the next three characters are numerical codes representing the word's remaining consonants. Vowels and some consonants, such as H, W, and Y, are ignored.</p> Algorithm Steps <p>The Soundex algorithm works by following these steps:</p> <ol> <li> <p>Retain the first letter of the word and remove all other vowels and the letters \"H\", \"W\", and \"Y\".</p> </li> <li> <p>Replace each remaining consonant (excluding the first letter) with a numerical code as follows:</p> <ol> <li>B, F, P, and V are replaced with \"1\"</li> <li>C, G, J, K, Q, S, X, and Z are replaced with \"2\"</li> <li>D and T are replaced with \"3\"</li> <li>L is replaced with \"4\"</li> <li>M and N are replaced with \"5\"</li> <li>R is replaced with \"6\"</li> </ol> </li> <li> <p>Combine the first letter and the numerical codes to form a four-character code. If there are fewer than four characters, pad the code with zeros.</p> </li> </ol> Example <p>You can test out the Soundex transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.soundex(\"Smith\"), phonetics.soundex(\"Smyth\"))\n</code></pre> <p>S5030 S5030</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/phonetic.html#metaphone","title":"Metaphone","text":"<p>Metaphone is an improved version of the Soundex algorithm that was developed to handle a wider range of words and languages. The Metaphone algorithm assigns a code to a word based on its phonetic pronunciation, but it takes into account the sound of the entire word, rather than just its first letter and consonants. The Metaphone algorithm works by applying a set of rules to the word's pronunciation, such as converting the \"TH\" sound to a \"T\" sound, or removing silent letters. The resulting code is a variable-length string of letters that represents the word's pronunciation.</p> Algorithm Steps <p>The Metaphone algorithm works by following these steps:</p> <ol> <li> <p>Convert the word to uppercase and remove all non-alphabetic characters.</p> </li> <li> <p>Apply a set of pronunciation rules to the word, such as:</p> <ol> <li>Convert the letters \"C\" and \"K\" to \"K\"</li> <li>Convert the letters \"PH\" to \"F\"</li> <li>Convert the letters \"W\" and \"H\" to nothing if they are not at the beginning of the word</li> </ol> </li> <li> <p>Apply a set of replacement rules to the resulting word, such as:</p> <ol> <li>Replace the letter \"G\" with \"J\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"C\" with \"S\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"X\" with \"KS\"</li> </ol> </li> <li> <p>If the resulting word ends with \"S\", remove it.</p> </li> <li> <p>If the resulting word ends with \"ED\", \"ING\", or \"ES\", remove it.</p> </li> <li> <p>If the resulting word starts with \"KN\", \"GN\", \"PN\", \"AE\", \"WR\", or \"WH\", remove the first letter.</p> </li> <li> <p>If the resulting word starts with a vowel, retain the first letter.</p> </li> <li> <p>Retain the first four characters of the resulting word, or pad it with zeros if it has fewer than four characters.</p> </li> </ol> Example <p>You can test out the Metaphone transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.metaphone(\"Smith\"), phonetics.metaphone(\"Smyth\"))\n</code></pre> <p>SM0 SM0</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/phonetic.html#double-metaphone","title":"Double Metaphone","text":"<p>Double Metaphone is an extension of the Metaphone algorithm that generates two codes for each word, one for the primary pronunciation and one for an alternate pronunciation. The Double Metaphone algorithm is designed to handle a wide range of languages and dialects, and it is more accurate than the original Metaphone algorithm.</p> <p>The Double Metaphone algorithm works by applying a set of rules to the word's pronunciation, similar to the Metaphone algorithm, but it generates two codes for each word. The primary code is the most likely pronunciation of the word, while the alternate code represents a less common pronunciation.</p> Algorithm Steps Standard Double MetaphoneAlternative Double Metaphone <p>The Double Metaphone algorithm works by following these steps:</p> <ol> <li> <p>Convert the word to uppercase and remove all non-alphabetic characters.</p> </li> <li> <p>Apply a set of pronunciation rules to the word, such as:</p> <ol> <li>Convert the letters \"C\" and \"K\" to \"K\"</li> <li>Convert the letters \"PH\" to \"F\"</li> <li>Convert the letters \"W\" and \"H\" to nothing if they are not at the beginning of the word</li> </ol> </li> <li> <p>Apply a set of replacement rules to the resulting word, such as:</p> <ol> <li>Replace the letter \"G\" with \"J\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"C\" with \"S\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"X\" with \"KS\"</li> </ol> </li> <li> <p>If the resulting word ends with \"S\", remove it.</p> </li> <li> <p>If the resulting word ends with \"ED\", \"ING\", or \"ES\", remove it.</p> </li> <li> <p>If the resulting word starts with \"KN\", \"GN\", \"PN\", \"AE\", \"WR\", or \"WH\", remove the first letter.</p> </li> <li> <p>If the resulting word starts with \"X\", \"Z\", \"GN\", or \"KN\", retain the first two characters.</p> </li> <li> <p>Apply a second set of rules to the resulting word to generate an alternative code.</p> </li> <li> <p>Return the primary and alternative codes as a tuple.</p> </li> </ol> <p>The Alternative Double Metaphone algorithm takes into account different contexts in the word and is generated by following these steps:</p> <ol> <li> <p>Apply a set of prefix rules, such as:</p> <ol> <li>Convert the letter \"G\" at the beginning of the word to \"K\" if it is followed by \"N\", \"NED\", or \"NER\"</li> <li>Convert the letter \"A\" at the beginning of the word to \"E\" if it is followed by \"SCH\"</li> </ol> </li> <li> <p>Apply a set of suffix rules, such as:</p> <ol> <li>Convert the letters \"E\" and \"I\" at the end of the word to \"Y\"</li> <li>Convert the letters \"S\" and \"Z\" at the end of the word to \"X\"</li> <li>Remove the letter \"D\" at the end of the word if it is preceded by \"N\"</li> </ol> </li> <li> <p>Apply a set of replacement rules, such as:</p> <ol> <li>Replace the letter \"C\" with \"X\" if it is followed by \"IA\" or \"H\"</li> <li>Replace the letter \"T\" with \"X\" if it is followed by \"IA\" or \"CH\"</li> </ol> </li> <li> <p>Retain the first four characters of the resulting word, or pad it with zeros if it has fewer than four characters.</p> </li> <li> <p>If the resulting word starts with \"X\", \"Z\", \"GN\", or \"KN\", retain the first two characters.</p> </li> <li> <p>Return the alternative code.</p> </li> </ol> Example <p>You can test out the Metaphone transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.dmetaphone(\"Smith\"), phonetics.dmetaphone(\"Smyth\"))\n</code></pre> <p>('SM0', 'XMT') ('SM0', 'XMT')</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/comparisons/regular_expressions.html","title":"Regular Expressions","text":""},{"location":"topic_guides/comparisons/regular_expressions.html#regular-expressions-regex","title":"Regular expressions (regex)","text":"<p>It can sometimes be useful to make comparisons based on substrings or parts of column values. For example, one approach to comparing postcodes is to consider their constituent components, e.g. area, district, etc (see Featuring Engineering for more details).</p> <p>The <code>regex_extract</code> option enables users to do this by supplying a regular expression pattern that defines the substring upon which to evaluate a comparison. This option gives users a convenient means of comparing data within existing columns without needing to engineer new features from source data. <code>regex_extract</code> is available to all string comparators, as well as 'exact match' and 'columns reversed' comparisons and levels. </p> <p>Further regex functionality is provided by the <code>valid_string_pattern</code> option. This option allows users to define a regular expression pattern that specifies a valid string format. Any column value that does not adhere to the given pattern will be treated as a null. This can be useful for enforcing a specific data format during record comparison without needing to revisit and standardized data again. The <code>valid_string_pattern</code> argument is available to the null level and can be used with any comparison that contains a null level, e.g. <code>exact_match()</code>.</p>"},{"location":"topic_guides/comparisons/regular_expressions.html#examples-using-regex_extract","title":"Examples using <code>regex_extract</code>","text":""},{"location":"topic_guides/comparisons/regular_expressions.html#exact-match","title":"Exact match","text":"<p>Suppose you wish to make comparisons on a postcode column in your data, however only care about finding links between people who share the same area code (given by the first 1 to 2 letters of the postcode). The <code>regex_extract</code> option can be used within an exact match comparison to do this by passing it the regular expression pattern \"<code>^[A-Z]{1,2}</code>\":</p> DuckDB <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\npc_comparison = cl.exact_match(\"postcode\", regex_extract=\"^[A-Z]{1,2}\")\n</code></pre> <p>This gives a comparison with the following levels:</p> Output <p>Comparison 'Exact match vs. anything else' of \"postcode\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"postcode_l\" IS NULL OR \"postcode_r\" IS NULL</li> <li>'Exact match' with SQL rule:      regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')  =      regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')      </li> <li>'All other comparisons' with SQL rule: ELSE </li> </ul> <p>Below is an example set of record comparisons that could have been generated using <code>pc_comparison</code>. The part of the postcode actually being compared under the hood (the area code) is indicated in bold.</p> person_id_l person_id_r postcode_l postcode_r comparison_level 7 1 SE1P 0NY SE1P 0NY exact match 5 1 SE2 4UZ SE1P 0NY exact match 9 2 SW14 7PQ SW3 9JG exact match 4 8 N7 8RL EC2R 8AH else level 6 3 SE2 4UZ null level <p>The postcode comparison template provides an example of a comparison which makes use of the <code>regex_extract</code> option across multiple exact match levels.</p>"},{"location":"topic_guides/comparisons/regular_expressions.html#levenshtein","title":"Levenshtein","text":"<p>Using <code>regex_extract</code> in a Levenshtein comparison could be useful when comparing telephone numbers. For example, perhaps you only care about matches on dialling code but still want to account for potential typos in the data. (For more information on the different types of string comparators, see String Comparators.)</p> <p>This is achieved below by using <code>regex_extract=\"^[0-9]{1,4}\"</code> within a levenshtein comparison to restrict the comparison to only the first 3 to 4 digits of a telephone number:</p> DuckDB <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\ntel_comparison = cl.levenshtein_at_thresholds(\"telephone\", regex_extract=\"^[0-9]{1,4}\")\n</code></pre> <p>which gives a comparison with the following levels:</p> Output <p>Comparison 'Exact match vs. telephone within levenshtein thresholds 1, 2 vs. anything else' of \"telephone\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"telephone_l\" IS NULL OR \"telephone_r\" IS NULL</li> <li>'Exact match' with SQL rule:      regexp_extract(\"telephone_l\", '^[0-9]{1,4}')  =      regexp_extract(\"telephone_r\", '^[0-9]{1,4}')      </li> <li>'Levenshtein &lt;= 1' with SQL rule: levenshtein(     regexp_extract(\"telephone_l\", '^[0-9]{1,4}') ,      regexp_extract(\"telephone_r\", '^[0-9]{1,4}') ) &lt;= 1 </li> <li>'Levenshtein &lt;= 2' with SQL rule: levenshtein(     regexp_extract(\"telephone_l\", '^[0-9]{1,4}') ,      regexp_extract(\"telephone_r\", '^[0-9]{1,4}') ) &lt;= 2 </li> <li>'All other comparisons' with SQL rule: ELSE </li> </ul> <p>Here is an example set of record comparisons that could have been generated using <code>tel_comparison</code>. The part of the telephone number actually being compared under the hood (the dialling code) is highlighted in bold:</p> person_id_l person_id_r telephone_l telephone_r comparison_level 7 1 020 5555 1234 020 4444 4573 exact match 5 3 0161 999 5678 0160 333 6521 levenshtein distance &lt;= 1 5 2 0161 999 5678 160 221 2198 levenshtein distance &lt;= 2 4 1 0141 777 9876 020 4444 4573 else level 6 7 020 5555 1234 null level"},{"location":"topic_guides/comparisons/regular_expressions.html#jaro-winkler","title":"Jaro-Winkler","text":"<p>The <code>regex_extract</code> option can be useful in a Jaro-Winkler comparison of email addresses when the email domain is not considered important. (For more information on the different types of string comparators, see String Comparators.)</p> <p>Here <code>regex_extract</code> is used to extract everything before the '@' symbol in an email address, the username.</p> DuckDB <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nemail_comparison = cl.jaro_winkler_at_thresholds(\"email\", regex_extract=\"^[^@]+\")\n</code></pre> <p>This gives a comparison with the following levels:</p> Output <p>Comparison 'Exact match vs. Email within jaro_winkler_similarity thresholds 0.9, 0.7 vs. anything else' of \"email\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"email_l\" IS NULL OR \"email_r\" IS NULL</li> <li>'Exact match' with SQL rule:      regexp_extract(\"email_l\", '^[^@]+')  =      regexp_extract(\"email_r\", '^[^@]+')</li> <li>'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(     regexp_extract(\"email_l\", '^[^@]+') ,      regexp_extract(\"email_r\", '^[^@]+') ) &gt;= 0.9</li> <li>'Jaro_winkler_similarity &gt;= 0.7' with SQL rule: jaro_winkler_similarity(     regexp_extract(\"email_l\", '^[^@]+') ,      regexp_extract(\"email_r\", '^[^@]+') ) &gt;= 0.7</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul> <p>Here is an example set of record comparisons that could have been generated using <code>email_comparison</code>. The part of the email address actually being compared under the hood is shown in bold:</p> person_id_l person_id_r email_l email_r comparison_level 7 1 exact match 5 1 jaro-winkler similarity &gt;= 0.9 9 2 jaro-winkler similarity &gt;= 0.7 4 8 else level 6 3 null level"},{"location":"topic_guides/comparisons/regular_expressions.html#example-using-valid_string_pattern","title":"Example using <code>valid_string_pattern</code>","text":"<p>Recall that <code>valid_string_pattern</code> defines a regular expression pattern that if not matched will result in the column value being treated as a null. This can be a useful method to enforce a particular string format.</p> <p>For example, you might want to insist that postcodes conform to the standard UK postcode format (see here), which in regex syntax corresponds to \"^[A-Z]{1,2}0-9? 0-9$\"</p> DuckDB <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\npc_comparison = cl.exact_match(\"postcode\", valid_string_pattern=\"^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\")\n</code></pre> <p>which gives a comparison with the following levels:</p> Output <p>Comparison 'Exact match vs. anything else' of \"postcode\".</p> <p>Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule:      regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9? 0-9\\(')  IS NULL OR      regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}\\)')  IS NULL OR     regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9? 0-9\\(') =='' OR      regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}\\)')  =='' - 'Exact match' with SQL rule: \"postcode_l\" = \"postcode_r\" - 'All other comparisons' with SQL rule: ELSE</p> <p>Here is an example set of record comparisons that could have been generated using this <code>pc_comparison</code> where postcodes which do not conform to the valid format are treated as null:</p> person_id_l person_id_r postcode_l postcode_r comparison_level 7 1 SE1P 0NY SE1P 0NY exact match 9 2 S 7PQ SW3 9JG null level 6 3 SE2 4UZ null level 5 1 SE2 4UZ SE1P 0NY else level"},{"location":"topic_guides/comparisons/regular_expressions.html#additional-info","title":"Additional info","text":"<p>Regular expressions containing \u201c\\\u201d (the python escape character) are not compatible with Splink\u2019s Spark linker so please consider using alternative syntax, for example replacing \u201c\\d\u201d with \u201c[0-9]\u201d.</p> <p>Different regex patterns can achieve the same result but with more or less efficiency. You might want to consider optimising your regular expressions to improve performance (see here, for example).</p>"},{"location":"topic_guides/comparisons/term-frequency.html","title":"Term-Frequency adjustments","text":"","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#term-frequency-adjustments","title":"Term-Frequency Adjustments","text":"","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#problem-statement","title":"Problem Statement","text":"<p>A common shortcoming of the Fellegi-Sunter model is that it doesn\u2019t account for skew in the distributions of linking variables. One of the starkest examples is a binary variable such as gender in the prison population, where male offenders outnumber female offenders by 10:1.</p> <p></p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#how-does-this-affect-our-m-and-u-probabilities","title":"How does this affect our m and u probabilities?","text":"<ul> <li> <p>m probability is unaffected - given two records are a match, the gender field should also match with roughly the same probability for males and females</p> </li> <li> <p>Given two records are not a match, however, it is far more likely that both records will be male than that they will both be female - u probability is too low for the more common value (male) and too high otherwise.</p> </li> </ul> <p>In this example, one solution might be to create an extra comparison level for matches on gender:</p> <ul> <li> <p><code>l.gender = r.gender AND l.gender = 'Male'</code></p> </li> <li> <p><code>l.gender = r.gender AND l.gender = 'Female'</code></p> </li> </ul> <p>However, this complexity forces us to estimate two m probabilities when one would do, and it becomes impractical if we extend to higher-cardinality variables like surname, requiring thousands of additional comparison levels.</p> <p></p> <p>This problem used to be addressed with an ex-post (after the fact) solution - once the linking is done, we have a look at the average match probability for each value in a column to determine which values tend to be stronger indicators of a match. If the average match probability for records pairs that share a surname is 0.2 but the average for the specific surname Smith is 0.1 then we know that the match weight for name should be adjusted downwards for Smiths.</p> <p>The shortcoming of this option is that in practice, the model training is conducted on the assumption that all name matches are equally informative, and all of the underlying probabilities are evaluated accordingly. Ideally, we want to be able to account for term frequencies within the Fellegi-Sunter framework as trained by the EM algorithm.</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#toy-example","title":"Toy Example","text":"<p>Below is an illustration of 2 datasets (10 records each) with skewed distributions of first name. A <code>link_and_dedupe</code> Splink model concatenates these two tables and deduplicates those 20 records.</p> <p></p> <p>In principle, u probabilities for a small dataset like this can be estimated directly - out of 190 possible pairwise comparisons, 77 of them have the same first name. Based on the assumption that matches are rare (i.e. the vast majority of these comparisons are non-matches), we use this as a direct estimate of u. Random sampling makes the same assumption, but by using a manageable-sized sample of a much larger dataset where it would be prohibitively costly to perform all possible comparisons (a Cartesian join).</p> <p>Once we have concatenated our input tables, it is useful to calculate the term frequencies (TF) of each value. Rather than keep a separate TF table, we can add a TF column to the concatenated table - this is what <code>df_concat_with_tf</code> refers to within Splink.</p> <p>Building on the example above, we can define the m and u probabilities for a specific first name value, and work out an expression for the resulting match weight.</p> <p></p> <p>Just as we can add independent match weights for name, DOB and other comparisons (as shown in the Splink waterfall charts), we can also add an independent TF adjustment term for each comparison. This is useful because:</p> <ul> <li> <p>The TF adjustment doesn't depend on m, and therefore does not have to be estimated by the EM algorithm - it is known already</p> </li> <li> <p>The EM algorithm benefits from the TF adjustment (rather than previous post hoc implementations)</p> </li> <li> <p>It is trivially easy to \u201cturn off\u201d TF adjustments in our final match weights if we wish</p> </li> <li> <p>We can easily disentangle and visualise the aggregate significance of a particular column, separately from the deviations within it (see charts below)</p> </li> </ul> <p></p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#visualising-tf-adjustments","title":"Visualising TF Adjustments","text":"<p>For an individual comparison of two records, we can see the impact of TF adjustments in the waterfall charts:</p> <p> </p> This example shows two records having a match weight of +15.69 due to a match on first name, surname and DOB. Due to relatively uncommon values for all 3 of these, they each have an additional term frequency adjustment contributing around +5 to the final match weight <p>We can also see these match weights and TF adjustments summarised using a chart like the below to highlight common and uncommon names. We do this already using the Splink linker's profile_columns method, but once we know the u probabilities for our comparison columns, we can show these outliers in terms of their impact on match weight:</p> <p> </p> In this example of names from FEBRL data used in the demo notebooks, we see that a match on first name has a match weight of +6. For an uncommon name like Portia this is increased, whereas a common name like Jack has a reduced match weight. This chart can be generated using `linker.tf_adjustment_chart(\"name\")`","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#applying-tf-adjustments-in-splink","title":"Applying TF adjustments in Splink","text":"<p>Depending on how you compose your Splink settings, TF adjustments can be applied to a specific comparison level in different ways:</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#comparison-template-library-functions","title":"Comparison (template) library functions","text":"<pre><code>import splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\nsex_comparison = cl.exact_match(\"sex\",\n  term_frequency_adjustments = True\n)\n\nname_comparison = cl.distance_function_at_thresholds(\"name\",\n  distance_function_name = 'jaccard',\n  distance_threshold_or_thresholds = [0.9, 0.7],\n  term_frequency_adjustments = True\n)\n\ndob_comparison = ctl.date_comparison(\"date_of_birth\",\n  term_frequency_adjustments = True\n)\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#comparison-level-library-functions","title":"Comparison level library functions","text":"<pre><code>import splink.duckdb.comparison_level_library as cll\n\nname_comparison = {\n    \"output_column_name\": \"name\",\n    \"comparison_description\": \"Full name\",\n    \"comparison_levels\": [\n        cll.null_level(\"full_name\"),\n        cll.exact_match_level(\"full_name\", term_frequency_adjustments = True),\n        cll.columns_reversed_level(\"first_name\", \"surname\",\n          tf_adjustment_column = \"surname\"\n        ),\n        cll.else_level(),\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#providing-a-detailed-spec-as-a-dictionary","title":"Providing a detailed spec as a dictionary","text":"<pre><code>comparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &gt; 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#more-advanced-applications","title":"More advanced applications","text":"<p>The code examples above show how we can use term frequencies for different columns for different comparison levels, and demonstrated a few other features of the TF adjustment implementation in Splink:</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#multiple-columns","title":"Multiple columns","text":"<p>Each comparison level can be adjusted on the basis of a specified column. In the case of exact match levels, this is trivial but it allows some partial matches to be reframed as exact matches on a different derived column. One example could be ethnicity, often provided in codes as a letter (W/M/B/A/O - the ethnic group) and a number. Without TF adjustments, an ethnicity comparison might have 3 levels - exact match, match on ethnic group (<code>LEFT(ethnicity,1)</code>), no match. By creating a derived column <code>ethnic_group = LEFT(ethnicity,1)</code> we can apply TF adjustments to both levels.</p> <pre><code>ethnicity_comparison = {\n    \"output_column_name\": \"ethnicity\",\n    \"comparison_description\": \"Self-defined ethnicity\",\n    \"comparison_levels\": [\n        cll.null_level(\"ethnicity\"),\n        cll.exact_match_level(\"ethnicity\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"ethnic_group\", term_frequency_adjustments = True),\n        cll.else_level(),\n    ],\n}\n</code></pre> <p>A more critical example would be a full name comparison that uses separate first name and surname columns. Previous implementations would apply TF adjustments to each name component independently, so \u201cJohn Smith\u201d would be adjusted down for the common name \u201cJohn\u201d and then again for the common name \u201cSmith\u201d. However, the frequencies of names are not generally independent (e.g. \u201cMohammed Khan\u201d is a relatively common full name despite neither name occurring frequently). A simple full name comparison could therefore be structured as follows:</p> <pre><code>name_comparison = {\n    \"output_column_name\": \"name\",\n    \"comparison_description\": \"Full name\",\n    \"comparison_levels\": [\n        cll.null_level(\"full_name\"),\n        cll.exact_match_level(\"full_name\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"surname\", term_frequency_adjustments = True),\n        cll.else_level(),\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#fuzzy-matches","title":"Fuzzy matches","text":"<p>All of the above discussion of TF adjustments has assumed an exact match on the column in question, but this need not be the case. Where we have a \u201cfuzzy\u201d match between string values, it is generally assumed that there has been some small corruption in the text, for a number of possible reasons. A trivial example could be <code>\"Smith\"</code> vs <code>\"Smith \"</code> which we know to be equivalent if not an exact string match.</p> <p>In the case of a fuzzy match, we may decide it is desirable to apply TF adjustments for the same reasons as an exact match, but given there are now two distinct sides to the comparison, there are also two different TF adjustments. Building on our assumption that one side is the \u201ccorrect\u201d or standard value and the other contains some mistake, Splink will simply use the greater of the two term frequencies. There should be more <code>\"Smith\"</code>s than <code>\"Smith \"</code>s, so the former provides the best estimate of the true prevalence of the name Smith in the data.</p> <p>In cases where this assumption might not hold and both values are valid and distinct (e.g. <code>\"Alex\"</code> v <code>\"Alexa\"</code>), this behaviour is still desirable. Taking the most common of the two ensures that we err on the side of lowering the match score for a more common name than increasing the score by assuming the less common name.</p> <p>TF adjustments will not be applied to any comparison level without explicitly being turned on, but to allow for some middle ground when applying them to fuzzy match column, there is a <code>tf_adjustment_weight</code> setting that can down-weight the TF adjustment. A weight of zero is equivalent to turning TF adjustments off, while a weight of 0.5 means the match weights are halved, mitigating their impact:</p> <pre><code>{\n  \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &gt; 0.8\",\n  \"label_for_charts\": \"Exact match\",\n  \"tf_adjustment_column\": \"first_name\",\n  \"tf_adjustment_weight\": 0.5\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/comparisons/term-frequency.html#low-frequency-outliers","title":"Low-frequency outliers","text":"<p>Another example of where you may wish to limit the impact of TF adjustments is for exceedingly rare values. As defined above, the TF-adjusted match weight, K is inversely proportional to the term frequency, allowing K to become very large in some cases.</p> <p>Let\u2019s say we have a handful of records with the misspelt first name \u201cSiohban\u201d (rather than \u201cSiobhan\u201d). Fuzzy matches between the two spellings will rightly be adjusted on the basis of the frequency of the correct spelling, but there will be a small number of cases where the misspellings match one another. Given we suspect these values are more likely to be misspellings of more common names, rather than a distinct and very rare name, we can mitigate this effect by imposing a minimum value on the term frequency used (equivalent to the u value). This can be added to your full settings dictionary as in the example above using <code>\"tf_minimum_u_value\": 0.001</code>. This means that for values with a frequency of &lt;1 in 1000, it will be set to 0.001.</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/data_preparation/feature_engineering.html","title":"Feature Engineering","text":"","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#feature-engineering-for-data-linkage","title":"Feature Engineering for Data Linkage","text":"<p>During record linkage, the features in a given dataset are used to provide evidence as to whether two records are a match. Like any predictive model, the quality of a Splink model is dictated by the features provided.</p> <p>Below are some examples of features that be created from common columns, and how to create more detailed comparisons with them in a Splink model.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#postcodes","title":"Postcodes","text":"<p>A sensible approach to comparing postcodes is to consider their constituent components. For example, UK postcodes can be broken down into the following substrings:</p> <p> See image source for more details.</p> <p>Splink already includes a pre-built postcode comparison template which does this for you, generating by default a comparison with levels for an exact match on full postcode, sector, district and area in turn. These individual postcode components are engineered under-the-hood using the <code>regex_extract</code> argument (see below and comparison_templates.ipynb for more details).</p> <p>Code examples to use the comparison template:</p>  DuckDB Spark Athena <pre><code>import splink.duckdb.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.athena.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. all other comparisons' of \"postcode\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"postcode_l\" IS NULL OR \"postcode_r\" IS NULL</li> <li>'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"</li> <li> <p>'Exact match Postcode Sector' with SQL rule:         regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9? [0-9]')     =         regexp_extract(\"postcode_r\", '^[A-Z]{1,2}0-9? [0-9]')</p> </li> <li> <p>'Exact match Postcode District' with SQL rule:         regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9?')     =         regexp_extract(\"postcode_r\", '^[A-Z]{1,2}0-9?')</p> </li> <li> <p>'Exact match Postcode Area' with SQL rule:         regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')     =         regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')</p> </li> <li> <p>'All other comparisons' with SQL rule: ELSE</p> </li> </ul> <p>Note that the 'Exact match Postcode District' level also captures matches on subdistricts where they exist in the data.</p> <p>However, performing comparisons based on substrings alone doesn't always give the best sense of whether two postcodes are close together since locations which are geographically close can be in different postcode regions e.g. London postcodes starting 'N' vs 'SW'.</p> <p>Fortunately, Splink includes functions cll.distance_in_km_level() and cl.distance_in_km_at_thresholds() to calculate the physical distance between two sets of latitude and longitude coordinates. Users have the option to include <code>cll.distance_in_km_level()</code> as additional levels in the <code>postcode_comparison()</code> template by supplying <code>lat_col</code>, <code>long_col</code> and <code>km_thresholds</code> arguments. Doing so can help to improve results. Latitude and longitude coordinates can be derived from a postcode column as described in the example below.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#example","title":"Example","text":"<p>There are a number of open source repositories of geospatial data that can be used for linkage, one example is geonames.</p> <p>Below is an example of adding latitude and longitude columns from geonames to create a more nuanced comparison.</p> <p>Read in a dataset with postcodes:</p> <pre><code>import pandas as pd\n\ndf = pd.read_parquet(\"/PATH/TO/DEMO/DATA/historical_figures_with_errors_50k.parquet\")\ndf[\"postcode_fake\"] = df[\"postcode_fake\"].str.upper()\ndf.head()\n</code></pre> Output unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon TQ13 8DF male politician 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8HU politician 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF politician <p>Then read in a list of GB postcodes downloaded from geonames.</p> <pre><code>import pandas as pd\n\nnames = ['country_code', 'postal_code', 'place_name', 'admin_name1', 'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3', 'admin_code3', 'latitude', 'longitude','accuracy']\npostcodes = pd.read_csv(\"GB_full.txt\", sep=\"\\t\", header = None, names=names)\npostcodes.head(5)\n</code></pre> Output country_code postal_code place_name admin_name1 admin_code1 admin_name2 admin_code2 admin_name3 admin_code3 latitude longitude accuracy 0 GB AL3 8QE Slip End England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8479 -0.4474 6 1 GB AL5 3NG Harpenden England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8321 -0.383 6 2 GB AL5 3NS Hyde England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8333 -0.3763 6 3 GB AL5 3QF Hyde England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8342 -0.3851 6 4 GB B10 0AB Birmingham England ENG West Midlands nan Birmingham District (B) E08000025 52.4706 -1.875 6 <p>Now combine the lat-long coordinates from the <code>GB_full.txt</code> lookup.</p> <pre><code>df_with_coordinates = df.merge(postcodes[[\"postal_code\", \"latitude\", \"longitude\"]],\n                                    left_on=\"postcode_fake\",\n                                    right_on=\"postal_code\",\n                                    how=\"left\")\ndf_with_coordinates = df_with_coordinates.rename({'postcode_fake':'postcode'}, axis=1)\n\ndf_with_coordinates.head()\n</code></pre> Output unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode gender occupation postal_code latitude longitude 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8HU politician TQ13 8HU 50.6876 -3.8958 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF politician TQ13 8DF 50.6927 -3.8139 <p>Now that coordinates have been added, a more detailed postcode comparison can be produced using the <code>postcode_comparison</code> template:</p>  DuckDB Spark Athena <pre><code>import splink.duckdb.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.athena.comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons' of \"postcode\", \"long\" and \"lat\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <pre><code>- 'Null' with SQL rule:\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    IS NULL OR\n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    IS NULL OR\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    =='' OR\n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    ==''\n- 'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"\n- 'Exact match Postcode Sector' with SQL rule:\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n    =\n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n\n- 'Exact match Postcode District' with SQL rule:\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n    =\n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n\n- 'Exact match Postcode Area' with SQL rule:\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')\n    =\n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')\n\n- 'Distance less than 1km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 1\n\n- 'Distance less than 10km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 10\n\n- 'Distance less than 50km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 50\n\n- 'All other comparisons' with SQL rule: ELSE\n</code></pre> <p>or by using <code>cll.distance_in_km_level()</code> in conjunction with other comparison levels:</p>  DuckDB Spark Athena <pre><code>import splink.duckdb.comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre> <pre><code>import splink.spark.comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre> <pre><code>import splink.athena.comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#phonetic-transformations","title":"Phonetic transformations","text":"<p>Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently. These are particularly useful for names and can be used as an additional comparison level within name comparisons.</p> <p>For a more detailed explanation on phonetic transformation algorithms, see the topic guide.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#example_1","title":"Example","text":"<p>There are a number of python packages which support phonetic transformations that can be applied to a pandas dataframe, which can then be loaded into the DuckDBLinker. For example, creating a Double Metaphone column with the phonetics python library:</p> <pre><code>import pandas as pd\nimport phonetics\n\ndf = pd.read_parquet(\"PATH/TO/DATA/fake_1000.parquet\")\n\n# Define a function to apply the dmetaphone phonetic algorithm to each name in the column\ndef dmetaphone_name(name):\n    if name is None:\n        pass\n    else:\n        return phonetics.dmetaphone(name)\n\n# Apply the function to the \"first_name\" and surname columns using the apply method\ndf['first_name_dm'] = df['first_name'].apply(dmetaphone_name)\ndf['surname_dm'] = df['surname'].apply(dmetaphone_name)\n\ndf.head()\n</code></pre> Output unique_id first_name surname dob city email group first_name_dm surname_dm 0 0 Julia 2015-10-29 London hannah88@powers.com 0 ('JL', 'AL') 1 1 Julia Taylor 2015-07-31 London hannah88@powers.com 0 ('JL', 'AL') ('TLR', '') 2 2 Julia Taylor 2016-01-27 London hannah88@powers.com 0 ('JL', 'AL') ('TLR', '') 3 3 Julia Taylor 2015-10-29 hannah88opowersc@m 0 ('JL', 'AL') ('TLR', '') 4 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 ('AN', '') ('ATSN', 'FTSN') <p>Note: Soundex and Metaphone are also supported in phoneitcs</p> <p>Now that the dmetaphone columns have been added, they can be used within comparisons. For example, using the name_comparison function from the comparison template library.</p>  DuckDB Spark <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\n</code></pre> Output <p>Comparison 'Exact match vs. Names with phonetic exact match vs. First_Name within jaro_winkler thresholds 0.95, 0.88 vs. anything else' of \"first_name\" and \"first_name_dm\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Exact match first_name_dm' with SQL rule: \"first_name_dm_l\" = \"first_name_dm_r\"</li> <li>'Jaro_winkler_similarity &gt;= 0.95' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.95</li> <li>'Jaro_winkler_similarity &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.88</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#full-name","title":"Full name","text":"<p>When comparing names, it can be helpful to construct a single comparison for for comparing the forename and surname of two records. If a splink model has a single comparison for forename and surname, one of the major benefits is being able to consider the term frequency of the full name, as well as for forename and surname individually.</p> <p>For example, in the UK, \u201cMohammed Khan\u201d is a relatively common full name despite neither \"Mohammed\" or \"Khan\" occurring frequently as forename or surname, respectively.</p> <p>For more on term frequency, see the dedicated topic guide.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/data_preparation/feature_engineering.html#example_2","title":"Example","text":"<p>It is very simple to create a full name column from a <code>forename</code> and a <code>surname</code> in python.</p> <pre><code>import pandas as pd\n\ndf = pd.read_parquet(\"PATH/TO/DATA/fake_1000.parquet\")\n\n# Create a new column \"full_name\" by combining \"first_name\" and \"surname\" columns\ndf['full_name'] = df['first_name'] + ' ' + df['surname']\n\ndf.head()\n</code></pre> Output unique_id first_name surname dob city email group full_name 0 0 Julia 2015-10-29 London hannah88@powers.com 0 nan 1 1 Julia Taylor 2015-07-31 London hannah88@powers.com 0 Julia  Taylor 2 2 Julia Taylor 2016-01-27 London hannah88@powers.com 0 Julia  Taylor 3 3 Julia Taylor 2015-10-29 hannah88opowersc@m 0 Julia  Taylor 4 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 oNah Watson <p>Now that the <code>full_name</code> column has been added, it can be used within comparisons. For example, using the forenname_surname_comparison function from the comparison template library.</p>  DuckDB Spark <pre><code>import splink.duckdb.comparison_template_library as ctl\n\nfull_name_comparison = ctl.forename_surname_comparison(\n    \"first_name\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.comparison_template_library as ctl\n\nfull_name_comparison = ctl.forename_surname_comparison(\n    \"first_name\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within jaro-winkler threshold 0.88 vs. First_Name within jaro-winkler threshold 0.88 vs. anything else' of \"surname\" and \"first_name\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: (\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)</li> <li>'Full name exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" AND \"surname_l\" = \"surname_r\"</li> <li>'Exact match on reversed cols' with SQL rule: \"first_name_l\" = \"surname_r\" and \"first_name_r\" = \"surname_l\"</li> <li>'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Jaro_winkler_similarity surname &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.88</li> <li>'Jaro_winkler_similarity first_name &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.88</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/evaluation/clusters.html","title":"Clusters","text":""},{"location":"topic_guides/evaluation/clusters.html#cluster-evaluation","title":"Cluster Evaluation","text":"<p>This page is under construction - check back soon!</p>"},{"location":"topic_guides/evaluation/edge_metrics.html","title":"Edge Metrics","text":"<p>This guide is intended to be a reference guide for Edge Metrics used throughout Splink. It will build up from basic principles into more complex metrics.</p> <p>Note</p> <p>All of these metrics are dependent on having a \"ground truth\" to compare against. This is generally provided by Clerical Labelling (i.e. labels created by a human). For more on how to generate this ground truth (and the impact that can have on Edge Metrics), check out the Clerical Labelling Topic Guide.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#the-basics","title":"The Basics","text":"<p>Any Edge (Link) within a Splink model will fall into one of four categories:</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#true-positive","title":"True Positive","text":"<p>Also known as: True Link</p> <p>A True Positive is a case where a Splink model correctly predicts a match between two records.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#true-negative","title":"True Negative","text":"<p>Also known as: True Non-link</p> <p>A True Negative is a case where a Splink model correctly predicts a non-match between two records.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#false-positive","title":"False Positive","text":"<p>Also known as: False Link, Type I Error</p> <p>A False Positive is a case where a Splink model incorrectly predicts a match between two records, when they are actually a non-match.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#false-negative","title":"False Negative","text":"<p>Also known as: False Non-link, Missed Link, Type II Error</p> <p>A False Negative is a case where a Splink model incorrectly predicts a non-match between two records, when they are actually a match.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#confusion-matrix","title":"Confusion Matrix","text":"<p>These can be summarised in a Confusion Matrix</p> <p></p> <p>In a perfect model there would be no False Positives or False Negatives (i.e. FP = 0 and FN = 0).</p> Confusion Matrix in Splink <p>For a more in-depth guide on how to use the Splink Confusion Matrix, check out the <code>confusion_matrix_from_labels_table</code> API documentation and Chart Gallery.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#metrics-for-linkage","title":"Metrics for Linkage","text":"<p>The confusion matrix shows counts of each link type, but we are generally more interested in proportions. I.e. what percentage of the time does the model get the answer right?</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#accuracy","title":"Accuracy","text":"<p>The simplest metric is </p> \\[\\textsf{Accuracy} = \\frac{\\textsf{True Positives}+\\textsf{True Negatives}}{\\textsf{All Predictions}}\\] <p>This measures the proportion of correct classifications (of any kind). This may be useful for balanced data but high accuracy can be achieved by simply assuming the majority class for highly imbalanced data (e.g. assuming non-matches).</p> Accuracy in Splink <ul> <li>Accuracy is a (non-default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>Accuracy can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul> <p></p>"},{"location":"topic_guides/evaluation/edge_metrics.html#true-positive-rate-recall","title":"True Positive Rate (Recall)","text":"<p>Also known as: Sensitivity</p> <p>The True Positive Rate (Recall) is the proportion of matches that are correctly predicted by Splink.</p> \\[\\textsf{Recall} = \\frac{\\textsf{True Positives}}{\\textsf{All Positives}} = \\frac{\\textsf{True Positives}}{\\textsf{True Positives} + \\textsf{False Negatives}}\\] Recall in Splink <ul> <li>Recall is a (default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>Recall can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> <li>The interaction between Precision and Recall can be viewed with the <code>precision_recall_chart_from_labels_table</code> method. Check out the API Documantation and Chart Gallery to learn more.</li> <li>Recall is used to as part of estimating the probablility thay two random records match. This value is then used as a baseline to which all addtional evidence from features in your model is added to produce a final Splink score. For further information, checkout the estimate_probability_two_random_records_match API Documentation and the Model Training Topic Guide (Coming soon).</li> </ul>"},{"location":"topic_guides/evaluation/edge_metrics.html#true-negative-rate-specificity","title":"True Negative Rate (Specificity)","text":"<p>Also known as: Selectivity</p> <p>The True Negative Rate (Specificity) is the proportion of non-matches that are correctly predicted by Splink.</p> \\[\\textsf{Specificity} = \\frac{\\textsf{True Negatives}}{\\textsf{All Negatives}} = \\frac{\\textsf{True Negatives}}{\\textsf{True Negatives} + \\textsf{False Positives}}\\] Specificity in Splink <ul> <li>Specificity is a (non-default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>Specificity can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul>"},{"location":"topic_guides/evaluation/edge_metrics.html#positive-predictive-value-precision","title":"Positive Predictive Value (Precision)","text":"<p>The Positive Predictive Value (Precision), is the proportion of predicted matches which are true matches.</p> \\[\\textsf{Precision} = \\frac{\\textsf{True Positives}}{\\textsf{All Predicted Positives}} = \\frac{\\textsf{True Positives}}{\\textsf{True Positives} + \\textsf{False Negatives}}\\] Precision in Splink <ul> <li>Precision is a (default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>Precision can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> <li>The interaction between Precision and Recall can be viewed with the <code>precision_recall_chart_from_labels_table</code> method. Check out the API Documantation and Chart Gallery to learn more.</li> </ul>"},{"location":"topic_guides/evaluation/edge_metrics.html#negative-predictive-value","title":"Negative Predictive Value","text":"<p>The Negative Predictive Value is the proportion of predicted non-matches which are true non-matches.</p> \\[\\textsf{Negative Predictive Value} = \\frac{\\textsf{True Negatives}}{\\textsf{All Predicted Negatives}} = \\frac{\\textsf{True Negatives}}{\\textsf{True Negatives} + \\textsf{False Positives}}\\] Negative Predicitive Value in Splink <ul> <li>Negative Predictive Value is a (non-default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>Negative Predictive Value can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul> <p>Warning</p> <p>Each of these metrics looks at just one row or column of the confusion matrix. A model cannot be meaningfully summarised by just one of these performance measures.</p> <p>\u201cPredicts cancer with 100% Precision\u201d - is true of a \u201cmodel\u201d that correctly identifies one known cancer patient, but misdiagnoses everyone else as cancer-free.</p> <p>\u201cAI judge\u2019s verdicts have Recall of 100%\u201d - is true for a power-mad AI judge that declares everyone guilty, regardless of any evidence to the contrary.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#composite-metrics-for-linkage","title":"Composite Metrics for Linkage","text":"<p>This section contains composite metrics i.e. combinations of metrics that can been derived from the confusion matrix (Precision, Recall, Specificity and Negative Predictive Value). </p> <p>Any comparison of two records has a number of possible outcomes (True Positives, False Positives etc.), each of which has a different impact on your specific use case. It is very rare that a single metric defines the desired behaviour of a model. Therefore, evaluating performance with a composite metric (or a combination of metrics) is advised.</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#f-score","title":"F Score","text":"<p>The F-Score is a weighted harmonic mean of Precision (Positive Predictive Value) and Recall (True Positive Rate). For a general weight \\(\\beta\\):</p> \\[F_{\\beta} = \\frac{(1 + \\beta^2) \\cdot \\textsf{Precision} \\cdot \\textsf{Recall}}{\\beta^2 \\cdot \\textsf{Precision} + \\textsf{Recall}}\\] <p>where Recall is \\(\\beta\\) times more important than Precision.</p> <p>For example, when Precision and Recall are equally weighted (\\(\\beta = 1\\)), we get:</p> \\[F_{1} = 2\\left[\\frac{1}{\\textsf{Precision}}+\\frac{1}{\\textsf{Recall}}\\right]^{-1} = \\frac{2 \\cdot \\textsf{Precision} \\cdot \\textsf{Recall}}{\\textsf{Precision} + \\textsf{Recall}}\\] <p>Other popular versions of the F score are \\(F_{2}\\) (Recall twice as important as Precision) and \\(F_{0.5}\\) (Precision twice as important as Recall)</p> F-Score in Splink <ul> <li>F-Score is a (non-default) output of <code>accuracy_chart_from_labels_table</code>. There is the option to include \\(F_{1}\\), \\(F_{0.5}\\) and \\(F_{2}\\) scores. Check out the API Documentation and Chart Gallery to learn more.</li> <li>F-Scores (\\(F_{1}\\), \\(F_{0.5}\\) and \\(F_{2}\\)) can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul> <p>Warning</p> <p>F-score does not account for class imbalance in the data, and is asymmetric (i.e. it considers the prediction of matching records, but ignores how well the model correctly predicts non-matching records).</p>"},{"location":"topic_guides/evaluation/edge_metrics.html#p4-score","title":"P4 Score","text":"<p>The \\(P_{4}\\) Score is the harmonic mean of the 4 metrics that can be directly derived from the confusion matrix:</p> \\[ 4\\left[\\frac{1}{\\textsf{Recall}}+\\frac{1}{\\textsf{Specificity}}+\\frac{1}{\\textsf{Precision}}+\\frac{1}{\\textsf{Negative Predictive Value}}\\right]^{-1} \\] <p>This addresses one of the issues with the F-Score as it considers how well the model predicts non-matching records as well as matching records.</p> <p>Note: all metrics are given equal weighting.</p> \\(P_{4}\\) in Splink <ul> <li>\\(P_{4}\\) is a (non-default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>\\(P_{4}\\) can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul>"},{"location":"topic_guides/evaluation/edge_metrics.html#matthews-correlation-coefficient","title":"Matthews Correlation Coefficient","text":"<p>The Matthews Correlation Coefficient (\\(\\phi\\)) is a measure of how correlation between predictions and actual observations.</p> \\[ \\phi = \\sqrt{\\textsf{Recall} \\cdot \\textsf{Specificity} \\cdot \\textsf{Precision} \\cdot \\textsf{Negative Predictive Value}} - \\sqrt{(1 - \\textsf{Recall})(1 - \\textsf{Specificity})(1 - \\textsf{Precision})(1 - \\textsf{Negative Predictive Value})} \\] Matthews Correlation Coefficient (\\(\\phi\\)) in Splink <ul> <li>\\(\\phi\\) is a (non-default) output of <code>accuracy_chart_from_labels_table</code> check out the API Documentation and Chart Gallery to learn more.</li> <li>\\(\\phi\\) can be calculated and output in tabular format in Splink using labels as a column in your linking datasets, or labels as a separate table. To try this yourself, check out the <code>truth_space_table_from_labels_column</code> and <code>truth_space_table_from_labels_table</code> methods, respectively.</li> </ul> <p>Note</p> <p>Unlike the other metrics in this guide, \\(\\phi\\) is a correlation coefficient, so can range from -1 to 1 (as opposed to a range of 0 to 1).</p> <p>In reality, linkage models should never be negatively correlated with actual observations, so \\(\\phi\\) can be used in the same way as other metrics.</p>"},{"location":"topic_guides/evaluation/edge_overview.html","title":"Overview","text":""},{"location":"topic_guides/evaluation/edge_overview.html#edge-evaluation","title":"Edge Evaluation","text":"<p>Once you have a trained model, you use it to generate edges (links) between entities (nodes). These edges will have a Match Weight and corresponding Probability.</p> <p>There are several strategies for checking whether the links created in your pipeline perform as you want/expect.</p>"},{"location":"topic_guides/evaluation/edge_overview.html#consider-the-edge-metrics","title":"Consider the Edge Metrics","text":"<p>Edge Metrics measure how links perform at an overall level.</p> <p>First, consider how you would like your model to perform. What is important for your use case? Do you want to ensure that you capture all possible matches (i.e. high recall)? Or do you want to minimise the number of incorrectly predicted matches (i.e. high precision)? Perhaps a combination of both?</p> <p>For a summary of all the edge metrics available in Splink, check out the Edge Metrics guide.</p> <p>Note</p> <p>To produce Edge Metrics you will require a \"ground truth\" to compare your linkage results against (which can be achieved by Clerical Labelling).</p>"},{"location":"topic_guides/evaluation/edge_overview.html#spot-checking-pairs-of-records","title":"Spot Checking pairs of records","text":"<p>Spot Checking real examples of record pairs is helpful for confidence in linkage results. It is an effective way to build intuition for how the model works in practice and allows you to interrogate edge cases.</p> <p>Results of individual record pairs can be examined with the Waterfall Chart.</p> <p>Choosing which pairs of records to spot check can be done by either:</p> <ul> <li>Looking at all combinations of comparison levels and choosing which to examine in the Comparison Viewer Dashboard.</li> <li>Identifying and examining records which have been incorrectly predicted by your Splink model.</li> </ul> <p>As you are checking real examples, you will often come across cases that have not been accounted for by your model which you believe signify a match (e.g. a fuzzy match for names). We recommend using this feedback loop to help iterate and improve the definition of your model.</p>"},{"location":"topic_guides/evaluation/edge_overview.html#choosing-a-threshold","title":"Choosing a Threshold","text":"<p>Threshold selection is a key decision point within a linkage pipeline. One of the major benefits of probabilistic linkage versus a deterministic (i.e. rules-based) approach is the ability to choose the amount of evidence required for two records to be considered a match (i.e. a threshold). </p> <p>When you have decided on the metrics that are important for your use case, you can use the Confusion Matrix and Accuracy Chart to get a first estimate for what your threshold should be.</p> <p>Note</p> <p>The Confusion Matrix and Accuracy Chart both require labelled data to act as a \"ground truth\" to compare your linkage results against.</p> <p>Once you have an initial threshold, you can use Comparison Viewer Dashboard to look at records on either side of your threshold to check whether the threshold makes intuitive sense.</p> <p>From here, we recommend an iterative process of tweaking your threshold based on your spot checking then looking at the impact that this has on your overall edge metrics. Other tools that can be useful during this iterative process include the Precision-Recall Chart, the ROC Chart as well as spot checking where the model has gone wrong. </p>"},{"location":"topic_guides/evaluation/edge_overview.html#in-summary","title":"In Summary","text":"<p>Evaluating the edges (links) of a linkage model depends on your use case. Defining what \"good\" looks like is a key step, which then allows you to choose a relevant metric (or metrics) for measuring success. </p> <p>Your desired metric should help give an initial estimation for a linkage threshold, then you can use spot checking to help settle on a final threshold.</p> <p>In general, the links between pairs of records are not the final output of linkage pipeline. Most use-cases use these links to group records together into clusters. In this instance, evaluating the links themselves is not sufficient, you have to evaluate the resulting clusters as well.</p>"},{"location":"topic_guides/evaluation/labelling.html","title":"Clerical Labelling","text":""},{"location":"topic_guides/evaluation/labelling.html#clerical-labelling","title":"Clerical Labelling","text":"<p>This page is under construction - check back soon!</p>"},{"location":"topic_guides/evaluation/model.html","title":"Model","text":""},{"location":"topic_guides/evaluation/model.html#model-evaluation","title":"Model Evaluation","text":"<p>The parameters in a trained Splink model determine the match probability (Splink score) assigned to pairwise record comparisons. Before scoring any pairs of records there are a number of ways to check whether your model will perform as you expect.</p>"},{"location":"topic_guides/evaluation/model.html#look-at-the-model-parameters","title":"Look at the model parameters","text":"<p>The final model is summarised in the match weights chart with each bar in the chart signifying the match weight (i.e. the amount of evidence for or against a match) for each comparison level in your model.</p> <p>If, after some investigation, you still can't make sense of some of the match weights, take a look at the corresponding \\(m\\) and \\(u\\) values generated to see if they themselves make sense. These can be viewed in the m u parameters chart.</p> <p>Remember that \\(\\textsf{Match Weight} = \\log_2 \\frac{m}{u}\\)</p>"},{"location":"topic_guides/evaluation/model.html#look-at-the-model-training","title":"Look at the model training","text":"<p>The behaviour of a model during training can offer some insight into its utility. The more stable a model is in the training process, the more reliable the outputs are.</p> <p>Stability of model training can be seen in the Expectation Maximisation stage (for \\(m\\) training):</p> <ul> <li> <p>Stability across EM training sessions can be seen through the parameter estimates chart</p> </li> <li> <p>Stability within each session is indicated by the speed of convergence of the algorithm. This is shown in the terminal output during training. In general, the fewer iterations required to converge the better.</p> </li> </ul> Example of convergence output <p></p>"},{"location":"topic_guides/evaluation/model.html#in-summary","title":"In summary","text":"<p>Evaluating a trained model is not an exact science - there are no metrics which can definitively say whether a model is good or bad at this stage. In most cases, applying human logic and heuristics is the best you can do to establish whether the model is sensible. Given the variety of potential use cases of Splink, there is no perfect, universal model, just models that can be tuned to produce useful outputs for a given application.</p> <p>The tools within Splink are intended to help identify areas where your model may not be performing as expected. In future versions releases we hope to automatically flag where there are areas of a model that require further investigation to make this process easier for the user.</p>"},{"location":"topic_guides/evaluation/overview.html","title":"Overview","text":""},{"location":"topic_guides/evaluation/overview.html#evaluation-overview","title":"Evaluation Overview","text":"<p>Evaluation is a non-trivial, but crucial, task in data linkage. Linkage pipelines are complex and require many design decisions, each of which has an impact on the end result. </p> <p>This set of topic guides is intended to provide some structure and guidance on how to evaluate a Splink model alongside its resulting links and clusters.</p>"},{"location":"topic_guides/evaluation/overview.html#how-do-we-evaluate-different-stages-of-the-pipeline","title":"How do we evaluate different stages of the pipeline?","text":"<p>Evaluation in a data linking pipeline can be broken into 3 broad categories:</p>"},{"location":"topic_guides/evaluation/overview.html#model-evaluation","title":"Model Evaluation","text":"<p>After you have trained your model, you can start evaluating the parameters and overall design of the model. To see how, check out the Model Evaluation guide.</p>"},{"location":"topic_guides/evaluation/overview.html#edge-link-evaluation","title":"Edge (Link) Evaluation","text":"<p>Once you have trained a model, you will use it to predict the probability of links (edges) between entities (nodes). To see how to evaluate these links, check out the Edge Evaluation guide.</p>"},{"location":"topic_guides/evaluation/overview.html#cluster-evaluation","title":"Cluster Evaluation","text":"<p>Once you have chosen a linkage threshold, the edges are used to generate clusters of records. To see how to evaluate these clusters, check out the Cluster Evaluation guide.</p> <p>Note</p> <p>In reality, the development of a linkage pipeline involves iterating through multiple versions of models, links and clusters. For example, for each model version you will generally want to understand the downstream impact on the links and clusters generated. As such, you will likely revisit each stage of evaluation a number of times before settling on a final output.</p> <p>The aim of these guides, and the tools provided in Splink, is to ensure that you are able to extract enough information from each iteration to better understand how your pipeline is working and identify areas for improvement.</p>"},{"location":"topic_guides/performance/drivers_of_performance.html","title":"Run times, performance and linking large data","text":"<p>This topic guide covers the fundamental drivers of the run time of Splink jobs. </p>","tags":["Performance"]},{"location":"topic_guides/performance/drivers_of_performance.html#blocking","title":"Blocking","text":"<p>The primary driver of run time is the number of record pairs that the Splink model has to process. In Splink, the number of pairs to consider is reduced using Blocking Rules which are covered in depth in their own set of topic guides. </p>","tags":["Performance"]},{"location":"topic_guides/performance/drivers_of_performance.html#complexity-of-comparisons","title":"Complexity of comparisons","text":"<p>More complex comparisons reduces performance. Complexity is added to comparisons in a number of ways, including:</p> <ul> <li>Increasing the number of comparison levels</li> <li>Using more computationally expensive comparison functions</li> <li>Adding Term Frequency Adjustments</li> </ul> <p>Performant Term Frequency Adjustments</p> <p>Model training with Term Frequency adjustments can be made more performant by setting <code>estimate_without_term_frequencies</code> parameter to <code>True</code> in <code>estimate_parameters_using_expectation_maximisation</code>.</p>","tags":["Performance"]},{"location":"topic_guides/performance/drivers_of_performance.html#retaining-columns-through-the-linkage-process","title":"Retaining columns through the linkage process","text":"<p>The size your dataset has an impact on the performance of Splink. This is also applicable to the tables that Splink creates and uses under the hood. Some Splink functionality requires additional calculated columns to be stored. For example:</p> <ul> <li>The <code>comparison_viewer_dashboard</code> requires <code>retain_matching_columns</code> and <code>retain_intermediate_calculation_columns</code> to be set to <code>True</code> in the settings dictionary, but this makes some processes less performant.</li> </ul>","tags":["Performance"]},{"location":"topic_guides/performance/drivers_of_performance.html#filtering-out-pairwise-in-the-predict-step","title":"Filtering out pairwise in the <code>predict()</code> step","text":"<p>Reducing the number of pairwise comparisons that need to be returned will make Splink perform faster. One way of doing this is to filter comparisons with a match score below a given threshold (using a <code>threshold_match_probability</code> or <code>threshold_match_weight</code>) when you call <code>predict()</code>.</p>","tags":["Performance"]},{"location":"topic_guides/performance/drivers_of_performance.html#spark-performance","title":"Spark Performance","text":"<p>As  Spark is designed to distribute processing across multiple machines so there are additional configuration options available to make jobs run more quickly. For more information, check out the Spark Performance Topic Guide.</p> <p>Balancing computational performance and model accuracy</p> <p>There is usually a trade off between performance and accuracy in Splink models. I.e. some model design decisions that improve computational performance can also have a negative impact the accuracy of the model. </p> <p>Be sure to check how the suggestions in this topic guide impact the accuracy of your model to ensure the best results.</p>","tags":["Performance"]},{"location":"topic_guides/performance/optimising_duckdb.html","title":"Optimising DuckDB performance","text":"","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#optimising-duckdb-jobs","title":"Optimising DuckDB jobs","text":"<p>This topic guide describes how to configure DuckDB to optimise performance</p> <p>It is assumed readers have already read the more general guide to linking big data, and have chosen appropriate blocking rules.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#summary","title":"Summary:","text":"<ul> <li>From <code>splink==3.9.11</code> onwards, DuckDB generally parallelises jobs well, so you should see 100% usage of all CPU cores for the main Splink operations (parameter estimation and prediction)</li> <li>In some cases <code>predict()</code> needs salting on <code>blocking_rules_to_generate_predictions</code> to achieve 100% CPU use. You're most likely to need this in the following scenarios:<ul> <li>Very high core count machines</li> <li>Splink models that contain a small number of <code>blocking_rules_to_generate_predictions</code></li> <li>Splink models that have a relatively small number of input rows (less than around 500k)</li> </ul> </li> <li>If you are facing memory issues with DuckDB, you have the option of using an on-disk database.</li> <li>Reducing the amount of parallelism by removing salting can also sometimes reduce memory usage</li> </ul> <p>You can find a blog post with formal benchmarks of DuckDB performance on a variety of machine types here.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#configuration","title":"Configuration","text":"","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#ensuring-100-cpu-usage-across-all-cores-on-predict","title":"Ensuring 100% CPU usage across all cores on <code>predict()</code>","text":"<p>The aim is for overall parallelism of the predict() step to closely align to the number of thread/vCPU cores you have: - If parallelism is too low, you won't use all your threads - If parallelism is too high, runtime will be longer.</p> <p>The number of CPU cores used is given by the following formula:</p> <p>\\(\\text{base parallelism} = \\frac{\\text{number of input rows}}{122,880}\\)</p> <p>\\(\\text{blocking rule parallelism}\\)</p> <p>\\(= \\text{count of blocking rules} \\times\\) \\(\\text{number of salting partitions per blocking rule}\\)</p> <p>\\(\\text{overall parallelism} = \\text{base parallelism} \\times \\text{blocking rule parallelism}\\)</p> <p>If overall parallelism is less than the total number of threads, then you won't achieve 100% CPU usage.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#example","title":"Example","text":"<p>Consider a deduplication job with 1,000,000 input rows, on a machine with 32 cores (64 threads)</p> <p>In our Splink suppose we set:</p> <pre><code>settings =  {\n    ...\n    \"blocking_rules_to_generate_predictions\" ; [\n        block_on([\"first_name\"], salting_partitions=2),\n        block_on([\"dob\"], salting_partitions=2),\n        block_on([\"surname\"], salting_partitions=2),\n    ]\n    ...\n}\n</code></pre> <p>Then we have:</p> <ul> <li>Base parallelism of 9.</li> <li>3 blocking rules</li> <li>2 salting partitions per blocking rule</li> </ul> <p>We therefore have paralleism of \\(9 \\times 3 \\times 2 = 54\\), which is less than the 64 threads, and therefore we won't quite achieve full parallelism.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#generalisation","title":"Generalisation","text":"<p>The above formula for overall parallelism assumes all blocking rules have the same number of salting partitions, which is not necessarily the case. In the more general case of variable numbers of salting partitions, the formula becomes</p> \\[ \\text{overall parallelism} = \\text{base parallelism} \\times \\text{total number of salted blocking partitions across all blocking rules} \\] <p>So for example, with two blocking rules, if the first has 2 salting partitions, and the second has 10 salting partitions, when we would multiply base parallelism by 12.</p> <p>This may be useful in the case one of the blocking rules produces more comparisons than another: the 'bigger' blocking rule can be salted more.</p> <p>For further information about how parallelism works in DuckDB, including links to relevant DuckDB documentation and discussions, see here.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#running-out-of-memory","title":"Running out of memory","text":"<p>If your job is running out of memory, the first thing to consider is tightening your blocking rules, or running the workload on a larger machine.</p> <p>If these are not possible, the following config options may help reduce memory usage:</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#using-an-on-disk-database","title":"Using an on-disk database","text":"<p>DuckDB can spill to disk using several settings:</p> <p>Use the special <code>:temporary:</code> connection built into Splink that creates a temporary on disk database</p> <pre><code>linker = DuckDBLinker(\n    df, settings, connection=\":temporary:\"\n)\n</code></pre> <p>Use an on-disk database:</p> <pre><code>con = duckdb.connect(database='my-db.duckdb')\nlinker = DuckDBLinker(\n    df, settings, connection=con\n)\n</code></pre> <p>Use an in-memory database, but ensure it can spill to disk:</p> <pre><code>con = duckdb.connect(\":memory:\")\n\ncon.execute(\"SET temp_directory='/path/to/temp';\")\nlinker = DuckDBLinker(\n    df, settings, connection=con\n)\n</code></pre> <p>See also this section of the DuckDB docs</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_duckdb.html#reducing-salting","title":"Reducing salting","text":"<p>Empirically we have noticed that there is a tension between parallelism and total memory usage. If you're running out of memory, you could consider reducing parallelism.</p>","tags":["Performance","DuckDB","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html","title":"Optimising Spark performance","text":"","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#optimising-spark-jobs","title":"Optimising Spark jobs","text":"<p>This topic guide describes how to configure Spark to optimise performance - especially large linkage jobs which are slow or are not completing using default settings.</p> <p>It is assumed readers have already read the more general guide to linking big data, and blocking rules are proportionate to the size of the Spark cluster. As a very rough guide, on a small cluster of (say) 8 machines, we recommend starting with blocking rules that generate around 100 million comparisons. Once this is working, loosening the blocking rules to around 1 billion comparisons or more is often achievable.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#summary","title":"Summary:","text":"<ul> <li>Ensure blocking rules are not generating too many comparisons.</li> <li>We recommend setting the <code>break_lineage_method</code> to <code>\"parquet\"</code>, which is the default</li> <li><code>num_partitions_on_repartition</code> should be set so that each file in the output of <code>predict()</code> is roughly 100MB.</li> <li>Try setting <code>spark.default.parallelism</code> to around 5x the number of CPUs in your cluster</li> </ul> <p>For a cluster with 10 CPUs, that outputs about 8GB of data in parquet format, the following setup may be appropriate:</p> <pre><code>spark.conf.set(\"spark.default.parallelism\", \"50\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\nlinker = SparkLinker(\n    person_standardised_nodes,\n    settings,\n    break_lineage_method=\"parquet\",\n    num_partitions_on_repartition=80,\n)\n</code></pre>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#breaking-lineage","title":"Breaking lineage","text":"<p>Splink uses an iterative algorithm for model training, and more generally, lineage is long and complex. We have found that big jobs fail to complete without further optimisation. This is a well-known problem:</p> <p>Quote</p> <p>\"This long lineage bottleneck is widely known by sophisticated Spark application programmers. A common practice for dealing with long lineage is to have the application program strategically checkpoint RDDs at code locations that truncate much of the lineage for checkpointed data and resume computation immediately from the checkpoint.\"</p> <p>Splink will automatically break lineage in sensible places. We have found in practice that, when running Spark jobs backed by AWS S3, the fastest method of breaking lineage is persisting outputs to <code>.parquet</code> file.</p> <p>You can do this using the <code>break_lineage_method</code> parameter as follows:</p> <pre><code>linker = SparkLinker(\n    person_standardised_nodes,\n    settings,\n    break_lineage_method=\"parquet\"\n)\n</code></pre> <p>Other options are <code>checkpoint</code> and <code>persist</code>. For different Spark setups, particularly if you have fast local storage, you may find these options perform better.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#spark-parallelism","title":"Spark Parallelism","text":"<p>We suggest setting default parallelism to roughly 5x the number of CPUs in your cluster. This is a very rough rule of thumb, and if you're encountering performance problems you may wish to experiment with different values.</p> <p>One way to set default parallelism is as follows:</p> <pre><code>from pyspark.context import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\n\nconf.set(\"spark.default.parallelism\", \"50\")\nconf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\n</code></pre> <p>In general, increasing parallelism will make Spark 'chunk' your job into a larger amount of smaller tasks. This may solve memory issues. But note there is a tradeoff here: if you increase parallelism too high, Spark may take too much time scheduling large numbers of tasks, and may even run out of memory performing this work. See here. Also note that when blocking, jobs cannot be split into a large number of tasks than the cardinality of the blocking rule. For example, if you block on month of birth, this will be split into 12 tasks, irrespective of the parallelism setting. See here. You can use salting (below) to partially address this limitation.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#repartition-after-blocking","title":"Repartition after blocking","text":"<p>For some jobs, setting <code>repartition_after_blocking=True</code> when you initialise the <code>SparkLinker</code> may improve performance.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#salting","title":"Salting","text":"<p>For very large jobs, you may find that salting your blocking keys results in faster run times.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/optimising_spark.html#general-spark-config","title":"General Spark config","text":"<p>Splink generates large numbers of record comparisons from relatively small input datasets. This is an unusual type of workload, and so default Spark parameters are not always appropriate. Some of the issues encountered are similar to performance issues encountered with Cartesian joins - so some of the tips in relevant articles may help.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/performance/salting.html","title":"Salting blocking rules","text":"","tags":["Performance","Salting","Spark"]},{"location":"topic_guides/performance/salting.html#salting-blocking-rules","title":"Salting blocking rules","text":"<p>For very large linkages using Apache Spark, Splink supports salting blocking rules.</p> <p>Under certain conditions, this can help Spark better parallelise workflows, leading to shorter run times, and avoiding out of memory errors. It is most likely to help where you have blocking rules that create very large numbers of comparisons (100m records+) and where there is skew in how record comparisons are made (e.g. blocking on full name creates more comparisons amongst 'John Smith's than many other names).</p> <p>Further information about the motivation for salting can be found here.</p> <p>Note that salting is only available for the Spark backend</p>","tags":["Performance","Salting","Spark"]},{"location":"topic_guides/performance/salting.html#how-to-use-salting","title":"How to use salting","text":"<p>To enable salting using the <code>SparkLinker</code>, you provide some of your blocking rules as a dictionary rather than a string.</p> <p>This enables you to choose the number of salts for each blocking rule.</p> <p>Blocking rules provided as plain strings default to no salting (<code>salting_partitions = 1</code>)</p> <p>The following code snippet illustrates:</p> <pre><code>import logging\n\nfrom pyspark.context import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nfrom splink.spark.linker import SparkLinker\nfrom splink.spark.comparison_library import levenshtein_at_thresholds, exact_match\n\nconf = SparkConf()\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.sql.shuffle.partitions\", \"8\")\nconf.set(\"spark.default.parallelism\", \"8\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\n\n\nsettings = {\n    \"probability_two_random_records_match\": 0.01,\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.dob = r.dob\",\n        {\"blocking_rule\": \"l.first_name = r.first_name\", \"salting_partitions\": 4},\n    ],\n    \"comparisons\": [\n        levenshtein_at_thresholds(\"first_name\", 2),\n        exact_match(\"surname\"),\n        exact_match(\"dob\"),\n        exact_match(\"city\", term_frequency_adjustments=True),\n        exact_match(\"email\"),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"additional_columns_to_retain\": [\"group\"],\n    \"max_iterations\": 1,\n    \"em_convergence\": 0.01,\n}\n\n\ndf = spark.read.csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\", header=True)\n\n\nlinker = SparkLinker(df, settings)\nlogging.getLogger(\"splink\").setLevel(5)\n\nlinker.load_settings(settings)\nlinker.deterministic_link()\n</code></pre> <p>And we can see that salting has been applied by looking at the SQL generated in the log:</p> <pre><code>SELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '0' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.dob = r.dob\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 1\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 2\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 3\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 4\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\n</code></pre>","tags":["Performance","Salting","Spark"]},{"location":"topic_guides/splink_fundamentals/link_type.html","title":"Link type - linking vs deduping","text":"","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/splink_fundamentals/link_type.html#link-type-linking-deduping-or-both","title":"Link type: Linking, Deduping or Both","text":"<p>Splink allows data to be linked, deduplicated or both.</p> <p>Linking refers to finding links between datasets, whereas deduplication finding links within datasets.</p> <p>Data linking is therefore only meaningful when more than one dataset is provided.</p> <p>This guide shows how to specify the settings dictionary and initialise the linker for the three link types.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/splink_fundamentals/link_type.html#deduplication","title":"Deduplication","text":"<p>The <code>dedupe_only</code> link type expects the user to provide a single input table, and is specified as follows</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = SparkLinker(df, settings)\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = AthenaLinker(df, settings)\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = SQLiteLinker(df, settings)\n</code></pre>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/splink_fundamentals/link_type.html#link-only","title":"Link only","text":"<p>The <code>link_only</code> link type expects the user to provide a list of input tables, and is specified as follows:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = DuckDBLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SparkLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = AthenaLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SQLiteLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <p>The <code>input_table_aliases</code> argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/splink_fundamentals/link_type.html#link-and-dedupe","title":"Link and dedupe","text":"<p>The <code>link_and_dedupe</code> link type expects the user to provide a list of input tables, and is specified as follows:</p>  DuckDB Spark Athena SQLite <pre><code>from splink.duckdb.linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = DuckDBLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SparkLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = AthenaLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SQLiteLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <p>The <code>input_table_aliases</code> argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html","title":"Retrieving and querying Splink results","text":"","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#retrieving-and-querying-splink-results","title":"Retrieving and Querying Splink Results","text":"<p>Once you have created your linkage model, trained your parameters and predicted your match probabilities you may want to run queries on your results. Splink provides several methods for interacting with the tables produced in this process.</p>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#splinkdataframe","title":"SplinkDataFrame","text":"<p>Splink returns tables of results using a class called a <code>SplinkDataFrame</code>. For example, when you run <code>df_predict = linker.predict()</code>, the result <code>df_predict</code> is a <code>SplinkDataFrame</code>.</p> <p>A <code>SplinkDataFrame</code> is an abstraction of a table in the underlying backend database, and provides several convenience methods for interacting with the underlying table. For detailed information check the full API.</p>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#converting-to-other-types","title":"Converting to other types","text":"<p>It's possible to convert a <code>SplinkDataFrame</code> into a Pandas dataframe using <code>splink_df.as_pandas_dataframe()</code>. However, this is not recommended because Splink results can be very large, so converting them into pandas can be slow and result in out of memory errors. Usually it will be better to use SQL to query the tables directly.</p> <p>Alternatively you can use <code>splink_df.as_record_dict()</code> to get the table as a list of dictionary records - similarly this is not recommended aside from sufficiently small tables, as it involves loading the full data set into memory.</p>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#querying-tables","title":"Querying tables","text":"<p>You can find out the name of the table in the underlying database using <code>splink_df.physical_name</code>. This enables you to run SQL queries directly against the results. You can execute queries using <code>linker.query_sql</code> -  this is the recommended approach as it's typically faster and more memory efficient than using pandas dataframes.</p> <p>The following is an example of this approach, in which we use SQL to find the best match to each input record in a <code>link_type=\"link_only\"</code> job (i.e remove duplicate matches):</p> <pre><code># linker is a Linker with link_type set to \"link_only\"\nresults = linker.predict(threshold_match_probability=0.75)\n\nsql = f\"\"\"\nwith ranked as\n(\nselect *,\nrow_number() OVER (\n    PARTITION BY unique_id_l order by match_weight desc\n    ) as row_number\nfrom {results.physical_name}\n)\n\nselect *\nfrom ranked\nwhere row_number = 1\n\"\"\"\n\ndf_query_result = linker.query_sql(sql)  # pandas dataframe\n</code></pre> <p>Note that <code>linker.query_sql</code> will return a pandas dataframe by default, but you can instead return a <code>SplinkDataFrame</code> as follows: </p><pre><code>df_query_result = linker.query_sql(sql, output_type='splink_df')\n</code></pre>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#saving-results","title":"Saving results","text":"<p>If you have a <code>SplinkDataFrame</code>, you may wish to store the results in some file outside of your database. As tables may be large, there are a couple of convenience methods for doing this directly without needing to load the table into memory. Currently Splink supports saving frames to either <code>csv</code> or <code>parquet</code> format. Of these we generally recommend the latter, as it is typed, compressed, column-oriented, and easily supports nested data.</p> <p>To save results, simply use the methods <code>to_csv()</code> or <code>to_parquet()</code> - for example: </p><pre><code>df_predict = linker.predict()\ndf_predict.to_parquet(\"splink_predictions.parquet\", overwrite=True)\n# or alternatively:\ndf_predict.to_csv(\"splink_predictions.csv\", overwrite=True)\n</code></pre>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/querying_splink_results.html#creating-a-splinkdataframe","title":"Creating a <code>SplinkDataFrame</code>","text":"<p>Generally speaking, any Splink method that results in a table will return a <code>SplinkDataFrame</code>. This includes:</p> <ul> <li><code>linker.predict()</code></li> <li><code>linker.cluster_pairwise_predictions_at_threshold()</code></li> <li><code>linker.find_matches_to_new_records()</code></li> <li><code>linker.compare_two_records()</code></li> <li><code>linker.count_num_comparisons_from_blocking_rules_for_prediction()</code></li> <li><code>linker.truth_space_table_from_labels_table()</code></li> <li><code>linker.prediction_errors_from_labels_table()</code></li> <li><code>linker.compute_tf_table()</code></li> <li><code>linker.deterministic_link()</code></li> </ul> <p>Aside from these you can create a <code>SplinkDataFrame</code> for any table in your database. You will need to already have a linker to manage interactions with the database: </p><pre><code>import pandas as pd\nimport duckdb\n\ncon = duckdb.connect()\ndf_numbers = pd.DataFrame({\"id\": [1, 2, 3], \"number\": [\"one\", \"two\", \"three\"]})\ncon.sql(\"CREATE TABLE number_table AS SELECT * FROM df_numbers\")\n\nfrom splink.duckdb.linker import DuckDBLinker, DuckDBDataFrame\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n\nlinker = DuckDBLinker(df, {\"link_type\": \"dedupe_only\"}, connection=con)\n\nmy_splink_df = DuckDBDataFrame(\"a_templated_name\", \"number_table\", linker)\n</code></pre> <p>Alternatively if you have an in-memory data source you can use <code>linker.register_table()</code> which will register the table with the database backend and return a <code>SplinkDataFrame</code>: </p><pre><code>import pandas as pd\nfrom splink.duckdb.linker import DuckDBLinker, DuckDBDataFrame\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n\nlinker = DuckDBLinker(df, {\"link_type\": \"dedupe_only\"})\n\ndf_numbers = pd.DataFrame({\"id\": [1, 2, 3], \"number\": [\"one\", \"two\", \"three\"]})\nsplink_df_numbers = linker.register_table(df_numbers, \"table_name_in_backend\")\n</code></pre>","tags":["SQL","Data Frames","SplinkDataFrame"]},{"location":"topic_guides/splink_fundamentals/settings.html","title":"Defining Splink models","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#defining-a-splink-model","title":"Defining a Splink Model","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#what-makes-a-splink-model","title":"What makes a Splink Model?","text":"<p>When building any linkage model in Splink, there are 3 key things which need to be defined:</p> <ol> <li>What type of linkage you want (defined by the link type)</li> <li>What pairs of records to consider (defined by blocking rules)</li> <li>What features to consider, and how they should be compared (defined by comparisons)</li> </ol>","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#defining-a-splink-model-with-a-settings-dictionary","title":"Defining a Splink model with a settings dictionary","text":"<p>All aspects of a Splink model are defined via the settings dictionary. The settings object is a json-like object which underpins a model.</p> <p>For example, consider a simple model (as defined in the README). The model is defined by the following settings dictionary</p> <pre><code>import splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\"),\n    ],\n}\n</code></pre> <p>Where:</p> <p>1. Type of linkage</p> <p>The <code>\"link_type\"</code> is defined as a deduplication for a single dataset.</p> <pre><code>    \"link_type\": \"dedupe_only\",\n</code></pre> <p>2. Pairs of records to consider</p> <p>The <code>\"blocking_rules_to_generate_predictions\"</code> define a subset of pairs of records for the model to be conder when making predictions. In this case, where there is a match on <code>\"first_name\"</code> or <code>\"surname\"</code>.</p> <pre><code>    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n</code></pre> <p>For more information on how blocking is used in Splink, see the dedicated topic guide.</p> <p>3. Features to consider, and how they should be compared</p> <p>The <code>\"comparisons\"</code> define the features to be compared between records: <code>\"first_name\"</code>, <code>\"surname\"</code>, <code>\"dob\"</code>, <code>\"city\"</code> and <code>\"email\"</code>.</p> <pre><code>    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\"),\n    ],\n</code></pre> <p>Using functions from the comparison template library and comparison library to define how these features should be compared.</p> <pre><code>import splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\n</code></pre> <p>For more information on how comparisons are defined, see the dedicated topic guide.</p> <p>These functions generate comparisons within the settings dictionary. See below for the full settings dictionary once the <code>comparison_library</code> and <code>comparison_template_library</code> functions have been evaluated and constructed:</p> Settings Dictionary in full <pre><code>{\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\"\n    ],\n    \"comparisons\": [\n        {\n            \"output_column_name\": \"first_name\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" IS NULL OR \\\"first_name_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n                    \"label_for_charts\": \"Exact match first_name\"\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"first_name_l\\\", \\\"first_name_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &gt;= 0.9\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.9\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &gt;= 0.8\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.8\"\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\"\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"surname\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" IS NULL OR \\\"surname_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" = \\\"surname_r\\\"\",\n                    \"label_for_charts\": \"Exact match surname\"\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"surname_l\\\", \\\"surname_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &gt;= 0.9\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.9\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &gt;= 0.8\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.8\"\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\"\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"dob\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" IS NULL OR \\\"dob_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" = \\\"dob_r\\\"\",\n                    \"label_for_charts\": \"Exact match\"\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"dob_l\\\", \\\"dob_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\"\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('month',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 1\\n        \",\n                    \"label_for_charts\": \"Within 1 month\"\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('year',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 1\\n        \",\n                    \"label_for_charts\": \"Within 1 year\"\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('year',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 10\\n        \",\n                    \"label_for_charts\": \"Within 10 years\"\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\"\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Dob within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"city\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" IS NULL OR \\\"city_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" = \\\"city_r\\\"\",\n                    \"label_for_charts\": \"Exact match\",\n                    \"tf_adjustment_column\": \"city\",\n                    \"tf_adjustment_weight\": 1.0\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\"\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"email\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" IS NULL OR \\\"email_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" = \\\"email_r\\\"\",\n                    \"label_for_charts\": \"Exact match\"\n                },\n                {\n                    \"sql_condition\": \"levenshtein(\\\"email_l\\\", \\\"email_r\\\") &lt;= 2\",\n                    \"label_for_charts\": \"Levenshtein &lt;= 2\"\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\"\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Email within levenshtein threshold 2 vs. anything else\"\n        }\n    ],\n    \"sql_dialect\": \"duckdb\",\n    \"linker_uid\": \"wpYkgjrm\",\n    \"probability_two_random_records_match\": 0.0001\n}\n</code></pre> <p>With our finalised settings object, we can train a Splink model using the following code:</p> Example model using the settings dictionary <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\", 2),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = \"l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\npairwise_predictions = linker.predict()\n\nclusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.95)\nclusters.as_pandas_dataframe(limit=5)\n</code></pre>","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#advanced-usage-of-the-settings-dictionary","title":"Advanced usage of the settings dictionary","text":"<p>The section above refers to the three key aspects of the Splink settings dictionary. In reality, these are a small proportion of the possible parameters that can be defined within the settings. However, these additional parameters are used much less frequently, either because they are not required or they have a sensible default.</p> <p>For a list of all possible parameters that can be used within the settings dictionary, see the Settings Dictionary Reference and the Interactive Settings Editor.</p>","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#saving-a-trained-model","title":"Saving a trained model","text":"<p>Once you have have a trained Splink model, it is often helpful to save out the model. The <code>save_model_to_json</code> function allows the user to save out the specifications of their trained model.</p> <pre><code>linker.save_model_to_json(\"model.json\")\n</code></pre> <p>which, using the example settings and model training from above, gives the following output:</p> Model JSON <pre><code>{\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\"\n    ],\n    \"comparisons\": [\n        {\n            \"output_column_name\": \"first_name\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" IS NULL OR \\\"first_name_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n                    \"label_for_charts\": \"Exact match first_name\",\n                    \"m_probability\": 0.42539626585084356,\n                    \"u_probability\": 0.0055691509084167595\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"first_name_l\\\", \\\"first_name_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\",\n                    \"m_probability\": 0.07083503518755285,\n                    \"u_probability\": 0.0015597577555308368\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &gt;= 0.9\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.9\",\n                    \"m_probability\": 0.059652193684047713,\n                    \"u_probability\": 0.0013298726980595723\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") &gt;= 0.8\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.8\",\n                    \"m_probability\": 0.07293119026085258,\n                    \"u_probability\": 0.0060561117290816955\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.3711853150167032,\n                    \"u_probability\": 0.9854851069089111\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"surname\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" IS NULL OR \\\"surname_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" = \\\"surname_r\\\"\",\n                    \"label_for_charts\": \"Exact match surname\",\n                    \"m_probability\": 0.4799599325681923,\n                    \"u_probability\": 0.0074429557488431336\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"surname_l\\\", \\\"surname_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\",\n                    \"m_probability\": 0.07008569271116605,\n                    \"u_probability\": 0.0019083296710011445\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &gt;= 0.9\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.9\",\n                    \"m_probability\": 0.020227438482587002,\n                    \"u_probability\": 0.0006063412008846001\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") &gt;= 0.8\",\n                    \"label_for_charts\": \"Jaro_winkler_similarity &gt;= 0.8\",\n                    \"m_probability\": 0.04851972996678833,\n                    \"u_probability\": 0.003760255509361861\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.38120720627126636,\n                    \"u_probability\": 0.9862821178699093\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"dob\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" IS NULL OR \\\"dob_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" = \\\"dob_r\\\"\",\n                    \"label_for_charts\": \"Exact match\",\n                    \"m_probability\": 0.6110365914628999,\n                    \"u_probability\": 0.003703703703703704\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"dob_l\\\", \\\"dob_r\\\") &lt;= 1\",\n                    \"label_for_charts\": \"Damerau_levenshtein &lt;= 1\",\n                    \"m_probability\": 0.011899382842926385,\n                    \"u_probability\": 0.0014194194194194194\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('month',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 1\\n        \",\n                    \"label_for_charts\": \"Within 1 month\",\n                    \"m_probability\": 0.14328591788922354,\n                    \"u_probability\": 0.004904904904904905\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('year',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 1\\n        \",\n                    \"label_for_charts\": \"Within 1 year\",\n                    \"m_probability\": 0.23377741936500165,\n                    \"u_probability\": 0.05535935935935936\n                },\n                {\n                    \"sql_condition\": \"\\n            abs(date_diff('year',strptime(\\\"dob_l\\\",\\n              '%Y-%m-%d'),strptime(\\\"dob_r\\\",\\n              '%Y-%m-%d'))) &lt;= 10\\n        \",\n                    \"label_for_charts\": \"Within 10 years\",\n                    \"u_probability\": 0.3138258258258258\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 6.88439948607414e-07,\n                    \"u_probability\": 0.6207867867867868\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Dob within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"city\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" IS NULL OR \\\"city_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" = \\\"city_r\\\"\",\n                    \"label_for_charts\": \"Exact match\",\n                    \"m_probability\": 0.6594293995874598,\n                    \"u_probability\": 0.09307809682341518,\n                    \"tf_adjustment_column\": \"city\",\n                    \"tf_adjustment_weight\": 1.0\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.34057060041254017,\n                    \"u_probability\": 0.9069219031765848\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"email\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" IS NULL OR \\\"email_r\\\" IS NULL\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" = \\\"email_r\\\"\",\n                    \"label_for_charts\": \"Exact match\",\n                    \"m_probability\": 0.676510796673033,\n                    \"u_probability\": 0.004070304803112018\n                },\n                {\n                    \"sql_condition\": \"levenshtein(\\\"email_l\\\", \\\"email_r\\\") &lt;= 2\",\n                    \"label_for_charts\": \"Levenshtein &lt;= 2\",\n                    \"m_probability\": 0.07663549181271126,\n                    \"u_probability\": 0.00038595529012665426\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.24685371151425578,\n                    \"u_probability\": 0.9955437399067614\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Email within levenshtein threshold 2 vs. anything else\"\n        }\n    ],\n    \"sql_dialect\": \"duckdb\",\n    \"linker_uid\": \"mHRb2HT7\",\n    \"probability_two_random_records_match\": 0.0001\n}\n</code></pre> <p>This is simply the settings dictionary with additional entries for <code>\"m_probability\"</code> and <code>\"u_probability\"</code> in each of the <code>\"comparison_levels\"</code>, which have estimated during model training.</p> <p>For example in the first name exact match level:</p> <pre><code>{\n    \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n    \"label_for_charts\": \"Exact match first_name\",\n    \"m_probability\": 0.42539626585084356,\n    \"u_probability\": 0.0055691509084167595\n},\n</code></pre> <p>where the <code>m_probability</code> and <code>u_probability</code> values here are then used to generate the match weight for an exact match on <code>\"first_name\"</code> between two records (i.e. the amount of evidence provided by records having the same first name) in model predictions.</p>","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/settings.html#loading-a-pre-trained-model","title":"Loading a pre-trained model","text":"<p>When using a pre-trained model, you can read in the model from a json and recreate the linker object to make new pairwise predictions. For example:</p> <pre><code>linker = DuckDBLinker(new_df)\nlinker.load_model(\"model.json\")\n</code></pre> <p>Where the linker is initialised with a dataset, but no settings dictionary. Then the <code>load_model</code> function is used to add the settings dictionary (including the trained <code>\"m_probability\"</code> and <code>\"u_probability\"</code> values) to the linker.</p> <p>Once you have loaded the model, you can generate predictions, clusters etc. as normal. For example:</p> <pre><code>pairwise_predictions = linker.predict()\n</code></pre>","tags":["settings","Dedupe","Link","Link and Dedupe","Comparisons","Blocking Rules"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html","title":"Backends overview","text":"","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#splinks-sql-backends-spark-duckdb-etc","title":"Splink's SQL backends: Spark, DuckDB, etc","text":"<p>Splink is a Python library. It implements all data linking computations by generating SQL, and submitting the SQL statements to a backend of the user's choosing for execution.</p> <p>The Splink code you write is almost identical between backends, so it's straightforward to migrate between backends. Often, it's a good idea to start working using DuckDB on a sample of data, because it will produce results very quickly. When you're comfortable with your model, you may wish to migrate to a big data backend to estimate/predict on the full dataset.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#choosing-a-backend","title":"Choosing a backend","text":"","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#considerations-when-choosing-a-sql-backend-for-splink","title":"Considerations when choosing a SQL backend for Splink","text":"<p>When choosing which backend to use when getting started with Splink, there are a number of factors to consider:</p> <ul> <li>the size of the dataset(s)</li> <li>the amount of boilerplate code/configuration required</li> <li>access to specific (sometimes proprietary) platforms</li> <li>the backend-specific features offered by Splink</li> <li>the level of support and active development offered by Splink</li> </ul> <p>Below is a short summary of each of the backends available in Splink.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#duckdb","title":"DuckDB","text":"<p>DuckDB is recommended for most users. It is the fastest backend, and is capable of linking large datasets, especially if you have access to high-spec machines.</p> <p>As a rough guide it can:</p> <ul> <li>Link up to around 5 million records on a modern laptop (4 core/16GB RAM)</li> <li>Link tens of millions of records on high spec cloud computers very fast.</li> </ul> <p>For further details, see the results of formal benchmarking here.</p> <p>DuckDB is also recommended because for many users its simplest to set up.</p> <p>It can be run on any device with python installed and it is installed automatically with Splink via <code>pip install splink</code>. DuckDB has complete coverage for the functions in the Splink comparison libraries and, as a mainstay of the Splink development team, is actively maintained with features being added regularly.</p> <p>See the DuckDB deduplication example notebook to get a better idea of how Splink works with DuckDB.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#spark","title":"Spark","text":"<p>Spark is a system for distributed computing which is great for large datasets. It is more involved in terms of configuration, with more boilerplate code than the likes of DuckDB. Spark has complete coverage for the functions in the Splink comparison libraries and, as a mainstay of the Splink development team, is actively maintained with features being added regularly.</p> <p>Spark is primarily recommended for users who either:</p> <ul> <li>Need to link enormous datasets (100 million records+), and have experience out of memory/out of disk problems with DuckDB</li> <li>Or have easier access to a Spark cluster than a single high-spec instance to run DuckDB</li> </ul> <p>If working with Databricks note that the Splink development team does not have access to a Databricks environment there will be instances where we will be unable to provide support.</p> <p>See the Spark deduplication example notebook to get a better idea of how Splink works with Spark.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#athena","title":"Athena","text":"<p>Athena is a big data SQL backend provided on AWS which is great for large datasets (10+ million records). It requires access to a live AWS account and as a persistent database, requires some additional management of the tables created by Splink. Athena has reasonable, but not complete, coverage for the functions in the Splink comparison libraries, with gaps in string fuzzy matching functionality due to the lack of some string functions in Athena's underlying SQL engine, Presto. At this time, the Athena backend is being used sparingly by the Splink development team so receives minimal levels of support.</p> <p>In addition, from a development perspective, the necessity for an AWS connection makes testing Athena code more difficult, so there may be occasional bugs that would normally be caught by our testing framework.</p> <p>See the Athena deduplication example notebook to get a better idea of how Splink works with Athena.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#sqlite","title":"SQLite","text":"<p>SQLite is similar to DuckDB in that it is, generally, more suited to smaller datasets. SQLite is simple to setup and can be run directly in a Jupyter notebook, but is not as performant as DuckDB. SQLite has reasonable, but not complete, coverage for the functions in the Splink comparison libraries, with gaps in array and date comparisons. String fuzzy matching, while not native to SQLite is available via python UDFs which has some performance implications. SQLite is not actively been used by the Splink team so receives minimal levels of support.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#postgresql","title":"PostgreSql","text":"<p>PostgreSql is a relatively new linker, so we have not fully tested performance or what size of datasets can processed with Splink. The Postgres backend requires a Postgres database, so it is recommend to use this backend only if you are working with a pre-existing Postgres database. Postgres has reasonable, but not complete, coverage for the functions in the Splink comparison libraries, with gaps in string fuzzy matching functionality due to the lack of some string functions in Postgres. At this time, the Postgres backend is not being actively used by the Splink development team so receives minimal levels of support.</p> <p>More details on using Postgres as a Splink backend can be found on the postgres page.</p>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#using-your-chosen-backend","title":"Using your chosen backend","text":"<p>Import the linker from the backend of your choosing, and the backend-specific comparison libraries.</p> <p>Once you have initialised the <code>linker</code> object, there is no difference in the subsequent code between backends.</p>  DuckDB Spark Athena SQLite PostgreSql <pre><code>from splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_level_library as cll\n\nlinker = DuckDBLinker(your_args)\n</code></pre> <pre><code>from splink.spark.linker import SparkLinker\nimport splink.spark.comparison_library as cl\nimport splink.spark.comparison_level_library as cll\n\nlinker = SparkLinker(your_args)\n</code></pre> <pre><code>from splink.athena.linker import AthenaLinker\nimport splink.athena.comparison_library as cl\nimport splink.athena.comparison_level_library as cll\n\nlinker = AthenaLinker(your_args)\n</code></pre> <pre><code>from splink.sqlite.linker import SQLiteLinker\nimport splink.sqlite.comparison_library as cl\nimport splink.sqlite.comparison_level_library as cll\n\nlinker = SQLiteLinker(your_args)\n</code></pre> <pre><code>from splink.postgres.linker import PostgresLinker\nimport splink.postgres.comparison_library as cl\nimport splink.postgres.comparison_level_library as cll\n\nlinker = PostgresLinker(your_args)\n</code></pre>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#additional-information-for-specific-backends","title":"Additional Information for specific backends","text":"","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/backends.html#sqlite_1","title":"SQLite","text":"<p>SQLite does not have native support for fuzzy string-matching functions. However, some are available for Splink users as python user-defined functions (UDFs):</p> <ul> <li><code>levenshtein</code></li> <li><code>damerau_levenshtein</code></li> <li><code>jaro</code></li> <li><code>jaro_winkler</code></li> </ul> <p>However, there are a couple of points to note:</p> <ul> <li>These functions are implemented using the rapidfuzz package, which must be installed if you wish to make use of them, via e.g. <code>pip install rapidfuzz</code>. If you do not wish to do so you can disable the use of these functions when creating your linker: <pre><code>linker = SQLiteLinker(df, settings, ..., register_udfs=False)\n</code></pre></li> <li>As these functions are implemented in python they will be considerably slower than any native-SQL comparisons. If you find that your model-training or predictions are taking a large time to run, you may wish to consider instead switching to DuckDB (or some other backend).</li> </ul>","tags":["Spark","DuckDB","Athena","SQLite","Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html","title":"PostgreSQL","text":"","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#using-postgresql-as-a-splink-backend","title":"Using PostgreSQL as a Splink backend","text":"<p>Splink is compatible with using PostgreSQL (or simply as Postgres) as a SQL backend - for other options have a look at the overview of Splink backends.</p>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#setup","title":"Setup","text":"<p>Splink makes use of SQLAlchemy for connecting to Postgres, and the default database adapter is <code>psycopg2</code>, but you should be able to use any other if you prefer. The <code>PostgresLinker</code> requires a valid engine upon creation to manage interactions with the database: </p><pre><code>from sqlalchemy import create_engine\n\nfrom splink.postgres.linker import PostgresLinker\nimport splink.postgres.comparison_library as cl\n\n# create a sqlalchemy engine to manage connecting to the database\nengine = create_engine(\"postgresql+psycopg2://USER:PASSWORD@HOST:PORT/DB_NAME\")\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n}\n</code></pre> <p>You can pass data to the linker in one of two ways:</p> <ul> <li> <p>use the name of a pre-existing table in your database </p><pre><code>linker = PostgresLinker(\"my_data_table\", settings, engine=engine)\n</code></pre> </li> <li> <p>or pass a pandas DataFrame directly, in which case the linker will create a corresponding table for you automatically in the database </p><pre><code>import pandas as pd\n\n# create pandas frame from csv\ndf = pd.read_csv(\"./my_data_table.csv\")\n\nlinker = PostgresLinker(df, settings, engine=engine)\n</code></pre> </li> </ul>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#permissions","title":"Permissions","text":"<p>When you connect to Postgres, you must do so with a role that has sufficient privileges for Splink to operate correctly. These are:</p> <ul> <li><code>CREATE ON DATABASE</code>, to allow Splink to create a schema for working, and install the <code>fuzzystrmatch</code> extension</li> <li><code>USAGE ON LANGUAGE SQL</code> and <code>USAGE ON TYPE float8</code> - these are required for creating the UDFs that Splink employs for calculations</li> </ul>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#things-to-know","title":"Things to know","text":"","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#schemas","title":"Schemas","text":"<p>When you create a <code>PostgresLinker</code>, Splink will create a new schema within the database you specify - by default this schema is called <code>splink</code>, but you can choose another name by passing the appropriate argument when creating the linker: </p><pre><code>linker = PostgresLinker(df, settings, engine=engine, schema=\"another_splink_schema\")\n</code></pre> This schema is where all of Splink's work will be carried out, and where any tables created by Splink will live. <p>By default when looking for tables, Splink will check the schema it created, and the <code>public</code> schema; if you have tables in other schemas that you would like to be discoverable by Splink, you can use the parameter <code>other_schemas_to_search</code>: </p><pre><code>linker = PostgresLinker(df, settings, engine=engine, other_schemas_to_search=[\"my_data_schema_1\", \"my_data_schema_2\"])\n</code></pre>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#user-defined-functions-udfs","title":"User-Defined Functions (UDFs)","text":"<p>Splink makes use of Postgres' user-defined functions in order to operate, which are defined in the schema created by Splink when you create the linker. These functions are all defined using SQL, and are:</p> <ul> <li><code>log2</code> - required for core Splink functionality</li> <li><code>datediff</code> - for the datediff comparison level</li> <li><code>ave_months_between</code> - for the datediff comparison level</li> <li><code>array_intersect</code> - for the array intersect comparison level</li> </ul> <p>Information</p> <p>The information below is only relevant if you are planning on making changes to Splink. If you are only intending to use Splink with Postgres, you do not need to read any further.</p>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#testing-splink-with-postgres","title":"Testing Splink with Postgres","text":"<p>To run only the Splink tests that run against Postgres, you can run simply: </p><pre><code>pytest -m postgres_only tests/\n</code></pre> For more information see the documentation page for testing in Splink. <p>The tests will are run using a temporary database and user that are created at the start of the test session, and destroyed at the end.</p>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#postgres-via-docker","title":"Postgres via docker","text":"<p>If you are trying to run tests with Splink on Postgres, or simply develop using Postgres, you may prefer to not actually install Postgres on you system, but to run it instead using Docker. In this case you can simply run the setup script (a thin wrapper around <code>docker-compose</code>): </p><pre><code>./scripts/postgres/setup.sh\n</code></pre> Included in the docker-compose file is a pgAdmin container to allow easy exploration of the database as you work, which can be accessed in-browser on the default port. <p>When you are finished you can remove these resources: </p><pre><code>./scripts/postgres/teardown.sh\n</code></pre>","tags":["Postgres","Backends"]},{"location":"topic_guides/splink_fundamentals/backends/postgres.html#running-with-a-pre-existing-database","title":"Running with a pre-existing database","text":"<p>If you have a pre-existing postgres server you wish to use to run the tests against, you will need to specify environment variables for the credentials where they differ from default (in parentheses):</p> <ul> <li><code>SPLINKTEST_PG_USER</code> (<code>splinkognito</code>)</li> <li><code>SPLINKTEST_PG_PASSWORD</code> (<code>splink123!</code>)</li> <li><code>SPLINKTEST_PG_HOST</code> (<code>localhost</code>)</li> <li><code>SPLINKTEST_PG_PORT</code> (<code>5432</code>)</li> <li><code>SPLINKTEST_PG_DB</code> (<code>splink_db</code>) - tests will not actually run against this, but it is from a connection to this that the temporary test database + user will be created</li> </ul> <p>While care has been taken to ensure that tests are run using minimal permissions, and are cleaned up after, it is probably wise to run tests connected to a non-important database, in case anything goes wrong. In addition to the above privileges, in order to run the tests you will need:</p> <ul> <li><code>CREATE DATABASE</code> to create a temporary testing database</li> <li><code>CREATEROLE</code> to create a temporary user role with limited privileges, which will be actually used for all the SQL execution in the tests</li> </ul>","tags":["Postgres","Backends"]},{"location":"topic_guides/theory/fellegi_sunter.html","title":"The Fellegi-Sunter Model","text":""},{"location":"topic_guides/theory/fellegi_sunter.html#the-fellegi-sunter-model","title":"The Fellegi-Sunter model","text":"<p>Splink implements the Fellegi-Sunter model for probabilistic record linkage. This topic guide is intended to give a high-level introduction to the model. If you aren't familiar with the concept probabilistic record linkage, it is recommended to read that topic guide first.</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#parameters-of-the-fellegi-sunter-model","title":"Parameters of the Fellegi-Sunter model","text":"<p>The Fellegi-Sunter model has three main parameters that need to be considered to generate a match probability between two records:</p> <ul> <li>\\(\\lambda\\) - probability that any two records match </li> <li>\\(m\\) - probability of a given observation given the records are a match</li> <li>\\(u\\) - probability of a given observation given the records are not a match</li> </ul>"},{"location":"topic_guides/theory/fellegi_sunter.html#probability","title":"\u03bb probability","text":"<p>The lambda (\\(\\lambda\\)) parameter is the prior probability that any two records match. I.e. assuming no other knowledge of the data, how likely is a match? Or, as a formula:</p> \\[ \\lambda = Pr(\\textsf{Records match}) \\] <p>This is the same for all records comparisons, but is highly dependent on:</p> <ul> <li>The total number of records  </li> <li>The number of duplicate records (more duplicates increases \\(\\lambda\\))  </li> <li>The overlap between datasets  <ul> <li>Two datasets covering the same cohort (high overlap, high \\(\\lambda\\))</li> <li>Two entirely independent datasets (low overlap, low \\(\\lambda\\))</li> </ul> </li> </ul>"},{"location":"topic_guides/theory/fellegi_sunter.html#m-probability","title":"m probability","text":"<p>The \\(m\\) probability is the probability of a given observation given the records are a match. Or, as a formula:</p> \\[ m = Pr(\\textsf{Observation | Records match}) \\] <p>For example, consider the the \\(m\\) probability of a match on Date of Birth (DOB). For two records that are a match, what is the probability that:</p> <ul> <li>DOB is the same:</li> <li>Almost 100%, say 98% \\(\\Longrightarrow m \\approx 0.98\\)</li> <li>DOB is different:</li> <li>Maybe a 2% chance of a data error? \\(\\Longrightarrow m \\approx 0.02\\)</li> </ul> <p>The \\(m\\) probability is largely a measure of data quality - if DOB is poorly collected, it may only match exactly for 50% of true matches.</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#u-probability","title":"u probability","text":"<p>The \\(u\\) probability is the probability of a given observation given the records are not a match. Or, as a formula:</p> \\[ u = Pr(\\textsf{Observation | Records do not match}) \\] <p>For example, consider the the \\(u\\) probability of a match on Surname. For two records that are not a match, what is the probability that:</p> <ul> <li>Surname is the same:</li> <li>Depending on the surname, &lt;1%? \\(\\Longrightarrow u \\approx 0.005\\)</li> <li>Surname is different:</li> <li>Almost 100% \\(\\Longrightarrow u \\approx 0.995\\)</li> </ul> <p>The \\(u\\) probability is a measure of coincidence. As there are so many possible surnames, the chance of sharing the same surname with a randomly-selected person is small.</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#interpreting-m-and-u","title":"Interpreting m and u","text":"<p>In the case of a perfect unique identifier:</p> <ul> <li>A person is only assigned one such value - \\(m = 1\\) (match) or \\(m=0\\) (non-match)  </li> <li>A value is only ever assigned to one person - \\(u = 0\\) (match) or \\(u = 1\\) (non-match)</li> </ul> <p>Where \\(m\\) and \\(u\\) deviate from these ideals can usually be intuitively explained:</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#m-probability_1","title":"m probability","text":"<p>A measure of data quality/reliability.</p> <p>How often might a person's information change legitimately or through data error? </p> <ul> <li>Names: typos, aliases, nicknames, middle names, married names etc.</li> <li>DOB: typos, estimates (e.g. 1st Jan YYYY where date not known)</li> <li>Address: formatting issues, moving house, multiple addresses, temporary addresses</li> </ul>"},{"location":"topic_guides/theory/fellegi_sunter.html#u-probability_1","title":"u probability","text":"<p>A measure of coincidence/cardinality<sup>1</sup>.</p> <p>How many different people might share a given identifier?</p> <ul> <li>DOB (high cardinality) \u2013 for a flat age distribution spanning ~30 years, there are ~10,000 DOBs (0.01% chance of a match)</li> <li>Sex (low cardinality) \u2013 only 2 potential values (~50% chance of a match)</li> </ul>"},{"location":"topic_guides/theory/fellegi_sunter.html#match-weights","title":"Match Weights","text":"<p>One of the key measures of evidence of a match between records is the match weight. </p>"},{"location":"topic_guides/theory/fellegi_sunter.html#deriving-match-weights-from-m-and-u","title":"Deriving Match Weights from m and u","text":"<p>The match weight is a measure of the relative size of \\(m\\) and \\(u\\):</p> \\[ \\begin{equation} \\begin{aligned}     M &amp;= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2 K \\\\[10pt]     &amp;= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2 m - \\log_2 u \\end{aligned} \\end{equation} \\] <p>where \\(\\lambda\\) is the probability that two random records match and \\(K=m/u\\) is the Bayes factor.</p> <p>A key assumption of the Fellegi Sunter model is that observations from different column/comparisons are independent of one another. This means that the Bayes factor for two records is the products of the Bayes factor for each column/comparison:</p> \\[ K_\\textsf{features} = K_\\textsf{forename} \\cdot K_\\textsf{surname} \\cdot K_\\textsf{dob} \\cdot K_\\textsf{city} \\cdot K_\\textsf{email} \\] <p>This, in turn, means that match weights are additive:</p> \\[ M_\\textsf{obs} = M_\\textsf{prior} + M_\\textsf{features} \\] <p>where \\(M_\\textsf{prior} = \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\) and \\(M_\\textsf{features} = M_\\textsf{forename} + M_\\textsf{surname} + M_\\textsf{dob} + M_\\textsf{city} + M_\\textsf{email}\\).</p> <p>So, considering these properties, the total match weight for two observed records can be rewritten as:</p> \\[ \\begin{equation} \\begin{aligned}     M_\\textsf{obs} &amp;= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\sum_{i}^\\textsf{features}\\log_2(\\frac{m_i}{u_i}) \\\\[10pt]     &amp;= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_i^\\textsf{features}\\frac{m_i}{u_i}\\right) \\end{aligned} \\end{equation} \\]"},{"location":"topic_guides/theory/fellegi_sunter.html#interpreting-match-weights","title":"Interpreting Match Weights","text":"<p>The match weight is the central metric showing the amount of evidence of a match is provided by each of the features in a model. The is most easily shown through Splink's Waterfall Chart:</p> <p></p> <ul> <li>1\ufe0f\u20e3 are the two records being compared</li> <li> <p>2\ufe0f\u20e3 is the match weight of the prior, \\(M_\\textsf{prior} = \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\).   This is the match weight if no additional knowledge of features is taken into account, and can be thought of as similar to the y-intercept in a simple regression.</p> </li> <li> <p>3\ufe0f\u20e3 are the match weights of each feature, \\(M_\\textsf{forename}\\), \\(M_\\textsf{surname}\\), \\(M_\\textsf{dob}\\), \\(M_\\textsf{city}\\) and \\(M_\\textsf{email}\\) respectively.</p> </li> <li> <p>4\ufe0f\u20e3 is the total match weight for two observed records, combining 2\ufe0f\u20e3 and 3\ufe0f\u20e3:</p> \\[  \\begin{equation} \\begin{aligned}     M_\\textsf{obs} &amp;= M_\\textsf{prior} + M_\\textsf{forename} + M_\\textsf{surname} + M_\\textsf{dob} + M_\\textsf{city} + M_\\textsf{email} \\\\[10pt]      &amp;= -6.67 + 4.74 + 6.49 - 1.97 - 1.12 + 8.00 \\\\[10pt]      &amp;= 9.48 \\end{aligned} \\end{equation} \\] </li> <li> <p>5\ufe0f\u20e3 is an axis representing the \\(\\textsf{match weight} = \\log_2(\\textsf{Bayes factor})\\))</p> </li> <li> <p>6\ufe0f\u20e3 is an axis representing the equivalent match probability (noting the non-linear scale). For more on the relationship between match weight and probability, see the sections below</p> </li> </ul>"},{"location":"topic_guides/theory/fellegi_sunter.html#match-probability","title":"Match Probability","text":"<p>Match probability is a more intuitive measure of similarity than match weight, and is, generally, used when choosing a similarity threshold for record matching.</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#deriving-match-probability-from-match-weight","title":"Deriving Match Probability from Match Weight","text":"<p>Probability of two records being a match can be derived from the total match weight:</p> \\[ Pr(\\textsf{Match | Observation}) = \\frac{2^{M_\\textsf{obs}}}{1+2^{M_\\textsf{obs}}} \\] Example <p>Consider the example in the Interpreting Match Weights section.  The total match weight, \\(M_\\textsf{obs} = 9.48\\). Therefore,</p> \\[ Pr(\\textsf{Match | Observation}) = \\frac{2^{9.48}}{1+2^{9.48}} \\approx 0.999 \\]"},{"location":"topic_guides/theory/fellegi_sunter.html#understanding-the-relationship-between-match-probability-and-match-weight","title":"Understanding the relationship between Match Probability and Match Weight","text":"<p>It can be helpful to build up some intuition for how match weight translates into match probability. </p> <p>Plotting match probability versus match weight gives the following chart:</p> <p></p> <p>Some observations from this chart:</p> <ul> <li>\\(\\textsf{Match weight} = 0 \\Longrightarrow \\textsf{Match probability} = 0.5\\)</li> <li>\\(\\textsf{Match weight} = 2 \\Longrightarrow \\textsf{Match probability} = 0.8\\) </li> <li>\\(\\textsf{Match weight} = 3 \\Longrightarrow \\textsf{Match probability} = 0.9\\) </li> <li>\\(\\textsf{Match weight} = 4 \\Longrightarrow \\textsf{Match probability} = 0.95\\) </li> <li>\\(\\textsf{Match weight} = 7 \\Longrightarrow \\textsf{Match probability} = 0.99\\) </li> </ul> <p>So, the impact of any additional match weight on match probability gets smaller as the total match weight increases. This makes intuitive sense as, when comparing two records, after you already have a lot of evidence/features indicating a match, adding more evidence/features will not have much of an impact on the probability of a match.</p> <p>Similarly, if you already have a lot of negative evidence/features indicating a match, adding more evidence/features will not have much of an impact on the probability of a match.</p>"},{"location":"topic_guides/theory/fellegi_sunter.html#deriving-match-probability-from-m-and-u","title":"Deriving Match Probability from m and u","text":"<p>Given the definitions for match probability and match weight above, we can rewrite the probability in terms of \\(m\\) and \\(u\\).</p> \\[ \\begin{equation} \\begin{aligned} Pr(\\textsf{Match | Observation}) &amp;= \\frac{2^{\\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right)}}{1+2^{\\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right)}} \\\\[20pt]  &amp;= \\frac{\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}}{1+\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}} \\\\[20pt]  &amp;= 1 - \\left[1+\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right]^{-1} \\end{aligned} \\end{equation} \\]"},{"location":"topic_guides/theory/fellegi_sunter.html#further-reading","title":"Further Reading","text":"<p>For a more in-depth introduction to the Fellegi-Sunter model, see Robin Linacre's Interactive Blogs including:</p> <ul> <li>The mathematics of the Fellegi Sunter model</li> <li>Visualising the Fellegi Sunter Model</li> <li>Understanding match weights</li> <li>Dependencies between match weights</li> <li>m and u probability generator</li> </ul> <p>Also, see the academic paper used as the basis for a similar implementation of Fellegi Sunter in the R fastlink package.</p> <ol> <li> <p>Cardinality is the the number of items in a set. In record linkage, cardinality refers to the number of possible values a feature could have. This is important in record linkage, as the number of possible options for e.g. date of birth has a significant impact on the amount of evidence that a match on date of birth provides for two records being a match.\u00a0\u21a9</p> </li> </ol>"},{"location":"topic_guides/theory/probabilistic_vs_deterministic.html","title":"Probabilistic vs Deterministic linkage","text":""},{"location":"topic_guides/theory/probabilistic_vs_deterministic.html#types-of-record-linkage","title":"Types of Record Linkage","text":"<p>There are two main types of record linkage - Deterministic and Probabilistic.</p>"},{"location":"topic_guides/theory/probabilistic_vs_deterministic.html#deterministic-linkage","title":"Deterministic Linkage","text":"<p>Deterministic Linkage is a rules-based approach for joining records together.</p> <p>For example, consider a single table with duplicates:</p> AID Name DOB Postcode A00001 Bob Smith 1990-05-09 AB12 3CD A00002 Robert Smith 1990-05-09 AB12 3CD A00003 Robert \u201cBurglar Bob\u201d Smith 1990-05-09 - <p>and some deterministic rules:</p> <pre><code>IF Name + DOB match (Rule 1)\nELSE \nIF Forename + DOB + Postcode match (Rule 2)\n</code></pre> <p>Applying these rules to the table above leads to no matches:</p> <p>A0001-A0002 No match (different forename) A0001-A0003 No match (different forename) A0002-A0003 No match (missing postcode)</p> <p>So, even a relatively simple dataset, with duplicates that are obvious to a human, will require more complex rules.</p> <p>In general, Deterministic linkage is:</p>      \u2705 Computationally cheap       \u2705 Capable of achieving high precision (few False Positives)          \u274c Difficult to do systematically       \u274c Difficult to optimise       \u274c Lacking in subtlety       \u274c Prone to Low recall (False Negatives)     Deterministic Linkage in Splink <p>While Splink is primarily a tool for Probabilistic linkage, Deterministic linkage is also supported (utilising blocking rules). See the example notebooks to see how this is Deterministic linkage is implemented in Splink.</p>"},{"location":"topic_guides/theory/probabilistic_vs_deterministic.html#probabilistic-linkage","title":"Probabilistic Linkage","text":"<p>Probabilistic Linkage is a evidence-based approach for joining records together.</p> <p>Linkage is probabilistic in the sense that it relies on the balance of evidence. In a large dataset, observing that two records match on the full name 'Robert Smith' provides some evidence that these two records may refer to the same person, but this evidence is inconclusive. However, the cumulative evidence from across multiple features within the dataset (e.g. date of birth, home address, email address) can provide conclusive evidence of a match. The evidence for a match is commonly represented as a probability. </p> <p>For example, putting the first 2 records of the table above through a probabilistic model gives a an overall probability that the records are a match: </p> <p>In addition, the breakdown of this probability by the evidence provided by each feature can be shown through a waterfall chart:</p> <p></p> <p>Given these probabilities, unlike (binary) Deterministic linkage, the user can choose an evidence threshold for what they consider a match before creating a new unique identifier.</p> <p>This is important, as it allows the linkage to be customised to best support the specific use case. For example, if it is important to:</p> <ul> <li>minimise False Positive matches (i.e. where False Negatives are less of a concern), a higher threshold for a match can be chosen. </li> <li>maximise True Positive matches (i.e. where False Positives are less of a concern), a lower threshold can be chosen.</li> </ul> <p>Further Reading</p> <p>For a more in-depth introduction to Probabilistic Data Linkage, including an interactive version of the waterfall chart above, see Robin Linacre's Blog.</p> Probabilistic Linkage in Splink <p>Splink is primarily a tool for Probabilistic linkage, and implements the Fellegi-Sunter model - the most common probabilistic record linkage model. See the Splink Tutorial for a step by step guide for Probabilistic linkage in Splink.</p> <p>A Topic Guide on the Fellegi-Sunter model is can be found here!</p>"},{"location":"topic_guides/theory/record_linkage.html","title":"Why do we need record linkage?","text":""},{"location":"topic_guides/theory/record_linkage.html#why-do-we-need-record-linkage","title":"Why do we need record linkage?","text":""},{"location":"topic_guides/theory/record_linkage.html#in-a-perfect-world","title":"In a perfect world","text":"<p>In a perfect world, everyone (and everything) would have a single, unique identifier. If this were the case, linking any datasets would be a simple left join. </p> Example <p>Consider 2 tables of people A and B with no duplicates and each person has a unique id UID. Joining these tables in SQL would simple be:</p> <pre><code>SELECT *\nFROM A\nLEFT JOIN B\nON A.UID = B.UID\n</code></pre>"},{"location":"topic_guides/theory/record_linkage.html#in-reality","title":"In reality","text":"<p>Real datasets often lack truly unique identifiers (both inside and across datasets).</p> <p>The overall aim of record linkage is to generate a unique identifier to be used like <code>UID</code> to our \"perfect world\" scenario. </p> <p>Record linkage the process of using the information within records to assess whether records refer to the same entity.  For example, if records refer to people, factors such as names, date of birth, location etc can be used to link records together.</p> <p>Record linkage can be done within datasets (deduplication) or between datasets (linkage), or both.</p>"},{"location":"blog/category/ethics.html","title":"Ethics","text":""},{"location":"blog/category/feature-updates.html","title":"Feature Updates","text":""}]}